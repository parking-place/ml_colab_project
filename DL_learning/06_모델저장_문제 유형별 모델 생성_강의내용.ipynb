{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "- 학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다. \n",
    "- 파이토치는 모델의 파라미터만 저장하는 방법과 모델 구조와 파라미터 모두를 저장하는 두가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "    - `torch.save(저장할 객체, 저장경로)`\n",
    "- 보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "    - `torch.save(model, 저장경로)`\n",
    "- 불러오기\n",
    "    - `load_model = torch.load(저장경로)`\n",
    "- 저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 모델의 파라미터만 저장\n",
    "- 모델을 구성하는 파라미터만 저장한다.\n",
    "- 모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "- 모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "- 모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "- `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "- 모델의 state_dict을 조회 후 저장한다.\n",
    "    - `torch.save(model.state_dict(), \"저장경로\")`\n",
    "- 생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    - `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_layer = nn.Linear(784, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = torch.flatten(X, start_dim=1)\n",
    "        X = nn.ReLU()(self.in_layer(X))\n",
    "        X = self.out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (in_layer): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model = Network()\n",
    "sample_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_layer.weight', 'in_layer.bias', 'out.weight', 'out.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict 조회\n",
    "sd = sample_model.state_dict()\n",
    "print(type(sd))\n",
    "sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['in_layer.weight'].shape, sd['in_layer.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001FDFEC2F1B0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0101,  0.0306, -0.0164,  0.0321, -0.0119,  0.0323,  0.0212, -0.0171,\n",
       "        -0.0073, -0.0351,  0.0132,  0.0166, -0.0245, -0.0214,  0.0279, -0.0108,\n",
       "        -0.0248,  0.0193, -0.0216, -0.0066, -0.0010, -0.0283, -0.0340,  0.0041,\n",
       "        -0.0054, -0.0333, -0.0097, -0.0122, -0.0289, -0.0154, -0.0025, -0.0081,\n",
       "        -0.0186,  0.0063, -0.0092, -0.0047, -0.0262, -0.0220,  0.0330,  0.0064,\n",
       "        -0.0155, -0.0262,  0.0228, -0.0118, -0.0217,  0.0313, -0.0102,  0.0343,\n",
       "         0.0271,  0.0144,  0.0318,  0.0012, -0.0118,  0.0072, -0.0020,  0.0168,\n",
       "        -0.0297,  0.0301, -0.0074, -0.0079,  0.0264, -0.0033,  0.0261, -0.0161],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_model.in_layer.weight\n",
    "sample_model.in_layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint를 저장 및 불러오기\n",
    "- 학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "- Dictionary에 필요한 요소들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "#### 이어학습\n",
    "model.train()\n",
    "#### 추론\n",
    "model.eval()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- MLP(Multi Layer Perceptron)\n",
    "    - Fully Connected Layer로 구성된 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchinfo\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Regression(회귀)\n",
    "\n",
    "## Boston Housing Dataset\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "- CRIM: 범죄율\n",
    "- ZN: 25,000 평방피트당 주거지역 비율\n",
    "- INDUS: 비소매 상업지구 비율\n",
    "- CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "- NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "- RM: 주택당 방의 수\n",
    "- AGE: 1940년 이전에 건설된 주택의 비율\n",
    "- DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "- RAD: 고속도로 접근성\n",
    "- TAX: 재산세율\n",
    "- PTRATIO: 학생/교사 비율\n",
    "- B: 흑인 비율\n",
    "- LSTAT: 하위 계층 비율\n",
    "<br><br>\n",
    "- **Target**\n",
    "    - MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.read_csv('boston_hosing.csv')\n",
    "boston.shape\n",
    "\n",
    "X_boston = boston.drop(columns='MEDV').values\n",
    "y_boston = boston['MEDV'].values.reshape(-1, 1) # 2차원\n",
    "X_boston.shape, y_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test set 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston,\n",
    "                                                    test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_scaled = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "# y를 Tensor 타입으로 변환.\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 102\n",
      "(tensor([-0.3726, -0.4996, -0.7049,  3.6645, -0.4249,  0.9357,  0.6937, -0.4372,\n",
      "        -0.1622, -0.5617, -0.4846,  0.3717, -0.4110]), tensor([26.7000]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "boston_train_set = TensorDataset(X_train_scaled, y_train_tensor)\n",
    "boston_test_set = TensorDataset(X_test_scaled, y_test_tensor)\n",
    "print(len(boston_train_set), len(boston_test_set))\n",
    "print(boston_train_set[0])\n",
    "\n",
    "# DataLoader\n",
    "boston_train_loader = DataLoader(boston_train_set, batch_size=200, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "boston_test_loader = DataLoader(boston_test_set, batch_size=len(boston_test_set))\n",
    "len(boston_train_loader), len(boston_test_loader)  # epoch 당 step 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_boston.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 입력 layer => in_feature: input data의 feature개수에 맞춘다.\n",
    "        self.lr1 = nn.Linear(13, 32)   # input\n",
    "        # Hidden layer => in_feature: 앞 Layer의 out_feature 개수에 맞춘다.\n",
    "        self.lr2 = nn.Linear(32, 16)  \n",
    "        # output layer => out_feature: 모델의 최종 출력 개수에 맞춘다. (집값 1개->1)\n",
    "        self.lr3 = nn.Linear(16, 1)\n",
    "    \n",
    "        \n",
    "    def forward(self, X):\n",
    "        # input layer\n",
    "        out = self.lr1(X)\n",
    "        out = nn.ReLU()(out)\n",
    "        # hidden\n",
    "        out = self.lr2(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        # output -> 회귀처리 모델에서 output layer에서는 활성함수를 적용하지 않는다.\n",
    "        #   예외: 출력결과가 특정 활성함수의 출력과 매칭될 경우.\n",
    "        #        output: 0 ~ 1 => logistic 함수사용.\n",
    "        #        output: -1 ~ 1=> hyperbolic tangent (tanh)\n",
    "        out  = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonModel()\n",
    "torchinfo.summary(boston_model, (200, 13))  #(모델, 입력데이터shape-(batch size, feature) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 1000\n",
    "LR = 0.001\n",
    "# 결과 저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "# 모델, loss함수(회귀-MSE), optimizer\n",
    "boston_model = BostonModel()\n",
    "boston_model = boston_model.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 594.37888, val loss: 570.43408\n",
      "[2/1000] train loss: 588.66394, val loss: 562.88055\n",
      "[3/1000] train loss: 578.88843, val loss: 554.63916\n",
      "[4/1000] train loss: 570.28241, val loss: 545.62061\n",
      "[5/1000] train loss: 566.16193, val loss: 535.91327\n",
      "[6/1000] train loss: 548.04317, val loss: 525.59784\n",
      "[7/1000] train loss: 544.15288, val loss: 514.36932\n",
      "[8/1000] train loss: 533.58899, val loss: 502.53384\n",
      "[9/1000] train loss: 520.16202, val loss: 490.04745\n",
      "[10/1000] train loss: 503.44720, val loss: 477.12799\n",
      "[11/1000] train loss: 491.97881, val loss: 463.80646\n",
      "[12/1000] train loss: 477.20920, val loss: 450.13510\n",
      "[13/1000] train loss: 462.22078, val loss: 436.23578\n",
      "[14/1000] train loss: 444.92764, val loss: 422.03226\n",
      "[15/1000] train loss: 432.14780, val loss: 407.62064\n",
      "[16/1000] train loss: 415.96544, val loss: 393.07483\n",
      "[17/1000] train loss: 398.83139, val loss: 378.47281\n",
      "[18/1000] train loss: 383.42761, val loss: 363.88638\n",
      "[19/1000] train loss: 368.05484, val loss: 349.37817\n",
      "[20/1000] train loss: 353.38503, val loss: 334.92365\n",
      "[21/1000] train loss: 332.67911, val loss: 320.70105\n",
      "[22/1000] train loss: 321.87312, val loss: 306.36954\n",
      "[23/1000] train loss: 301.61053, val loss: 292.33179\n",
      "[24/1000] train loss: 288.17752, val loss: 278.65146\n",
      "[25/1000] train loss: 272.13469, val loss: 265.46292\n",
      "[26/1000] train loss: 259.12704, val loss: 252.69608\n",
      "[27/1000] train loss: 246.58121, val loss: 240.46005\n",
      "[28/1000] train loss: 231.71478, val loss: 228.92163\n",
      "[29/1000] train loss: 215.85726, val loss: 217.97432\n",
      "[30/1000] train loss: 207.66993, val loss: 207.50531\n",
      "[31/1000] train loss: 195.67847, val loss: 197.70747\n",
      "[32/1000] train loss: 184.70064, val loss: 188.51060\n",
      "[33/1000] train loss: 173.69517, val loss: 179.90436\n",
      "[34/1000] train loss: 163.70976, val loss: 171.89708\n",
      "[35/1000] train loss: 155.40461, val loss: 164.39897\n",
      "[36/1000] train loss: 147.08203, val loss: 157.43117\n",
      "[37/1000] train loss: 139.42784, val loss: 150.92471\n",
      "[38/1000] train loss: 132.05980, val loss: 144.90482\n",
      "[39/1000] train loss: 124.56440, val loss: 139.31673\n",
      "[40/1000] train loss: 118.96735, val loss: 134.11900\n",
      "[41/1000] train loss: 112.88076, val loss: 129.32040\n",
      "[42/1000] train loss: 106.91881, val loss: 124.91167\n",
      "[43/1000] train loss: 103.19195, val loss: 120.71027\n",
      "[44/1000] train loss: 95.64942, val loss: 116.93513\n",
      "[45/1000] train loss: 94.00289, val loss: 113.32394\n",
      "[46/1000] train loss: 89.68946, val loss: 109.93806\n",
      "[47/1000] train loss: 86.00298, val loss: 106.78872\n",
      "[48/1000] train loss: 81.77673, val loss: 103.86143\n",
      "[49/1000] train loss: 79.14735, val loss: 101.08900\n",
      "[50/1000] train loss: 76.01100, val loss: 98.45947\n",
      "[51/1000] train loss: 73.04960, val loss: 95.98785\n",
      "[52/1000] train loss: 70.08507, val loss: 93.65334\n",
      "[53/1000] train loss: 65.05212, val loss: 91.48329\n",
      "[54/1000] train loss: 65.53206, val loss: 89.35059\n",
      "[55/1000] train loss: 63.14630, val loss: 87.30978\n",
      "[56/1000] train loss: 61.38455, val loss: 85.41808\n",
      "[57/1000] train loss: 59.28687, val loss: 83.62177\n",
      "[58/1000] train loss: 57.10672, val loss: 81.92748\n",
      "[59/1000] train loss: 55.42293, val loss: 80.28802\n",
      "[60/1000] train loss: 53.60099, val loss: 78.71456\n",
      "[61/1000] train loss: 49.56720, val loss: 77.24380\n",
      "[62/1000] train loss: 50.28346, val loss: 75.69942\n",
      "[63/1000] train loss: 47.28542, val loss: 74.24863\n",
      "[64/1000] train loss: 47.40870, val loss: 72.83549\n",
      "[65/1000] train loss: 45.98719, val loss: 71.46269\n",
      "[66/1000] train loss: 43.60907, val loss: 70.25201\n",
      "[67/1000] train loss: 43.64853, val loss: 69.03398\n",
      "[68/1000] train loss: 41.93509, val loss: 67.89504\n",
      "[69/1000] train loss: 41.34433, val loss: 66.77949\n",
      "[70/1000] train loss: 40.04880, val loss: 65.70341\n",
      "[71/1000] train loss: 38.81893, val loss: 64.73434\n",
      "[72/1000] train loss: 38.34298, val loss: 63.77055\n",
      "[73/1000] train loss: 37.39608, val loss: 62.85468\n",
      "[74/1000] train loss: 36.09349, val loss: 61.95921\n",
      "[75/1000] train loss: 35.71299, val loss: 61.10288\n",
      "[76/1000] train loss: 34.58481, val loss: 60.30463\n",
      "[77/1000] train loss: 34.17209, val loss: 59.54082\n",
      "[78/1000] train loss: 33.60917, val loss: 58.77293\n",
      "[79/1000] train loss: 32.56846, val loss: 58.05486\n",
      "[80/1000] train loss: 31.90950, val loss: 57.33314\n",
      "[81/1000] train loss: 31.31500, val loss: 56.69246\n",
      "[82/1000] train loss: 30.87609, val loss: 56.04202\n",
      "[83/1000] train loss: 30.46867, val loss: 55.41656\n",
      "[84/1000] train loss: 29.85958, val loss: 54.81738\n",
      "[85/1000] train loss: 29.57012, val loss: 54.23070\n",
      "[86/1000] train loss: 28.95261, val loss: 53.61811\n",
      "[87/1000] train loss: 28.43888, val loss: 53.05193\n",
      "[88/1000] train loss: 27.97784, val loss: 52.49604\n",
      "[89/1000] train loss: 27.56157, val loss: 51.98011\n",
      "[90/1000] train loss: 27.21108, val loss: 51.45367\n",
      "[91/1000] train loss: 26.88984, val loss: 50.95412\n",
      "[92/1000] train loss: 25.67509, val loss: 50.47048\n",
      "[93/1000] train loss: 26.13541, val loss: 50.02700\n",
      "[94/1000] train loss: 25.92374, val loss: 49.54753\n",
      "[95/1000] train loss: 25.62069, val loss: 49.09598\n",
      "[96/1000] train loss: 23.23847, val loss: 48.80254\n",
      "[97/1000] train loss: 24.76808, val loss: 48.37637\n",
      "[98/1000] train loss: 24.66081, val loss: 47.91581\n",
      "[99/1000] train loss: 24.28041, val loss: 47.49018\n",
      "[100/1000] train loss: 24.20727, val loss: 47.10630\n",
      "[101/1000] train loss: 23.95191, val loss: 46.69148\n",
      "[102/1000] train loss: 23.59808, val loss: 46.25690\n",
      "[103/1000] train loss: 23.46990, val loss: 45.87302\n",
      "[104/1000] train loss: 23.06732, val loss: 45.52484\n",
      "[105/1000] train loss: 22.97179, val loss: 45.20013\n",
      "[106/1000] train loss: 22.80039, val loss: 44.88390\n",
      "[107/1000] train loss: 22.46075, val loss: 44.53638\n",
      "[108/1000] train loss: 22.44215, val loss: 44.20955\n",
      "[109/1000] train loss: 21.95626, val loss: 43.92160\n",
      "[110/1000] train loss: 21.60365, val loss: 43.63688\n",
      "[111/1000] train loss: 21.36169, val loss: 43.29761\n",
      "[112/1000] train loss: 21.66782, val loss: 43.02995\n",
      "[113/1000] train loss: 21.46171, val loss: 42.74675\n",
      "[114/1000] train loss: 21.11337, val loss: 42.43841\n",
      "[115/1000] train loss: 21.24596, val loss: 42.18327\n",
      "[116/1000] train loss: 20.70803, val loss: 41.93864\n",
      "[117/1000] train loss: 20.90133, val loss: 41.64139\n",
      "[118/1000] train loss: 18.96504, val loss: 41.51876\n",
      "[119/1000] train loss: 20.48494, val loss: 41.22355\n",
      "[120/1000] train loss: 20.50382, val loss: 40.94671\n",
      "[121/1000] train loss: 20.15074, val loss: 40.63837\n",
      "[122/1000] train loss: 20.09080, val loss: 40.37399\n",
      "[123/1000] train loss: 20.02295, val loss: 40.09501\n",
      "[124/1000] train loss: 19.89075, val loss: 39.85528\n",
      "[125/1000] train loss: 19.80662, val loss: 39.65081\n",
      "[126/1000] train loss: 19.68623, val loss: 39.44688\n",
      "[127/1000] train loss: 19.53397, val loss: 39.18604\n",
      "[128/1000] train loss: 19.37829, val loss: 38.99076\n",
      "[129/1000] train loss: 19.08894, val loss: 38.74807\n",
      "[130/1000] train loss: 19.05601, val loss: 38.50860\n",
      "[131/1000] train loss: 18.77457, val loss: 38.29783\n",
      "[132/1000] train loss: 18.93945, val loss: 38.04119\n",
      "[133/1000] train loss: 18.68240, val loss: 37.85474\n",
      "[134/1000] train loss: 18.56606, val loss: 37.65210\n",
      "[135/1000] train loss: 18.61533, val loss: 37.44573\n",
      "[136/1000] train loss: 18.40655, val loss: 37.22993\n",
      "[137/1000] train loss: 18.42374, val loss: 37.06831\n",
      "[138/1000] train loss: 18.21906, val loss: 36.93315\n",
      "[139/1000] train loss: 17.89433, val loss: 36.76494\n",
      "[140/1000] train loss: 18.01266, val loss: 36.57037\n",
      "[141/1000] train loss: 17.65322, val loss: 36.34428\n",
      "[142/1000] train loss: 17.86106, val loss: 36.20615\n",
      "[143/1000] train loss: 17.70987, val loss: 36.01036\n",
      "[144/1000] train loss: 17.40926, val loss: 35.86336\n",
      "[145/1000] train loss: 17.41410, val loss: 35.67381\n",
      "[146/1000] train loss: 17.17671, val loss: 35.46183\n",
      "[147/1000] train loss: 17.30713, val loss: 35.30429\n",
      "[148/1000] train loss: 17.19514, val loss: 35.13166\n",
      "[149/1000] train loss: 17.16928, val loss: 34.94283\n",
      "[150/1000] train loss: 17.08226, val loss: 34.79747\n",
      "[151/1000] train loss: 16.48721, val loss: 34.56900\n",
      "[152/1000] train loss: 16.97955, val loss: 34.38063\n",
      "[153/1000] train loss: 16.72632, val loss: 34.18617\n",
      "[154/1000] train loss: 16.64289, val loss: 34.06793\n",
      "[155/1000] train loss: 16.57964, val loss: 33.92138\n",
      "[156/1000] train loss: 16.42066, val loss: 33.83148\n",
      "[157/1000] train loss: 16.19176, val loss: 33.64802\n",
      "[158/1000] train loss: 16.31036, val loss: 33.49192\n",
      "[159/1000] train loss: 15.98592, val loss: 33.37859\n",
      "[160/1000] train loss: 16.18191, val loss: 33.23966\n",
      "[161/1000] train loss: 16.09659, val loss: 33.05972\n",
      "[162/1000] train loss: 15.63031, val loss: 32.95806\n",
      "[163/1000] train loss: 15.69392, val loss: 32.85466\n",
      "[164/1000] train loss: 15.85941, val loss: 32.74502\n",
      "[165/1000] train loss: 15.69601, val loss: 32.60904\n",
      "[166/1000] train loss: 15.67155, val loss: 32.39585\n",
      "[167/1000] train loss: 15.51041, val loss: 32.33676\n",
      "[168/1000] train loss: 15.08885, val loss: 32.11640\n",
      "[169/1000] train loss: 15.32249, val loss: 32.04140\n",
      "[170/1000] train loss: 15.28005, val loss: 31.93475\n",
      "[171/1000] train loss: 15.30472, val loss: 31.77993\n",
      "[172/1000] train loss: 15.15501, val loss: 31.67103\n",
      "[173/1000] train loss: 15.10671, val loss: 31.55642\n",
      "[174/1000] train loss: 14.72147, val loss: 31.53170\n",
      "[175/1000] train loss: 14.90168, val loss: 31.39972\n",
      "[176/1000] train loss: 14.78180, val loss: 31.21549\n",
      "[177/1000] train loss: 14.43297, val loss: 31.09301\n",
      "[178/1000] train loss: 14.70107, val loss: 31.02994\n",
      "[179/1000] train loss: 14.68054, val loss: 30.87478\n",
      "[180/1000] train loss: 14.57710, val loss: 30.77544\n",
      "[181/1000] train loss: 14.25503, val loss: 30.64750\n",
      "[182/1000] train loss: 14.43558, val loss: 30.48214\n",
      "[183/1000] train loss: 14.34798, val loss: 30.40458\n",
      "[184/1000] train loss: 14.32441, val loss: 30.29029\n",
      "[185/1000] train loss: 14.18427, val loss: 30.15951\n",
      "[186/1000] train loss: 14.07807, val loss: 30.03232\n",
      "[187/1000] train loss: 14.07722, val loss: 29.92075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188/1000] train loss: 14.03581, val loss: 29.85709\n",
      "[189/1000] train loss: 13.43996, val loss: 29.75305\n",
      "[190/1000] train loss: 13.95107, val loss: 29.69041\n",
      "[191/1000] train loss: 13.42567, val loss: 29.68712\n",
      "[192/1000] train loss: 13.74028, val loss: 29.57350\n",
      "[193/1000] train loss: 13.67047, val loss: 29.48989\n",
      "[194/1000] train loss: 13.55108, val loss: 29.29179\n",
      "[195/1000] train loss: 13.52221, val loss: 29.23738\n",
      "[196/1000] train loss: 13.50324, val loss: 29.07234\n",
      "[197/1000] train loss: 13.39694, val loss: 29.00282\n",
      "[198/1000] train loss: 13.38988, val loss: 28.90586\n",
      "[199/1000] train loss: 13.27199, val loss: 28.76713\n",
      "[200/1000] train loss: 13.24864, val loss: 28.72983\n",
      "[201/1000] train loss: 13.17902, val loss: 28.60137\n",
      "[202/1000] train loss: 13.08836, val loss: 28.50629\n",
      "[203/1000] train loss: 13.09205, val loss: 28.39671\n",
      "[204/1000] train loss: 13.07368, val loss: 28.25850\n",
      "[205/1000] train loss: 12.96353, val loss: 28.17633\n",
      "[206/1000] train loss: 12.82444, val loss: 28.03016\n",
      "[207/1000] train loss: 12.67687, val loss: 27.98951\n",
      "[208/1000] train loss: 12.70986, val loss: 27.87662\n",
      "[209/1000] train loss: 12.66675, val loss: 27.80449\n",
      "[210/1000] train loss: 12.63183, val loss: 27.78686\n",
      "[211/1000] train loss: 12.71218, val loss: 27.69827\n",
      "[212/1000] train loss: 12.36597, val loss: 27.55536\n",
      "[213/1000] train loss: 12.62513, val loss: 27.46457\n",
      "[214/1000] train loss: 12.50241, val loss: 27.46326\n",
      "[215/1000] train loss: 12.41280, val loss: 27.34576\n",
      "[216/1000] train loss: 12.26592, val loss: 27.28527\n",
      "[217/1000] train loss: 12.24279, val loss: 27.10080\n",
      "[218/1000] train loss: 12.25381, val loss: 27.10903\n",
      "[219/1000] train loss: 12.18726, val loss: 27.02219\n",
      "[220/1000] train loss: 12.14659, val loss: 26.97163\n",
      "[221/1000] train loss: 12.02293, val loss: 27.00105\n",
      "[222/1000] train loss: 12.01487, val loss: 26.86224\n",
      "[223/1000] train loss: 12.00609, val loss: 26.68814\n",
      "[224/1000] train loss: 11.93788, val loss: 26.60067\n",
      "[225/1000] train loss: 11.92581, val loss: 26.58886\n",
      "[226/1000] train loss: 11.86964, val loss: 26.47900\n",
      "[227/1000] train loss: 11.73508, val loss: 26.38459\n",
      "[228/1000] train loss: 11.69593, val loss: 26.31735\n",
      "[229/1000] train loss: 11.74489, val loss: 26.20397\n",
      "[230/1000] train loss: 11.64393, val loss: 26.24422\n",
      "[231/1000] train loss: 11.55100, val loss: 26.04883\n",
      "[232/1000] train loss: 11.55110, val loss: 25.99339\n",
      "[233/1000] train loss: 11.60232, val loss: 26.01097\n",
      "[234/1000] train loss: 11.59488, val loss: 25.82929\n",
      "[235/1000] train loss: 11.21060, val loss: 25.77105\n",
      "[236/1000] train loss: 11.39755, val loss: 25.68769\n",
      "[237/1000] train loss: 11.14070, val loss: 25.64801\n",
      "[238/1000] train loss: 11.28085, val loss: 25.51578\n",
      "[239/1000] train loss: 11.26766, val loss: 25.43241\n",
      "[240/1000] train loss: 11.16425, val loss: 25.39799\n",
      "[241/1000] train loss: 10.99222, val loss: 25.25740\n",
      "[242/1000] train loss: 11.05197, val loss: 25.28619\n",
      "[243/1000] train loss: 11.01592, val loss: 25.28972\n",
      "[244/1000] train loss: 10.97615, val loss: 25.14513\n",
      "[245/1000] train loss: 11.01981, val loss: 25.20477\n",
      "[246/1000] train loss: 10.82013, val loss: 25.06525\n",
      "[247/1000] train loss: 10.88403, val loss: 24.95768\n",
      "[248/1000] train loss: 10.76104, val loss: 24.91826\n",
      "[249/1000] train loss: 10.78041, val loss: 24.95745\n",
      "[250/1000] train loss: 10.80693, val loss: 24.92653\n",
      "[251/1000] train loss: 10.66562, val loss: 24.73041\n",
      "[252/1000] train loss: 10.59292, val loss: 24.67128\n",
      "[253/1000] train loss: 10.59006, val loss: 24.62319\n",
      "[254/1000] train loss: 10.56822, val loss: 24.53422\n",
      "[255/1000] train loss: 10.54556, val loss: 24.53687\n",
      "[256/1000] train loss: 10.49152, val loss: 24.41533\n",
      "[257/1000] train loss: 10.57498, val loss: 24.26634\n",
      "[258/1000] train loss: 10.42486, val loss: 24.22130\n",
      "[259/1000] train loss: 10.31366, val loss: 24.26229\n",
      "[260/1000] train loss: 10.28528, val loss: 24.22242\n",
      "[261/1000] train loss: 10.30132, val loss: 24.21204\n",
      "[262/1000] train loss: 10.24222, val loss: 24.01513\n",
      "[263/1000] train loss: 10.19821, val loss: 23.91505\n",
      "[264/1000] train loss: 10.14103, val loss: 23.94556\n",
      "[265/1000] train loss: 10.12702, val loss: 23.88242\n",
      "[266/1000] train loss: 9.66882, val loss: 23.81281\n",
      "[267/1000] train loss: 10.07026, val loss: 23.85555\n",
      "[268/1000] train loss: 10.02216, val loss: 23.73909\n",
      "[269/1000] train loss: 9.85561, val loss: 23.72317\n",
      "[270/1000] train loss: 9.98178, val loss: 23.76594\n",
      "[271/1000] train loss: 9.96686, val loss: 23.53141\n",
      "[272/1000] train loss: 9.96854, val loss: 23.44014\n",
      "[273/1000] train loss: 9.87528, val loss: 23.52809\n",
      "[274/1000] train loss: 9.82330, val loss: 23.47962\n",
      "[275/1000] train loss: 9.72203, val loss: 23.35970\n",
      "[276/1000] train loss: 9.76649, val loss: 23.26747\n",
      "[277/1000] train loss: 9.69699, val loss: 23.24095\n",
      "[278/1000] train loss: 9.65801, val loss: 23.11929\n",
      "[279/1000] train loss: 9.59102, val loss: 23.20731\n",
      "[280/1000] train loss: 9.58087, val loss: 23.16298\n",
      "[281/1000] train loss: 9.62241, val loss: 23.29539\n",
      "[282/1000] train loss: 9.33344, val loss: 23.24485\n",
      "[283/1000] train loss: 9.50953, val loss: 23.03555\n",
      "[284/1000] train loss: 9.41317, val loss: 23.08447\n",
      "[285/1000] train loss: 9.44688, val loss: 23.13357\n",
      "[286/1000] train loss: 9.42465, val loss: 23.04274\n",
      "[287/1000] train loss: 9.36814, val loss: 22.85034\n",
      "[288/1000] train loss: 9.24898, val loss: 22.84921\n",
      "[289/1000] train loss: 9.30204, val loss: 22.83014\n",
      "[290/1000] train loss: 9.32965, val loss: 22.94359\n",
      "[291/1000] train loss: 9.28044, val loss: 22.78734\n",
      "[292/1000] train loss: 9.26466, val loss: 22.65631\n",
      "[293/1000] train loss: 9.22884, val loss: 22.75187\n",
      "[294/1000] train loss: 9.08291, val loss: 22.63802\n",
      "[295/1000] train loss: 9.13667, val loss: 22.59308\n",
      "[296/1000] train loss: 9.13111, val loss: 22.60541\n",
      "[297/1000] train loss: 9.05537, val loss: 22.56880\n",
      "[298/1000] train loss: 9.03827, val loss: 22.49344\n",
      "[299/1000] train loss: 9.05668, val loss: 22.41800\n",
      "[300/1000] train loss: 9.05271, val loss: 22.32274\n",
      "[301/1000] train loss: 9.00753, val loss: 22.27941\n",
      "[302/1000] train loss: 8.89554, val loss: 22.37279\n",
      "[303/1000] train loss: 8.91528, val loss: 22.30659\n",
      "[304/1000] train loss: 8.87551, val loss: 22.20521\n",
      "[305/1000] train loss: 8.85374, val loss: 22.26135\n",
      "[306/1000] train loss: 8.80971, val loss: 22.10411\n",
      "[307/1000] train loss: 8.80294, val loss: 22.11705\n",
      "[308/1000] train loss: 8.61108, val loss: 22.15240\n",
      "[309/1000] train loss: 8.78421, val loss: 22.13099\n",
      "[310/1000] train loss: 8.73566, val loss: 22.16265\n",
      "[311/1000] train loss: 8.62865, val loss: 22.02986\n",
      "[312/1000] train loss: 8.66538, val loss: 21.90084\n",
      "[313/1000] train loss: 8.74907, val loss: 21.93768\n",
      "[314/1000] train loss: 8.63366, val loss: 22.00946\n",
      "[315/1000] train loss: 8.66442, val loss: 21.78962\n",
      "[316/1000] train loss: 8.09188, val loss: 21.69755\n",
      "[317/1000] train loss: 8.65903, val loss: 21.85656\n",
      "[318/1000] train loss: 8.50888, val loss: 21.83022\n",
      "[319/1000] train loss: 8.55217, val loss: 21.84654\n",
      "[320/1000] train loss: 8.33702, val loss: 21.78170\n",
      "[321/1000] train loss: 8.53318, val loss: 21.78749\n",
      "[322/1000] train loss: 8.46816, val loss: 21.80314\n",
      "[323/1000] train loss: 7.84886, val loss: 21.81380\n",
      "[324/1000] train loss: 8.42308, val loss: 21.83437\n",
      "[325/1000] train loss: 8.33379, val loss: 21.62877\n",
      "[326/1000] train loss: 8.28043, val loss: 21.58463\n",
      "[327/1000] train loss: 8.36665, val loss: 21.40585\n",
      "[328/1000] train loss: 8.34686, val loss: 21.56573\n",
      "[329/1000] train loss: 8.27344, val loss: 21.64790\n",
      "[330/1000] train loss: 8.22656, val loss: 21.50201\n",
      "[331/1000] train loss: 8.34385, val loss: 21.38845\n",
      "[332/1000] train loss: 8.25229, val loss: 21.52815\n",
      "[333/1000] train loss: 8.25823, val loss: 21.39484\n",
      "[334/1000] train loss: 8.20572, val loss: 21.29698\n",
      "[335/1000] train loss: 8.18234, val loss: 21.31812\n",
      "[336/1000] train loss: 8.24981, val loss: 21.10219\n",
      "[337/1000] train loss: 8.07305, val loss: 21.41368\n",
      "[338/1000] train loss: 8.07861, val loss: 21.38037\n",
      "[339/1000] train loss: 8.03853, val loss: 21.29380\n",
      "[340/1000] train loss: 8.02716, val loss: 21.34398\n",
      "[341/1000] train loss: 8.07452, val loss: 21.50116\n",
      "[342/1000] train loss: 8.08144, val loss: 21.35050\n",
      "[343/1000] train loss: 7.80700, val loss: 21.28852\n",
      "[344/1000] train loss: 7.96274, val loss: 21.21440\n",
      "[345/1000] train loss: 7.92839, val loss: 21.26912\n",
      "[346/1000] train loss: 7.93607, val loss: 21.08392\n",
      "[347/1000] train loss: 7.91948, val loss: 20.88797\n",
      "[348/1000] train loss: 7.95258, val loss: 20.96720\n",
      "[349/1000] train loss: 7.93101, val loss: 21.01478\n",
      "[350/1000] train loss: 8.01045, val loss: 21.13121\n",
      "[351/1000] train loss: 7.85057, val loss: 21.01485\n",
      "[352/1000] train loss: 7.82201, val loss: 20.77231\n",
      "[353/1000] train loss: 7.88409, val loss: 21.05342\n",
      "[354/1000] train loss: 7.87889, val loss: 21.10302\n",
      "[355/1000] train loss: 7.73251, val loss: 20.87316\n",
      "[356/1000] train loss: 7.76127, val loss: 20.96920\n",
      "[357/1000] train loss: 7.77721, val loss: 20.92745\n",
      "[358/1000] train loss: 7.77487, val loss: 20.80115\n",
      "[359/1000] train loss: 7.70211, val loss: 20.89678\n",
      "[360/1000] train loss: 7.78552, val loss: 20.73849\n",
      "[361/1000] train loss: 7.72262, val loss: 20.69732\n",
      "[362/1000] train loss: 7.71013, val loss: 20.66927\n",
      "[363/1000] train loss: 7.63990, val loss: 20.65397\n",
      "[364/1000] train loss: 7.68150, val loss: 20.74825\n",
      "[365/1000] train loss: 7.69686, val loss: 20.74300\n",
      "[366/1000] train loss: 7.67845, val loss: 20.63718\n",
      "[367/1000] train loss: 7.64220, val loss: 20.77313\n",
      "[368/1000] train loss: 7.59205, val loss: 20.68545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369/1000] train loss: 7.60804, val loss: 20.66871\n",
      "[370/1000] train loss: 7.54912, val loss: 20.60338\n",
      "[371/1000] train loss: 7.62600, val loss: 20.82917\n",
      "[372/1000] train loss: 7.58908, val loss: 20.41633\n",
      "[373/1000] train loss: 7.51596, val loss: 20.56753\n",
      "[374/1000] train loss: 7.54869, val loss: 20.52883\n",
      "[375/1000] train loss: 7.57290, val loss: 20.34956\n",
      "[376/1000] train loss: 7.47508, val loss: 20.49363\n",
      "[377/1000] train loss: 7.24138, val loss: 20.58097\n",
      "[378/1000] train loss: 7.43049, val loss: 20.48440\n",
      "[379/1000] train loss: 7.44113, val loss: 20.34090\n",
      "[380/1000] train loss: 7.43820, val loss: 20.29401\n",
      "[381/1000] train loss: 7.47553, val loss: 20.45416\n",
      "[382/1000] train loss: 7.42168, val loss: 20.62436\n",
      "[383/1000] train loss: 7.42469, val loss: 20.37425\n",
      "[384/1000] train loss: 7.49128, val loss: 20.38163\n",
      "[385/1000] train loss: 7.48760, val loss: 20.17632\n",
      "[386/1000] train loss: 7.30771, val loss: 20.30191\n",
      "[387/1000] train loss: 7.26711, val loss: 20.40251\n",
      "[388/1000] train loss: 7.39609, val loss: 20.36347\n",
      "[389/1000] train loss: 7.26633, val loss: 20.27132\n",
      "[390/1000] train loss: 7.31863, val loss: 20.37713\n",
      "[391/1000] train loss: 7.27064, val loss: 20.20066\n",
      "[392/1000] train loss: 7.28867, val loss: 20.37342\n",
      "[393/1000] train loss: 7.23167, val loss: 20.22309\n",
      "[394/1000] train loss: 7.20246, val loss: 20.37081\n",
      "[395/1000] train loss: 7.27257, val loss: 20.41336\n",
      "[396/1000] train loss: 7.25073, val loss: 20.31857\n",
      "[397/1000] train loss: 7.25477, val loss: 20.16534\n",
      "[398/1000] train loss: 7.17311, val loss: 20.08847\n",
      "[399/1000] train loss: 7.19676, val loss: 20.17995\n",
      "[400/1000] train loss: 7.23384, val loss: 20.25499\n",
      "[401/1000] train loss: 7.09945, val loss: 20.23890\n",
      "[402/1000] train loss: 7.15184, val loss: 20.03019\n",
      "[403/1000] train loss: 7.14391, val loss: 20.17805\n",
      "[404/1000] train loss: 7.09341, val loss: 20.07927\n",
      "[405/1000] train loss: 7.15191, val loss: 20.04379\n",
      "[406/1000] train loss: 7.10964, val loss: 19.92924\n",
      "[407/1000] train loss: 6.94126, val loss: 20.22589\n",
      "[408/1000] train loss: 7.09306, val loss: 20.03697\n",
      "[409/1000] train loss: 7.09890, val loss: 20.05600\n",
      "[410/1000] train loss: 7.15921, val loss: 19.82458\n",
      "[411/1000] train loss: 7.13786, val loss: 19.98990\n",
      "[412/1000] train loss: 7.03779, val loss: 19.93831\n",
      "[413/1000] train loss: 7.01910, val loss: 20.12156\n",
      "[414/1000] train loss: 6.94541, val loss: 19.87494\n",
      "[415/1000] train loss: 6.95963, val loss: 19.92639\n",
      "[416/1000] train loss: 7.00870, val loss: 19.92414\n",
      "[417/1000] train loss: 6.97601, val loss: 20.02238\n",
      "[418/1000] train loss: 6.97840, val loss: 19.94024\n",
      "[419/1000] train loss: 6.79651, val loss: 19.91020\n",
      "[420/1000] train loss: 6.90743, val loss: 19.78201\n",
      "[421/1000] train loss: 6.89622, val loss: 20.03865\n",
      "[422/1000] train loss: 6.94411, val loss: 19.79696\n",
      "[423/1000] train loss: 6.80466, val loss: 19.82132\n",
      "[424/1000] train loss: 6.88547, val loss: 19.76959\n",
      "[425/1000] train loss: 6.93294, val loss: 19.86389\n",
      "[426/1000] train loss: 6.90907, val loss: 19.92383\n",
      "[427/1000] train loss: 6.83386, val loss: 19.58194\n",
      "[428/1000] train loss: 6.85324, val loss: 19.76365\n",
      "[429/1000] train loss: 6.87000, val loss: 19.64162\n",
      "[430/1000] train loss: 6.80416, val loss: 19.84632\n",
      "[431/1000] train loss: 6.80403, val loss: 19.73771\n",
      "[432/1000] train loss: 6.84377, val loss: 19.49700\n",
      "[433/1000] train loss: 6.84449, val loss: 19.68327\n",
      "[434/1000] train loss: 6.81585, val loss: 19.44563\n",
      "[435/1000] train loss: 6.95308, val loss: 19.51394\n",
      "[436/1000] train loss: 6.81279, val loss: 19.79251\n",
      "[437/1000] train loss: 6.72095, val loss: 19.55271\n",
      "[438/1000] train loss: 6.77447, val loss: 19.71077\n",
      "[439/1000] train loss: 6.75611, val loss: 19.70464\n",
      "[440/1000] train loss: 6.74123, val loss: 19.57368\n",
      "[441/1000] train loss: 6.76719, val loss: 19.53972\n",
      "[442/1000] train loss: 6.68904, val loss: 19.45322\n",
      "[443/1000] train loss: 6.67352, val loss: 19.77830\n",
      "[444/1000] train loss: 6.65000, val loss: 19.56918\n",
      "[445/1000] train loss: 6.68788, val loss: 19.51771\n",
      "[446/1000] train loss: 6.62089, val loss: 19.43771\n",
      "[447/1000] train loss: 6.67991, val loss: 19.39404\n",
      "[448/1000] train loss: 6.63094, val loss: 19.37729\n",
      "[449/1000] train loss: 6.65150, val loss: 19.42646\n",
      "[450/1000] train loss: 6.66067, val loss: 19.58886\n",
      "[451/1000] train loss: 6.59522, val loss: 19.53298\n",
      "[452/1000] train loss: 6.64951, val loss: 19.36420\n",
      "[453/1000] train loss: 6.62371, val loss: 19.22954\n",
      "[454/1000] train loss: 6.67106, val loss: 19.47205\n",
      "[455/1000] train loss: 6.53524, val loss: 19.33268\n",
      "[456/1000] train loss: 6.61441, val loss: 19.16536\n",
      "[457/1000] train loss: 6.57780, val loss: 19.30242\n",
      "[458/1000] train loss: 6.52554, val loss: 19.17310\n",
      "[459/1000] train loss: 6.18849, val loss: 19.51669\n",
      "[460/1000] train loss: 6.58234, val loss: 19.44283\n",
      "[461/1000] train loss: 6.45329, val loss: 19.59193\n",
      "[462/1000] train loss: 6.51535, val loss: 19.30747\n",
      "[463/1000] train loss: 6.49609, val loss: 19.23093\n",
      "[464/1000] train loss: 6.52368, val loss: 19.19041\n",
      "[465/1000] train loss: 6.46662, val loss: 19.44311\n",
      "[466/1000] train loss: 6.51840, val loss: 19.24915\n",
      "[467/1000] train loss: 6.48865, val loss: 19.30582\n",
      "[468/1000] train loss: 6.57864, val loss: 19.64767\n",
      "[469/1000] train loss: 6.48076, val loss: 19.46966\n",
      "[470/1000] train loss: 6.48291, val loss: 19.28246\n",
      "[471/1000] train loss: 6.48136, val loss: 19.30641\n",
      "[472/1000] train loss: 6.51481, val loss: 19.31156\n",
      "[473/1000] train loss: 6.49025, val loss: 19.38817\n",
      "[474/1000] train loss: 6.43089, val loss: 19.35324\n",
      "[475/1000] train loss: 6.32154, val loss: 19.14684\n",
      "[476/1000] train loss: 6.44815, val loss: 19.06578\n",
      "[477/1000] train loss: 6.43782, val loss: 19.38461\n",
      "[478/1000] train loss: 6.41522, val loss: 19.06935\n",
      "[479/1000] train loss: 6.40786, val loss: 19.28513\n",
      "[480/1000] train loss: 6.41170, val loss: 18.96939\n",
      "[481/1000] train loss: 6.39416, val loss: 19.13498\n",
      "[482/1000] train loss: 6.41609, val loss: 19.08166\n",
      "[483/1000] train loss: 6.36028, val loss: 19.03915\n",
      "[484/1000] train loss: 6.29811, val loss: 19.16630\n",
      "[485/1000] train loss: 6.38236, val loss: 19.25828\n",
      "[486/1000] train loss: 6.30281, val loss: 19.21240\n",
      "[487/1000] train loss: 6.19753, val loss: 19.13707\n",
      "[488/1000] train loss: 6.35924, val loss: 19.14866\n",
      "[489/1000] train loss: 6.31556, val loss: 19.25537\n",
      "[490/1000] train loss: 6.35345, val loss: 19.23904\n",
      "[491/1000] train loss: 6.38409, val loss: 19.25571\n",
      "[492/1000] train loss: 6.29234, val loss: 19.10909\n",
      "[493/1000] train loss: 6.23661, val loss: 19.04719\n",
      "[494/1000] train loss: 6.11127, val loss: 19.01125\n",
      "[495/1000] train loss: 6.31826, val loss: 19.05908\n",
      "[496/1000] train loss: 6.17820, val loss: 19.07319\n",
      "[497/1000] train loss: 6.26734, val loss: 19.14851\n",
      "[498/1000] train loss: 6.31817, val loss: 18.91395\n",
      "[499/1000] train loss: 6.30793, val loss: 19.17114\n",
      "[500/1000] train loss: 6.28133, val loss: 18.69771\n",
      "[501/1000] train loss: 6.24413, val loss: 18.79174\n",
      "[502/1000] train loss: 6.18240, val loss: 18.72801\n",
      "[503/1000] train loss: 6.22033, val loss: 18.73559\n",
      "[504/1000] train loss: 6.16849, val loss: 18.81396\n",
      "[505/1000] train loss: 6.17681, val loss: 18.79388\n",
      "[506/1000] train loss: 6.17504, val loss: 18.81786\n",
      "[507/1000] train loss: 6.13789, val loss: 19.05464\n",
      "[508/1000] train loss: 6.17453, val loss: 18.76212\n",
      "[509/1000] train loss: 6.01887, val loss: 18.74762\n",
      "[510/1000] train loss: 6.07165, val loss: 18.93732\n",
      "[511/1000] train loss: 6.15057, val loss: 18.83058\n",
      "[512/1000] train loss: 6.04625, val loss: 18.72140\n",
      "[513/1000] train loss: 6.09071, val loss: 18.56072\n",
      "[514/1000] train loss: 6.04398, val loss: 18.73556\n",
      "[515/1000] train loss: 6.13745, val loss: 18.89410\n",
      "[516/1000] train loss: 5.99383, val loss: 18.83020\n",
      "[517/1000] train loss: 6.14453, val loss: 19.07492\n",
      "[518/1000] train loss: 5.73336, val loss: 18.84731\n",
      "[519/1000] train loss: 5.93926, val loss: 18.90130\n",
      "[520/1000] train loss: 6.04397, val loss: 18.50026\n",
      "[521/1000] train loss: 5.96995, val loss: 18.87834\n",
      "[522/1000] train loss: 5.99996, val loss: 18.79562\n",
      "[523/1000] train loss: 6.07389, val loss: 18.98294\n",
      "[524/1000] train loss: 6.03577, val loss: 18.65989\n",
      "[525/1000] train loss: 6.15874, val loss: 18.79989\n",
      "[526/1000] train loss: 6.11912, val loss: 18.52369\n",
      "[527/1000] train loss: 5.94151, val loss: 18.70560\n",
      "[528/1000] train loss: 5.81692, val loss: 18.69548\n",
      "[529/1000] train loss: 5.94464, val loss: 18.65697\n",
      "[530/1000] train loss: 5.99517, val loss: 18.76291\n",
      "[531/1000] train loss: 5.93256, val loss: 18.76052\n",
      "[532/1000] train loss: 5.95298, val loss: 18.62511\n",
      "[533/1000] train loss: 5.98980, val loss: 18.44343\n",
      "[534/1000] train loss: 5.85027, val loss: 18.72544\n",
      "[535/1000] train loss: 5.91907, val loss: 18.53121\n",
      "[536/1000] train loss: 5.96005, val loss: 18.74119\n",
      "[537/1000] train loss: 5.98989, val loss: 18.59821\n",
      "[538/1000] train loss: 5.95148, val loss: 18.64515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[539/1000] train loss: 5.87677, val loss: 18.62009\n",
      "[540/1000] train loss: 5.92120, val loss: 18.64030\n",
      "[541/1000] train loss: 5.58653, val loss: 18.84785\n",
      "[542/1000] train loss: 5.82917, val loss: 18.63009\n",
      "[543/1000] train loss: 5.80379, val loss: 18.73122\n",
      "[544/1000] train loss: 5.79611, val loss: 18.79180\n",
      "[545/1000] train loss: 5.80332, val loss: 18.60736\n",
      "[546/1000] train loss: 5.78727, val loss: 18.72855\n",
      "[547/1000] train loss: 5.80943, val loss: 18.45576\n",
      "[548/1000] train loss: 5.82752, val loss: 18.43685\n",
      "[549/1000] train loss: 5.76200, val loss: 18.58054\n",
      "[550/1000] train loss: 5.69265, val loss: 18.64350\n",
      "[551/1000] train loss: 5.44366, val loss: 18.65419\n",
      "[552/1000] train loss: 5.74092, val loss: 18.56622\n",
      "[553/1000] train loss: 5.81909, val loss: 18.50211\n",
      "[554/1000] train loss: 5.89867, val loss: 18.33023\n",
      "[555/1000] train loss: 5.79794, val loss: 18.45414\n",
      "[556/1000] train loss: 5.90812, val loss: 18.82593\n",
      "[557/1000] train loss: 5.76738, val loss: 18.68765\n",
      "[558/1000] train loss: 5.78983, val loss: 18.53084\n",
      "[559/1000] train loss: 5.80548, val loss: 18.68179\n",
      "[560/1000] train loss: 5.75840, val loss: 18.54592\n",
      "[561/1000] train loss: 5.73430, val loss: 18.57501\n",
      "[562/1000] train loss: 5.69395, val loss: 18.23940\n",
      "[563/1000] train loss: 5.84572, val loss: 18.64063\n",
      "[564/1000] train loss: 5.69724, val loss: 18.48219\n",
      "[565/1000] train loss: 5.75633, val loss: 18.30762\n",
      "[566/1000] train loss: 5.74690, val loss: 18.54694\n",
      "[567/1000] train loss: 5.70953, val loss: 18.60600\n",
      "[568/1000] train loss: 5.70292, val loss: 18.25219\n",
      "[569/1000] train loss: 5.68621, val loss: 18.48127\n",
      "[570/1000] train loss: 5.68447, val loss: 18.20570\n",
      "[571/1000] train loss: 5.75694, val loss: 18.21968\n",
      "[572/1000] train loss: 5.73328, val loss: 18.25580\n",
      "[573/1000] train loss: 5.61140, val loss: 18.42394\n",
      "[574/1000] train loss: 5.57376, val loss: 18.37617\n",
      "[575/1000] train loss: 5.69676, val loss: 18.30122\n",
      "[576/1000] train loss: 5.67091, val loss: 18.43646\n",
      "[577/1000] train loss: 5.65234, val loss: 18.47188\n",
      "[578/1000] train loss: 5.72865, val loss: 18.07321\n",
      "[579/1000] train loss: 5.61027, val loss: 18.53030\n",
      "[580/1000] train loss: 5.59163, val loss: 18.36886\n",
      "[581/1000] train loss: 5.60146, val loss: 18.39516\n",
      "[582/1000] train loss: 5.75833, val loss: 18.50217\n",
      "[583/1000] train loss: 5.77710, val loss: 18.15526\n",
      "[584/1000] train loss: 5.60665, val loss: 18.30994\n",
      "[585/1000] train loss: 5.62777, val loss: 18.23713\n",
      "[586/1000] train loss: 5.55069, val loss: 18.19283\n",
      "[587/1000] train loss: 5.64157, val loss: 18.46090\n",
      "[588/1000] train loss: 5.61352, val loss: 18.38951\n",
      "[589/1000] train loss: 5.53459, val loss: 18.25416\n",
      "[590/1000] train loss: 5.58541, val loss: 18.28917\n",
      "[591/1000] train loss: 5.60321, val loss: 18.36921\n",
      "[592/1000] train loss: 5.47177, val loss: 18.43038\n",
      "[593/1000] train loss: 5.67374, val loss: 18.57062\n",
      "[594/1000] train loss: 5.58123, val loss: 18.28361\n",
      "[595/1000] train loss: 5.53795, val loss: 18.34734\n",
      "[596/1000] train loss: 5.53615, val loss: 18.27664\n",
      "[597/1000] train loss: 5.54485, val loss: 18.22588\n",
      "[598/1000] train loss: 5.50822, val loss: 18.16627\n",
      "[599/1000] train loss: 5.55533, val loss: 18.08992\n",
      "[600/1000] train loss: 5.53333, val loss: 18.06320\n",
      "[601/1000] train loss: 5.50083, val loss: 18.02261\n",
      "[602/1000] train loss: 5.56221, val loss: 17.92973\n",
      "[603/1000] train loss: 5.44555, val loss: 18.16269\n",
      "[604/1000] train loss: 5.46071, val loss: 18.18912\n",
      "[605/1000] train loss: 5.46491, val loss: 18.21912\n",
      "[606/1000] train loss: 5.42876, val loss: 18.17229\n",
      "[607/1000] train loss: 5.51864, val loss: 17.98678\n",
      "[608/1000] train loss: 5.41803, val loss: 18.04298\n",
      "[609/1000] train loss: 5.46283, val loss: 18.24244\n",
      "[610/1000] train loss: 5.42144, val loss: 18.03875\n",
      "[611/1000] train loss: 5.54043, val loss: 17.94562\n",
      "[612/1000] train loss: 5.54288, val loss: 18.29216\n",
      "[613/1000] train loss: 5.51640, val loss: 17.99017\n",
      "[614/1000] train loss: 5.48892, val loss: 18.16389\n",
      "[615/1000] train loss: 5.43279, val loss: 18.18306\n",
      "[616/1000] train loss: 5.41689, val loss: 18.04863\n",
      "[617/1000] train loss: 5.47754, val loss: 18.00767\n",
      "[618/1000] train loss: 5.43101, val loss: 18.27341\n",
      "[619/1000] train loss: 5.49991, val loss: 18.38128\n",
      "[620/1000] train loss: 5.24582, val loss: 18.06964\n",
      "[621/1000] train loss: 5.50066, val loss: 18.00127\n",
      "[622/1000] train loss: 5.41969, val loss: 17.98110\n",
      "[623/1000] train loss: 5.41185, val loss: 18.19435\n",
      "[624/1000] train loss: 5.41443, val loss: 18.00977\n",
      "[625/1000] train loss: 5.41846, val loss: 18.23511\n",
      "[626/1000] train loss: 5.44167, val loss: 17.82363\n",
      "[627/1000] train loss: 5.45010, val loss: 17.84623\n",
      "[628/1000] train loss: 5.38584, val loss: 18.07594\n",
      "[629/1000] train loss: 5.29906, val loss: 17.96131\n",
      "[630/1000] train loss: 5.37275, val loss: 17.92315\n",
      "[631/1000] train loss: 5.37177, val loss: 18.13328\n",
      "[632/1000] train loss: 5.45305, val loss: 18.25117\n",
      "[633/1000] train loss: 5.34387, val loss: 18.01389\n",
      "[634/1000] train loss: 5.30982, val loss: 17.92278\n",
      "[635/1000] train loss: 5.28205, val loss: 17.93253\n",
      "[636/1000] train loss: 5.30437, val loss: 17.98380\n",
      "[637/1000] train loss: 5.14297, val loss: 17.94767\n",
      "[638/1000] train loss: 5.33805, val loss: 18.25078\n",
      "[639/1000] train loss: 5.34973, val loss: 18.16250\n",
      "[640/1000] train loss: 5.26908, val loss: 17.90148\n",
      "[641/1000] train loss: 5.27317, val loss: 17.96685\n",
      "[642/1000] train loss: 5.31532, val loss: 17.90070\n",
      "[643/1000] train loss: 5.30160, val loss: 18.02154\n",
      "[644/1000] train loss: 5.35894, val loss: 18.05422\n",
      "[645/1000] train loss: 5.30263, val loss: 18.07302\n",
      "[646/1000] train loss: 5.26673, val loss: 17.91038\n",
      "[647/1000] train loss: 5.32628, val loss: 18.22505\n",
      "[648/1000] train loss: 5.30694, val loss: 17.86525\n",
      "[649/1000] train loss: 5.30474, val loss: 17.97499\n",
      "[650/1000] train loss: 5.17368, val loss: 17.84778\n",
      "[651/1000] train loss: 5.21672, val loss: 18.15296\n",
      "[652/1000] train loss: 5.32518, val loss: 18.15705\n",
      "[653/1000] train loss: 5.19082, val loss: 18.20649\n",
      "[654/1000] train loss: 5.22843, val loss: 17.86223\n",
      "[655/1000] train loss: 5.23859, val loss: 17.83887\n",
      "[656/1000] train loss: 5.26582, val loss: 17.90664\n",
      "[657/1000] train loss: 5.21724, val loss: 18.15151\n",
      "[658/1000] train loss: 5.20176, val loss: 17.98979\n",
      "[659/1000] train loss: 5.22956, val loss: 17.69622\n",
      "[660/1000] train loss: 5.21859, val loss: 17.80055\n",
      "[661/1000] train loss: 5.21023, val loss: 18.06671\n",
      "[662/1000] train loss: 5.19602, val loss: 17.99282\n",
      "[663/1000] train loss: 5.15620, val loss: 17.78642\n",
      "[664/1000] train loss: 5.18661, val loss: 17.85160\n",
      "[665/1000] train loss: 5.19643, val loss: 18.19052\n",
      "[666/1000] train loss: 5.37053, val loss: 18.18777\n",
      "[667/1000] train loss: 5.24583, val loss: 17.86997\n",
      "[668/1000] train loss: 5.13088, val loss: 17.77338\n",
      "[669/1000] train loss: 5.17771, val loss: 17.80004\n",
      "[670/1000] train loss: 5.08516, val loss: 17.97489\n",
      "[671/1000] train loss: 5.24016, val loss: 18.14329\n",
      "[672/1000] train loss: 5.24613, val loss: 18.17453\n",
      "[673/1000] train loss: 5.11863, val loss: 17.91268\n",
      "[674/1000] train loss: 5.14705, val loss: 18.11478\n",
      "[675/1000] train loss: 4.80755, val loss: 18.12479\n",
      "[676/1000] train loss: 5.06695, val loss: 17.85341\n",
      "[677/1000] train loss: 5.10593, val loss: 17.83432\n",
      "[678/1000] train loss: 5.11817, val loss: 17.77122\n",
      "[679/1000] train loss: 5.13732, val loss: 17.88854\n",
      "[680/1000] train loss: 5.12019, val loss: 17.84969\n",
      "[681/1000] train loss: 5.11592, val loss: 17.88625\n",
      "[682/1000] train loss: 5.01370, val loss: 17.82626\n",
      "[683/1000] train loss: 5.05537, val loss: 17.68132\n",
      "[684/1000] train loss: 5.08472, val loss: 18.01386\n",
      "[685/1000] train loss: 5.08636, val loss: 17.91498\n",
      "[686/1000] train loss: 5.01023, val loss: 17.90287\n",
      "[687/1000] train loss: 5.05623, val loss: 17.69408\n",
      "[688/1000] train loss: 5.06735, val loss: 17.86787\n",
      "[689/1000] train loss: 5.08040, val loss: 17.74174\n",
      "[690/1000] train loss: 5.07576, val loss: 17.94609\n",
      "[691/1000] train loss: 5.15728, val loss: 17.67277\n",
      "[692/1000] train loss: 4.99753, val loss: 17.90091\n",
      "[693/1000] train loss: 5.07293, val loss: 17.79246\n",
      "[694/1000] train loss: 5.10173, val loss: 17.63481\n",
      "[695/1000] train loss: 4.95682, val loss: 17.72771\n",
      "[696/1000] train loss: 5.02899, val loss: 17.62552\n",
      "[697/1000] train loss: 4.98485, val loss: 17.77989\n",
      "[698/1000] train loss: 4.91448, val loss: 17.92190\n",
      "[699/1000] train loss: 4.92826, val loss: 17.61358\n",
      "[700/1000] train loss: 5.04941, val loss: 17.86889\n",
      "[701/1000] train loss: 5.00443, val loss: 17.62248\n",
      "[702/1000] train loss: 4.99873, val loss: 17.75734\n",
      "[703/1000] train loss: 5.04836, val loss: 17.93784\n",
      "[704/1000] train loss: 4.98641, val loss: 17.94217\n",
      "[705/1000] train loss: 4.98358, val loss: 17.65630\n",
      "[706/1000] train loss: 4.94756, val loss: 17.82416\n",
      "[707/1000] train loss: 4.98278, val loss: 17.79272\n",
      "[708/1000] train loss: 5.01445, val loss: 17.79188\n",
      "[709/1000] train loss: 4.99022, val loss: 17.77203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[710/1000] train loss: 4.93367, val loss: 17.54746\n",
      "[711/1000] train loss: 4.95557, val loss: 17.91472\n",
      "[712/1000] train loss: 4.82547, val loss: 17.86888\n",
      "[713/1000] train loss: 4.88253, val loss: 17.55740\n",
      "[714/1000] train loss: 4.95272, val loss: 17.84825\n",
      "[715/1000] train loss: 5.03406, val loss: 17.60475\n",
      "[716/1000] train loss: 4.97201, val loss: 17.72867\n",
      "[717/1000] train loss: 4.92281, val loss: 17.84910\n",
      "[718/1000] train loss: 4.93288, val loss: 17.90830\n",
      "[719/1000] train loss: 4.98112, val loss: 17.80557\n",
      "[720/1000] train loss: 4.92837, val loss: 17.80976\n",
      "[721/1000] train loss: 5.02506, val loss: 18.15514\n",
      "[722/1000] train loss: 4.99658, val loss: 17.48354\n",
      "[723/1000] train loss: 5.00025, val loss: 17.58223\n",
      "[724/1000] train loss: 4.98544, val loss: 17.52023\n",
      "[725/1000] train loss: 4.87955, val loss: 17.66625\n",
      "[726/1000] train loss: 4.80672, val loss: 17.69999\n",
      "[727/1000] train loss: 4.89564, val loss: 17.67380\n",
      "[728/1000] train loss: 4.91108, val loss: 17.89026\n",
      "[729/1000] train loss: 4.87545, val loss: 17.65607\n",
      "[730/1000] train loss: 4.82215, val loss: 17.76332\n",
      "[731/1000] train loss: 4.86589, val loss: 17.78528\n",
      "[732/1000] train loss: 4.93674, val loss: 17.61440\n",
      "[733/1000] train loss: 4.89198, val loss: 17.48914\n",
      "[734/1000] train loss: 4.92297, val loss: 17.94012\n",
      "[735/1000] train loss: 4.84515, val loss: 17.81979\n",
      "[736/1000] train loss: 4.83260, val loss: 17.82316\n",
      "[737/1000] train loss: 4.83930, val loss: 17.80915\n",
      "[738/1000] train loss: 4.85810, val loss: 17.79885\n",
      "[739/1000] train loss: 4.84959, val loss: 17.70741\n",
      "[740/1000] train loss: 4.80120, val loss: 17.59663\n",
      "[741/1000] train loss: 4.83816, val loss: 17.69639\n",
      "[742/1000] train loss: 4.89915, val loss: 17.92085\n",
      "[743/1000] train loss: 4.83493, val loss: 17.62638\n",
      "[744/1000] train loss: 4.78751, val loss: 17.78695\n",
      "[745/1000] train loss: 4.81083, val loss: 17.68205\n",
      "[746/1000] train loss: 4.78127, val loss: 17.68538\n",
      "[747/1000] train loss: 4.80092, val loss: 17.46751\n",
      "[748/1000] train loss: 4.88378, val loss: 17.70602\n",
      "[749/1000] train loss: 4.81315, val loss: 17.87695\n",
      "[750/1000] train loss: 4.77725, val loss: 18.03201\n",
      "[751/1000] train loss: 4.83206, val loss: 17.60703\n",
      "[752/1000] train loss: 4.80567, val loss: 17.62939\n",
      "[753/1000] train loss: 4.80614, val loss: 17.88754\n",
      "[754/1000] train loss: 4.84336, val loss: 17.88457\n",
      "[755/1000] train loss: 4.76941, val loss: 17.70149\n",
      "[756/1000] train loss: 4.95352, val loss: 17.56896\n",
      "[757/1000] train loss: 4.83481, val loss: 17.62230\n",
      "[758/1000] train loss: 4.71708, val loss: 17.77273\n",
      "[759/1000] train loss: 4.72514, val loss: 17.73138\n",
      "[760/1000] train loss: 4.74121, val loss: 17.80836\n",
      "[761/1000] train loss: 4.73363, val loss: 17.69826\n",
      "[762/1000] train loss: 4.65590, val loss: 17.71832\n",
      "[763/1000] train loss: 4.73701, val loss: 17.56152\n",
      "[764/1000] train loss: 4.71358, val loss: 17.63234\n",
      "[765/1000] train loss: 4.73151, val loss: 17.78262\n",
      "[766/1000] train loss: 4.66236, val loss: 17.70266\n",
      "[767/1000] train loss: 4.81211, val loss: 17.40523\n",
      "[768/1000] train loss: 4.79122, val loss: 17.52001\n",
      "[769/1000] train loss: 4.71927, val loss: 17.79243\n",
      "[770/1000] train loss: 4.65496, val loss: 17.64997\n",
      "[771/1000] train loss: 4.73250, val loss: 17.64743\n",
      "[772/1000] train loss: 4.69519, val loss: 17.70371\n",
      "[773/1000] train loss: 4.71860, val loss: 17.79539\n",
      "[774/1000] train loss: 4.67921, val loss: 17.50995\n",
      "[775/1000] train loss: 4.65168, val loss: 17.68614\n",
      "[776/1000] train loss: 4.67261, val loss: 17.82836\n",
      "[777/1000] train loss: 4.72058, val loss: 17.94631\n",
      "[778/1000] train loss: 4.65010, val loss: 17.86185\n",
      "[779/1000] train loss: 4.66869, val loss: 17.62220\n",
      "[780/1000] train loss: 4.58634, val loss: 17.67006\n",
      "[781/1000] train loss: 4.71165, val loss: 17.46535\n",
      "[782/1000] train loss: 4.64286, val loss: 17.54884\n",
      "[783/1000] train loss: 4.62839, val loss: 17.66105\n",
      "[784/1000] train loss: 4.70628, val loss: 17.84343\n",
      "[785/1000] train loss: 4.62091, val loss: 17.78068\n",
      "[786/1000] train loss: 4.65456, val loss: 17.65153\n",
      "[787/1000] train loss: 4.48076, val loss: 17.56242\n",
      "[788/1000] train loss: 4.67885, val loss: 17.75738\n",
      "[789/1000] train loss: 4.60827, val loss: 17.81047\n",
      "[790/1000] train loss: 4.64711, val loss: 17.73442\n",
      "[791/1000] train loss: 4.64108, val loss: 17.53100\n",
      "[792/1000] train loss: 4.66791, val loss: 17.76111\n",
      "[793/1000] train loss: 4.66357, val loss: 17.70878\n",
      "[794/1000] train loss: 4.63239, val loss: 17.53627\n",
      "[795/1000] train loss: 4.66269, val loss: 17.85027\n",
      "[796/1000] train loss: 4.66921, val loss: 17.51970\n",
      "[797/1000] train loss: 4.64253, val loss: 17.67467\n",
      "[798/1000] train loss: 4.58386, val loss: 17.68568\n",
      "[799/1000] train loss: 4.44295, val loss: 17.79232\n",
      "[800/1000] train loss: 4.55233, val loss: 17.81133\n",
      "[801/1000] train loss: 4.42890, val loss: 17.84349\n",
      "[802/1000] train loss: 4.61964, val loss: 17.48864\n",
      "[803/1000] train loss: 4.60724, val loss: 17.82631\n",
      "[804/1000] train loss: 4.62820, val loss: 17.75220\n",
      "[805/1000] train loss: 4.62202, val loss: 17.70524\n",
      "[806/1000] train loss: 4.54754, val loss: 17.65514\n",
      "[807/1000] train loss: 4.63956, val loss: 17.50342\n",
      "[808/1000] train loss: 4.60069, val loss: 17.84945\n",
      "[809/1000] train loss: 4.60844, val loss: 17.49821\n",
      "[810/1000] train loss: 4.65997, val loss: 17.74426\n",
      "[811/1000] train loss: 4.54660, val loss: 17.59329\n",
      "[812/1000] train loss: 4.59782, val loss: 17.56271\n",
      "[813/1000] train loss: 4.60455, val loss: 17.55295\n",
      "[814/1000] train loss: 4.60113, val loss: 17.50947\n",
      "[815/1000] train loss: 4.60879, val loss: 17.80885\n",
      "[816/1000] train loss: 4.55947, val loss: 17.63306\n",
      "[817/1000] train loss: 4.36170, val loss: 17.75795\n",
      "[818/1000] train loss: 4.51820, val loss: 17.77651\n",
      "[819/1000] train loss: 4.48338, val loss: 17.68606\n",
      "[820/1000] train loss: 4.61932, val loss: 17.74327\n",
      "[821/1000] train loss: 4.56315, val loss: 17.69482\n",
      "[822/1000] train loss: 4.61104, val loss: 17.39098\n",
      "[823/1000] train loss: 4.56363, val loss: 17.82313\n",
      "[824/1000] train loss: 4.55921, val loss: 17.70696\n",
      "[825/1000] train loss: 4.54999, val loss: 17.55684\n",
      "[826/1000] train loss: 4.47698, val loss: 17.60564\n",
      "[827/1000] train loss: 4.52692, val loss: 17.70858\n",
      "[828/1000] train loss: 4.49216, val loss: 17.51486\n",
      "[829/1000] train loss: 4.60009, val loss: 17.40115\n",
      "[830/1000] train loss: 4.51128, val loss: 17.89579\n",
      "[831/1000] train loss: 4.49387, val loss: 17.44302\n",
      "[832/1000] train loss: 4.57470, val loss: 17.89956\n",
      "[833/1000] train loss: 4.51737, val loss: 17.36453\n",
      "[834/1000] train loss: 4.48506, val loss: 17.52345\n",
      "[835/1000] train loss: 4.41646, val loss: 17.53775\n",
      "[836/1000] train loss: 4.52305, val loss: 17.38843\n",
      "[837/1000] train loss: 4.45703, val loss: 17.59238\n",
      "[838/1000] train loss: 4.48203, val loss: 17.49628\n",
      "[839/1000] train loss: 4.49058, val loss: 17.56834\n",
      "[840/1000] train loss: 4.50168, val loss: 17.68667\n",
      "[841/1000] train loss: 4.51033, val loss: 17.37983\n",
      "[842/1000] train loss: 4.42695, val loss: 17.54416\n",
      "[843/1000] train loss: 4.46701, val loss: 17.72339\n",
      "[844/1000] train loss: 4.53360, val loss: 17.81060\n",
      "[845/1000] train loss: 4.45864, val loss: 17.42581\n",
      "[846/1000] train loss: 4.48461, val loss: 17.45023\n",
      "[847/1000] train loss: 4.52690, val loss: 17.94336\n",
      "[848/1000] train loss: 4.53263, val loss: 17.47811\n",
      "[849/1000] train loss: 4.42339, val loss: 17.27651\n",
      "[850/1000] train loss: 4.43722, val loss: 17.33469\n",
      "[851/1000] train loss: 4.50964, val loss: 17.41430\n",
      "[852/1000] train loss: 4.49311, val loss: 17.27442\n",
      "[853/1000] train loss: 4.38960, val loss: 17.64108\n",
      "[854/1000] train loss: 4.41898, val loss: 17.32981\n",
      "[855/1000] train loss: 4.46272, val loss: 17.31522\n",
      "[856/1000] train loss: 4.34495, val loss: 17.45226\n",
      "[857/1000] train loss: 4.36311, val loss: 17.43002\n",
      "[858/1000] train loss: 4.42484, val loss: 17.53046\n",
      "[859/1000] train loss: 4.44638, val loss: 17.38484\n",
      "[860/1000] train loss: 4.47641, val loss: 17.32433\n",
      "[861/1000] train loss: 4.45369, val loss: 17.75296\n",
      "[862/1000] train loss: 4.34855, val loss: 17.40249\n",
      "[863/1000] train loss: 4.43777, val loss: 17.38478\n",
      "[864/1000] train loss: 4.32405, val loss: 17.34369\n",
      "[865/1000] train loss: 4.35579, val loss: 17.56507\n",
      "[866/1000] train loss: 4.37021, val loss: 17.47236\n",
      "[867/1000] train loss: 4.39689, val loss: 17.34957\n",
      "[868/1000] train loss: 4.43813, val loss: 17.63183\n",
      "[869/1000] train loss: 4.41302, val loss: 17.45322\n",
      "[870/1000] train loss: 4.41509, val loss: 17.74134\n",
      "[871/1000] train loss: 4.43190, val loss: 17.83105\n",
      "[872/1000] train loss: 4.46603, val loss: 17.50732\n",
      "[873/1000] train loss: 4.35433, val loss: 17.32966\n",
      "[874/1000] train loss: 4.55901, val loss: 18.03333\n",
      "[875/1000] train loss: 4.50817, val loss: 17.70666\n",
      "[876/1000] train loss: 4.36448, val loss: 17.33658\n",
      "[877/1000] train loss: 4.37500, val loss: 17.46766\n",
      "[878/1000] train loss: 4.38032, val loss: 17.42952\n",
      "[879/1000] train loss: 4.37246, val loss: 17.30589\n",
      "[880/1000] train loss: 4.40628, val loss: 17.23524\n",
      "[881/1000] train loss: 4.36530, val loss: 17.58831\n",
      "[882/1000] train loss: 4.32307, val loss: 17.39831\n",
      "[883/1000] train loss: 4.35887, val loss: 17.53236\n",
      "[884/1000] train loss: 4.32846, val loss: 17.56370\n",
      "[885/1000] train loss: 4.28947, val loss: 17.47880\n",
      "[886/1000] train loss: 4.49070, val loss: 17.94684\n",
      "[887/1000] train loss: 4.34696, val loss: 17.50569\n",
      "[888/1000] train loss: 4.34121, val loss: 17.64598\n",
      "[889/1000] train loss: 4.31787, val loss: 17.47826\n",
      "[890/1000] train loss: 4.31914, val loss: 17.63145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[891/1000] train loss: 4.38095, val loss: 17.55646\n",
      "[892/1000] train loss: 4.34498, val loss: 17.37203\n",
      "[893/1000] train loss: 4.32068, val loss: 17.44959\n",
      "[894/1000] train loss: 4.34863, val loss: 17.59552\n",
      "[895/1000] train loss: 4.37120, val loss: 17.32354\n",
      "[896/1000] train loss: 4.33419, val loss: 17.32356\n",
      "[897/1000] train loss: 4.37318, val loss: 17.46570\n",
      "[898/1000] train loss: 4.26696, val loss: 17.63577\n",
      "[899/1000] train loss: 4.25540, val loss: 17.56383\n",
      "[900/1000] train loss: 4.30632, val loss: 17.27955\n",
      "[901/1000] train loss: 4.40951, val loss: 17.46954\n",
      "[902/1000] train loss: 4.31255, val loss: 17.55950\n",
      "[903/1000] train loss: 4.29405, val loss: 17.42506\n",
      "[904/1000] train loss: 4.30637, val loss: 17.58564\n",
      "[905/1000] train loss: 4.30498, val loss: 17.72968\n",
      "[906/1000] train loss: 4.36319, val loss: 17.61466\n",
      "[907/1000] train loss: 4.18989, val loss: 17.49767\n",
      "[908/1000] train loss: 4.27131, val loss: 17.35843\n",
      "[909/1000] train loss: 4.26067, val loss: 17.62847\n",
      "[910/1000] train loss: 4.14274, val loss: 17.47519\n",
      "[911/1000] train loss: 4.26328, val loss: 17.48304\n",
      "[912/1000] train loss: 4.26785, val loss: 17.52383\n",
      "[913/1000] train loss: 4.23263, val loss: 17.36261\n",
      "[914/1000] train loss: 4.26678, val loss: 17.44504\n",
      "[915/1000] train loss: 4.32384, val loss: 17.23163\n",
      "[916/1000] train loss: 4.31511, val loss: 17.25601\n",
      "[917/1000] train loss: 4.23287, val loss: 17.58570\n",
      "[918/1000] train loss: 4.30357, val loss: 17.45276\n",
      "[919/1000] train loss: 4.25266, val loss: 17.67891\n",
      "[920/1000] train loss: 4.22881, val loss: 17.42930\n",
      "[921/1000] train loss: 4.23090, val loss: 17.54314\n",
      "[922/1000] train loss: 4.26686, val loss: 17.44409\n",
      "[923/1000] train loss: 4.16093, val loss: 17.60541\n",
      "[924/1000] train loss: 4.24906, val loss: 17.63344\n",
      "[925/1000] train loss: 4.22050, val loss: 17.47974\n",
      "[926/1000] train loss: 4.23851, val loss: 17.58315\n",
      "[927/1000] train loss: 4.25506, val loss: 17.50917\n",
      "[928/1000] train loss: 4.21613, val loss: 17.56909\n",
      "[929/1000] train loss: 4.24111, val loss: 17.64659\n",
      "[930/1000] train loss: 4.24134, val loss: 17.33498\n",
      "[931/1000] train loss: 4.16843, val loss: 17.58594\n",
      "[932/1000] train loss: 4.22206, val loss: 17.62230\n",
      "[933/1000] train loss: 4.07672, val loss: 17.75270\n",
      "[934/1000] train loss: 4.19473, val loss: 17.65439\n",
      "[935/1000] train loss: 4.16435, val loss: 17.47747\n",
      "[936/1000] train loss: 4.16620, val loss: 17.38185\n",
      "[937/1000] train loss: 4.18810, val loss: 17.51024\n",
      "[938/1000] train loss: 4.21878, val loss: 17.65056\n",
      "[939/1000] train loss: 4.20249, val loss: 17.62008\n",
      "[940/1000] train loss: 4.19021, val loss: 17.36616\n",
      "[941/1000] train loss: 4.21669, val loss: 17.53196\n",
      "[942/1000] train loss: 4.20909, val loss: 17.71582\n",
      "[943/1000] train loss: 3.92111, val loss: 17.37554\n",
      "[944/1000] train loss: 4.20404, val loss: 17.75006\n",
      "[945/1000] train loss: 4.17538, val loss: 17.37386\n",
      "[946/1000] train loss: 4.17896, val loss: 17.61915\n",
      "[947/1000] train loss: 4.17579, val loss: 17.57938\n",
      "[948/1000] train loss: 4.28764, val loss: 17.73140\n",
      "[949/1000] train loss: 4.13012, val loss: 17.44086\n",
      "[950/1000] train loss: 4.21465, val loss: 17.31596\n",
      "[951/1000] train loss: 4.12219, val loss: 17.67963\n",
      "[952/1000] train loss: 4.16546, val loss: 17.45985\n",
      "[953/1000] train loss: 4.16432, val loss: 17.48217\n",
      "[954/1000] train loss: 4.31490, val loss: 17.35503\n",
      "[955/1000] train loss: 4.24643, val loss: 17.47541\n",
      "[956/1000] train loss: 4.23229, val loss: 17.92266\n",
      "[957/1000] train loss: 4.30001, val loss: 17.92135\n",
      "[958/1000] train loss: 4.15265, val loss: 17.40347\n",
      "[959/1000] train loss: 4.15189, val loss: 17.70381\n",
      "[960/1000] train loss: 4.09041, val loss: 17.79477\n",
      "[961/1000] train loss: 4.11718, val loss: 17.50306\n",
      "[962/1000] train loss: 4.21023, val loss: 17.50095\n",
      "[963/1000] train loss: 4.12708, val loss: 17.54913\n",
      "[964/1000] train loss: 4.14637, val loss: 17.75936\n",
      "[965/1000] train loss: 4.11804, val loss: 17.43821\n",
      "[966/1000] train loss: 4.13803, val loss: 17.51643\n",
      "[967/1000] train loss: 4.10226, val loss: 17.65303\n",
      "[968/1000] train loss: 4.11778, val loss: 17.50975\n",
      "[969/1000] train loss: 4.16378, val loss: 17.43848\n",
      "[970/1000] train loss: 4.13607, val loss: 17.47786\n",
      "[971/1000] train loss: 4.18795, val loss: 17.89352\n",
      "[972/1000] train loss: 4.13352, val loss: 17.59750\n",
      "[973/1000] train loss: 4.20084, val loss: 17.90506\n",
      "[974/1000] train loss: 4.07737, val loss: 17.73431\n",
      "[975/1000] train loss: 4.04271, val loss: 17.51854\n",
      "[976/1000] train loss: 4.12077, val loss: 17.73626\n",
      "[977/1000] train loss: 4.19350, val loss: 17.69159\n",
      "[978/1000] train loss: 4.11701, val loss: 17.36790\n",
      "[979/1000] train loss: 4.10097, val loss: 17.55414\n",
      "[980/1000] train loss: 4.11049, val loss: 17.45720\n",
      "[981/1000] train loss: 4.04272, val loss: 17.82932\n",
      "[982/1000] train loss: 4.13341, val loss: 17.74652\n",
      "[983/1000] train loss: 4.11364, val loss: 17.53071\n",
      "[984/1000] train loss: 4.07205, val loss: 17.62978\n",
      "[985/1000] train loss: 4.05887, val loss: 17.76987\n",
      "[986/1000] train loss: 4.14707, val loss: 17.92488\n",
      "[987/1000] train loss: 4.06845, val loss: 17.53437\n",
      "[988/1000] train loss: 4.07908, val loss: 17.69878\n",
      "[989/1000] train loss: 4.05757, val loss: 17.60165\n",
      "[990/1000] train loss: 4.14090, val loss: 17.50333\n",
      "[991/1000] train loss: 4.11000, val loss: 17.61003\n",
      "[992/1000] train loss: 4.07905, val loss: 17.78813\n",
      "[993/1000] train loss: 4.12483, val loss: 17.68288\n",
      "[994/1000] train loss: 4.12884, val loss: 17.83666\n",
      "[995/1000] train loss: 4.08103, val loss: 17.38956\n",
      "[996/1000] train loss: 4.06160, val loss: 17.61989\n",
      "[997/1000] train loss: 4.05050, val loss: 17.54354\n",
      "[998/1000] train loss: 4.09376, val loss: 17.51266\n",
      "[999/1000] train loss: 4.06640, val loss: 17.89757\n",
      "[1000/1000] train loss: 4.02671, val loss: 17.74675\n"
     ]
    }
   ],
   "source": [
    "# 학습: 두단계 - 학습 + 검증\n",
    "for epoch in range(N_EPOCH):\n",
    "    ################\n",
    "    # 학습 - 모델을 train 모드로 변경\n",
    "    ################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in boston_train_loader:\n",
    "        # X, y를 device로 이동\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 1. 모델 추정\n",
    "        pred = boston_model(X) # 순전파(forward propagation)\n",
    "        # 2. loss 계산\n",
    "        loss = loss_fn(pred, y)  # 추정, 정답\n",
    "        # 3. 모델 파라미터를 업데이트\n",
    "        ## 3.1 파라미터들의 기울기를 초기화\n",
    "        optimizer.zero_grad()\n",
    "        ## 3.2 역전파(back propagration)을 해서 파라미터들의 기울기를 계산(grad속성에 저장)\n",
    "        loss.backward()\n",
    "        ## 3.3 파라미터 업데이트 처리.=> 1 step\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    #평균 loss\n",
    "    train_loss /= len(boston_train_loader)\n",
    "    # 1 epoch 학습끝\n",
    "    ###############################\n",
    "    # 검증 - 모델을 평가모드로 변경\n",
    "    ###############################\n",
    "    boston_model.eval() #evalutation mode 로 변환\n",
    "    val_loss = 0.0\n",
    "    # 역전파를 통한 gradient 계산이 필요 없기 때문에 일시적으로 grad_fn 을 구하지 않도록 처리.\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in boston_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            # 1.추정\n",
    "            pred_val = boston_model(X_val)\n",
    "            # 2. loss 계산\n",
    "            val_loss += loss_fn(pred_val, y_val).item()\n",
    "        val_loss /= len(boston_test_loader)\n",
    "    # epoch에 대한 검증 완료\n",
    "    # 결과 출력\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f}, val loss: {val_loss:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(N_EPOCH), train_loss_list, label=\"train loss\")\n",
    "plt.plot(range(N_EPOCH), val_loss_list, label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(3, 30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "- 모델구조, 파라미터 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'models/boston_model.pt'\n",
    "torch.save(boston_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_boston_model_1 = torch.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─Linear: 1-2                            [200, 16]                 528\n",
       "├─Linear: 1-3                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(load_boston_model_1, (200, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_1.to(device)\n",
    "load_boston_model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_1(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.74675178527832"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state_dict 저장 및 로딩\n",
    "- 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path2 = \"models/boston_state_dict.pt\"\n",
    "model_sd = boston_model.state_dict()\n",
    "\n",
    "torch.save(model_sd, save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#state_dict 를 로딩 \n",
    "## 1. 모델객체 생성\n",
    "load_boston_model_2 = BostonModel().to(device)\n",
    "## 2. state_dict 불러오기\n",
    "load_sd = torch.load(save_path2)\n",
    "## 3. 불러온 state_dict(파라미터들)을 모델에 덮어 씌우기\n",
    "load_boston_model_2.load_state_dict(load_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.74675178527832"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss = 0.0\n",
    "load_boston_model_2.to(device)\n",
    "load_boston_model_2.eval() #평가모드 변환\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in boston_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "        # 1.추정\n",
    "        pred_val = load_boston_model_2(X_val)\n",
    "        # 2. loss 계산\n",
    "        val_loss += loss_fn(pred_val, y_val).item()\n",
    "    val_loss /= len(boston_test_loader)\n",
    "    \n",
    "val_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 분류 (Classification)\n",
    "\n",
    "## Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋. \n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "이미지는 28x28 크기이며 Gray scale이다. *레이블*(label)은 0에서 9까지의 정수 배열이다. 아래 표는 이미지에 있는 의류의 **클래스**(class)들이다.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>레이블</th>\n",
    "    <th>클래스</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>T-shirt/top</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Trousers</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pullover</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>Dress</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>Coat</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>Sandal</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>Shirt</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>7</td>\n",
    "    <td>Sneaker</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>8</td>\n",
    "    <td>Bag</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>9</td>\n",
    "    <td>Ankle boot</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Dress', 'Pullover', 'Sandal'], dtype='<U11'), 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_class = np.array(['T-shirt/top', 'Trousers', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'])\n",
    "class_to_index = {key:value for value, key in enumerate(index_to_class)}\n",
    "\n",
    "index_to_class[[3, 2, 5]], class_to_index['Pullover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # channel first 처리. 0 ~ 1 scaling, torch.Tensor 변환\n",
    "#     transforms.Normalize(mean=0.5, std=0.5)  # 표준화((pixcel-mean)/std). (-1 ~ 1)\n",
    "])\n",
    "# Dataset loading\n",
    "fmnist_trainset = datasets.FashionMNIST(root=\"datasets\", train=True, \n",
    "                                        download=True, transform=transform)\n",
    "fmnist_testset = datasets.FashionMNIST(root=\"datasets\", train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: datasets\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n",
      "Dataset FashionMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: datasets\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=0.5, std=0.5)\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(fmnist_trainset)\n",
    "print(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fmnist_trainset), len(fmnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist_trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Ankle boot\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x, y = fmnist_trainset[0]\n",
    "print(y, index_to_class[y])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader 생성\n",
    "fmnist_train_loader = DataLoader(fmnist_trainset, batch_size=128, \n",
    "                                 shuffle=True, drop_last=True)\n",
    "fmnist_test_loader = DataLoader(fmnist_testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class FashionMNISTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(28*28, 2048)\n",
    "        self.lr2 = nn.Linear(2048, 1024)\n",
    "        self.lr3 = nn.Linear(1024, 512)\n",
    "        self.lr4 = nn.Linear(512, 256)\n",
    "        self.lr5 = nn.Linear(256, 128)\n",
    "        self.lr6 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 10) #out_feature: 10 - 10개 class별 확률.\n",
    "    def forward(self, X):\n",
    "        out = nn.Flatten()(X)\n",
    "        out = nn.ReLU()(self.lr1(out))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = nn.ReLU()(self.lr3(out))\n",
    "        out = nn.ReLU()(self.lr4(out))\n",
    "        out = nn.ReLU()(self.lr5(out))\n",
    "        out = nn.ReLU()(self.lr6(out))\n",
    "        out = self.output(out)\n",
    "#         nn.Softmax()(out)\n",
    "        # 다중분류의 output는 Softmax()함수로 계산해서 확률로 만들어서 출력해야 한다.\n",
    "        # 모델에서는 Linear를 통과한 결과를 반환.\n",
    "        # loss함수인 CrossEntropyLoss() 에서 softmax를 적용한다. \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=2048, bias=True)\n",
       "  (lr2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (lr3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_model = FashionMNISTModel()\n",
    "f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [128, 10]                 --\n",
       "├─Linear: 1-1                            [128, 2048]               1,607,680\n",
       "├─Linear: 1-2                            [128, 1024]               2,098,176\n",
       "├─Linear: 1-3                            [128, 512]                524,800\n",
       "├─Linear: 1-4                            [128, 256]                131,328\n",
       "├─Linear: 1-5                            [128, 128]                32,896\n",
       "├─Linear: 1-6                            [128, 64]                 8,256\n",
       "├─Linear: 1-7                            [128, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 563.68\n",
       "==========================================================================================\n",
       "Input size (MB): 0.40\n",
       "Forward/backward pass size (MB): 4.14\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 22.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(f_model, (128, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "tensor([-0.0953, -0.0415, -0.1023, -0.0777,  0.0733, -0.0241,  0.0092,  0.0143,\n",
      "         0.0351, -0.0144], grad_fn=<SelectBackward0>)\n",
      "-0.2233768105506897 4\n"
     ]
    }
   ],
   "source": [
    "# 추정\n",
    "i = torch.ones((2, 1, 28, 28), dtype=torch.float32)\n",
    "# i.shape\n",
    "y_hat = f_model(i)\n",
    "print(y_hat.shape)\n",
    "print(y_hat[0])\n",
    "print(y_hat[0].sum().item(), y_hat[0].argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0928, 0.0980, 0.0922, 0.0945, 0.1099, 0.0997, 0.1031, 0.1036, 0.1058,\n",
      "        0.1006], grad_fn=<SoftmaxBackward0>)\n",
      "0.9999998807907104 4\n"
     ]
    }
   ],
   "source": [
    "# 모델이 추정한 결과의 class를 알고 싶을 경우는 Softmax를 계산할 필요 없다.\n",
    "# 모델의 추정 확률을 알고 싶을 경우 softmax를 계산한다.\n",
    "y_hat2 = nn.Softmax(dim=-1)(y_hat[0])\n",
    "print(y_hat2)\n",
    "print(y_hat2.sum().item(), y_hat2.argmax(dim=-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 0.64573 val loss: 0.44694 val_accuracy: 0.83560\n",
      "저장 : 1 epoch - 이전 best_score: inf, 현재 score: 0.44694148324712923\n",
      "[2/1000] train loss: 0.39959 val loss: 0.41623 val_accuracy: 0.84830\n",
      "저장 : 2 epoch - 이전 best_score: 0.44694148324712923, 현재 score: 0.41622666222385213\n",
      "[3/1000] train loss: 0.35046 val loss: 0.37972 val_accuracy: 0.86540\n",
      "저장 : 3 epoch - 이전 best_score: 0.41622666222385213, 현재 score: 0.37972138256211824\n",
      "[4/1000] train loss: 0.32516 val loss: 0.36200 val_accuracy: 0.87300\n",
      "저장 : 4 epoch - 이전 best_score: 0.37972138256211824, 현재 score: 0.3619976341724396\n",
      "[5/1000] train loss: 0.30089 val loss: 0.34995 val_accuracy: 0.87490\n",
      "저장 : 5 epoch - 이전 best_score: 0.3619976341724396, 현재 score: 0.34995350694354577\n",
      "[6/1000] train loss: 0.28679 val loss: 0.34006 val_accuracy: 0.87580\n",
      "저장 : 6 epoch - 이전 best_score: 0.34995350694354577, 현재 score: 0.34005686848223965\n",
      "[7/1000] train loss: 0.27029 val loss: 0.33066 val_accuracy: 0.88260\n",
      "저장 : 7 epoch - 이전 best_score: 0.34005686848223965, 현재 score: 0.3306598020128057\n",
      "[8/1000] train loss: 0.26060 val loss: 0.37497 val_accuracy: 0.87530\n",
      "[9/1000] train loss: 0.24898 val loss: 0.34361 val_accuracy: 0.88150\n",
      "[10/1000] train loss: 0.23553 val loss: 0.34211 val_accuracy: 0.88610\n",
      "[11/1000] train loss: 0.22948 val loss: 0.33179 val_accuracy: 0.88350\n",
      "[12/1000] train loss: 0.21956 val loss: 0.32850 val_accuracy: 0.88770\n",
      "저장 : 12 epoch - 이전 best_score: 0.3306598020128057, 현재 score: 0.3285034714808947\n",
      "[13/1000] train loss: 0.21044 val loss: 0.36551 val_accuracy: 0.87820\n",
      "[14/1000] train loss: 0.20219 val loss: 0.37195 val_accuracy: 0.88270\n",
      "[15/1000] train loss: 0.19401 val loss: 0.34741 val_accuracy: 0.88940\n",
      "[16/1000] train loss: 0.19369 val loss: 0.33551 val_accuracy: 0.89010\n",
      "[17/1000] train loss: 0.18018 val loss: 0.31779 val_accuracy: 0.89260\n",
      "저장 : 17 epoch - 이전 best_score: 0.3285034714808947, 현재 score: 0.31778975510144536\n",
      "[18/1000] train loss: 0.17582 val loss: 0.32701 val_accuracy: 0.89040\n",
      "[19/1000] train loss: 0.16822 val loss: 0.35893 val_accuracy: 0.88760\n",
      "[20/1000] train loss: 0.16432 val loss: 0.33531 val_accuracy: 0.89700\n",
      "[21/1000] train loss: 0.15708 val loss: 0.42709 val_accuracy: 0.89420\n",
      "[22/1000] train loss: 0.15122 val loss: 0.35812 val_accuracy: 0.89610\n",
      "[23/1000] train loss: 0.14431 val loss: 0.38950 val_accuracy: 0.89590\n",
      "[24/1000] train loss: 0.13934 val loss: 0.37964 val_accuracy: 0.89620\n",
      "[25/1000] train loss: 0.14007 val loss: 0.36406 val_accuracy: 0.89430\n",
      "[26/1000] train loss: 0.13265 val loss: 0.37816 val_accuracy: 0.89820\n",
      "[27/1000] train loss: 0.12831 val loss: 0.43196 val_accuracy: 0.89310\n",
      "조기종료: epoch-27. 0.31779에서 개선이 안됨.\n",
      "학습에 걸린시간: 1182.291600227356초\n"
     ]
    }
   ],
   "source": [
    "#### 학습\n",
    "import time\n",
    "device = \"cpu\"\n",
    "# 모델 생성 + device 이동\n",
    "fmnist_model = FashionMNISTModel().to(device)\n",
    "# loss -> 다중분류: CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(fmnist_model.parameters(), lr=0.001)\n",
    "\n",
    "# 결과저장할 리스트\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "####################\n",
    "# 가장 성능 좋은 epoch의 모델을 학습도중 저장. -> 성능이 개선될때 마다 저장.\n",
    "# 필요한 변수들 정의\n",
    "####################\n",
    "best_score = torch.inf # 학습중 가장 좋은 평가지표(val_loss)를 저장.\n",
    "save_model_path = \"models/fashion_mnist_best_model.pt\"\n",
    "\n",
    "####################\n",
    "# 조기종료 (Eearly Stopping) - 특정 epoch동안 성능 개선이 없으면 학습을 중단.\n",
    "####################\n",
    "patience = 10   # 성능 개선 여부를 몇 epoch동안 확인 할 것인지.\n",
    "trigger_cnt = 0 # 몇 epoch째 성능개선을 기다리는지를 저장할 변수.\n",
    "\n",
    "N_EPOCH = 1000\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH): #에폭 학습\n",
    "    ########### 학습\n",
    "    fmnist_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in fmnist_train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = fmnist_model(X)\n",
    "        loss = loss_fn(pred, y) # pred: Softmax(), y: one hot encoding 처리\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(fmnist_train_loader)   # 평균 loss 계산\n",
    "    #### 1 epoch 학습 종료\n",
    "     \n",
    "    ############ 검증\n",
    "    fmnist_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in fmnist_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            pred_val = fmnist_model(X_val)  # Softmax 적용 전. -> loss는 이 값으로 계산.\n",
    "            pred_label = pred_val.argmax(dim=-1)  # accuracy  계산은 이 값으로 한다.\n",
    "            \n",
    "            # val-loss\n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            # val-accuracy\n",
    "            val_acc += torch.sum(pred_label == y_val).item()  #현 배치에서 맞은 것의 개수\n",
    "        # val_loss, val_acc의 평균\n",
    "        val_loss /= len(fmnist_test_loader)  # step 수로 나눔.\n",
    "        val_acc /= len(fmnist_test_loader.dataset) # 총 데이터 개수로 나눔.\n",
    "    \n",
    "    # 현재 Epoch에 대한 학습, 검증 종료\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss:.5f} val loss: {val_loss:.5f} val_accuracy: {val_acc:.5f}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    ######################################## \n",
    "    #  조기종료, 모델 저장 처리\n",
    "    #   현 epoch의 val_loss가 best_score보다 개선된 경우.(작은 경우)\n",
    "    #######################################\n",
    "    if val_loss < best_score: #성능개선\n",
    "        # 저장/조기종료\n",
    "        print(f\"저장 : {epoch+1} epoch - 이전 best_score: {best_score}, 현재 score: {val_loss}\")\n",
    "        best_score = val_loss\n",
    "        torch.save(fmnist_model, save_model_path)\n",
    "        trigger_cnt = 0\n",
    "        \n",
    "    else:\n",
    "        # 저장안하기/trigger_cnt 를 증가.=>조기종료\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"조기종료: epoch-{epoch+1}. {best_score:.5f}에서 개선이 안됨.\")\n",
    "            break\n",
    "    \n",
    "e = time.time()\n",
    "print(f'학습에 걸린시간: {e-s}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1182/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_list, label='train loss')\n",
    "plt.plot(val_loss_list, label='validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_acc_list)\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_load_model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in fmnist_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        pred_val = fmnist_load_model(X_val)  # Softmax 적용 전. -> loss는 이 값으로 계산.\n",
    "        pred_label = pred_val.argmax(dim=-1)  # accuracy  계산은 이 값으로 한다.\n",
    "\n",
    "        # val-loss\n",
    "        loss_val = loss_fn(pred_val, y_val)\n",
    "        val_loss += loss_val.item()\n",
    "        # val-accuracy\n",
    "        val_acc += torch.sum(pred_label == y_val).item()  #현 배치에서 맞은 것의 개수\n",
    "    # val_loss, val_acc의 평균\n",
    "    val_loss /= len(fmnist_test_loader)  # step 수로 나눔.\n",
    "    val_acc /= len(fmnist_test_loader.dataset) # 총 데이터 개수로 나눔."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31778975510144536, 0.8926)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "- **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "        \n",
    "- 위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "- Feature\n",
    "    - 종양에 대한 다양한 측정값들\n",
    "- Target의 class\n",
    "    - 0 - malignant(악성종양)\n",
    "    - 1 - benign(양성종양)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1, 1)  # 2차원으로 변경. ==> 모델 출력 shape과 맞춰준다. \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_tensor = torch.tensor(scaler.fit_transform(X_train), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(scaler.transform(X_test), dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([426, 1]), torch.Size([143, 1]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape, y_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - 데이터셋이 Tensor 객체로 메모리에 loading된 경우\n",
    "wb_train_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "wb_test_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoader\n",
    "wb_train_loader = DataLoader(wb_train_set, batch_size=len(wb_train_set),\n",
    "                             shuffle=True, drop_last=True)\n",
    "wb_test_loader = DataLoader(wb_test_set, batch_size=len(wb_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([426, 30]), torch.Size([426, 1]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(wb_train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.1193e-09,  6.7160e-09,  5.5967e-09,  0.0000e+00,  6.7160e-09,\n",
       "          1.1193e-09, -8.9547e-09, -1.1193e-09, -6.7160e-09, -5.5967e-10,\n",
       "         -8.9547e-09, -4.4773e-09,  0.0000e+00,  6.7160e-09,  3.3580e-09,\n",
       "         -2.2387e-09, -2.2387e-09, -1.1193e-08, -6.7160e-09,  2.2387e-09,\n",
       "          0.0000e+00, -4.4773e-09, -5.5967e-09, -3.3580e-09,  4.4773e-09,\n",
       "         -2.2387e-09,  6.7160e-09,  1.3432e-08, -4.4773e-09,  1.3432e-08]),\n",
       " tensor([1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012, 1.0012,\n",
       "         1.0012, 1.0012, 1.0012]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0), x.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_class = [\"악성종양\", \"양성종양\"]\n",
    "class_to_index = {\"악성종양\":0, \"양성종양\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델정의\n",
    "class BreastCancerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30, 32)\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 1)  # 출력- 값 1개 (positive의 확률을 출력)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = nn.ReLU()(self.lr1(X))\n",
    "        out = nn.ReLU()(self.lr2(out))\n",
    "        out = self.output(out)   \n",
    "        # 2진분류->positive 확률을 출력 -> output의 출력결과를 logistic 함수에 입력\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5423],\n",
       "        [0.5423],\n",
       "        [0.5423],\n",
       "        [0.5423],\n",
       "        [0.5423]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_model_tmp = BreastCancerModel()\n",
    "tmp_x = torch.ones(5, 30)\n",
    "y_tmp = bc_model_tmp(tmp_x)\n",
    "y_tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 0.6626929044723511, val loss: 0.6568688750267029, val accuracy: 0.8111888111888111\n",
      "1 epoch에서 저장. 이전 score: inf, 현재 score: 0.6568688750267029\n",
      "[2/500] train loss: 0.6550933122634888, val loss: 0.6497716307640076, val accuracy: 0.8531468531468531\n",
      "2 epoch에서 저장. 이전 score: 0.6568688750267029, 현재 score: 0.6497716307640076\n",
      "[3/500] train loss: 0.6475935578346252, val loss: 0.6427285671234131, val accuracy: 0.8601398601398601\n",
      "3 epoch에서 저장. 이전 score: 0.6497716307640076, 현재 score: 0.6427285671234131\n",
      "[4/500] train loss: 0.6401549577713013, val loss: 0.6357156038284302, val accuracy: 0.8741258741258742\n",
      "4 epoch에서 저장. 이전 score: 0.6427285671234131, 현재 score: 0.6357156038284302\n",
      "[5/500] train loss: 0.6327526569366455, val loss: 0.6287819147109985, val accuracy: 0.8881118881118881\n",
      "5 epoch에서 저장. 이전 score: 0.6357156038284302, 현재 score: 0.6287819147109985\n",
      "[6/500] train loss: 0.6253658533096313, val loss: 0.6218072175979614, val accuracy: 0.8951048951048951\n",
      "6 epoch에서 저장. 이전 score: 0.6287819147109985, 현재 score: 0.6218072175979614\n",
      "[7/500] train loss: 0.6179954409599304, val loss: 0.6148743629455566, val accuracy: 0.8951048951048951\n",
      "7 epoch에서 저장. 이전 score: 0.6218072175979614, 현재 score: 0.6148743629455566\n",
      "[8/500] train loss: 0.6106575727462769, val loss: 0.6079598665237427, val accuracy: 0.8951048951048951\n",
      "8 epoch에서 저장. 이전 score: 0.6148743629455566, 현재 score: 0.6079598665237427\n",
      "[9/500] train loss: 0.6033331155776978, val loss: 0.6009827852249146, val accuracy: 0.8951048951048951\n",
      "9 epoch에서 저장. 이전 score: 0.6079598665237427, 현재 score: 0.6009827852249146\n",
      "[10/500] train loss: 0.5960204601287842, val loss: 0.5940472483634949, val accuracy: 0.9020979020979021\n",
      "10 epoch에서 저장. 이전 score: 0.6009827852249146, 현재 score: 0.5940472483634949\n",
      "[11/500] train loss: 0.588736355304718, val loss: 0.5871055722236633, val accuracy: 0.8951048951048951\n",
      "11 epoch에서 저장. 이전 score: 0.5940472483634949, 현재 score: 0.5871055722236633\n",
      "[12/500] train loss: 0.5814359188079834, val loss: 0.5801252722740173, val accuracy: 0.8951048951048951\n",
      "12 epoch에서 저장. 이전 score: 0.5871055722236633, 현재 score: 0.5801252722740173\n",
      "[13/500] train loss: 0.5741046071052551, val loss: 0.5730679631233215, val accuracy: 0.8951048951048951\n",
      "13 epoch에서 저장. 이전 score: 0.5801252722740173, 현재 score: 0.5730679631233215\n",
      "[14/500] train loss: 0.566733181476593, val loss: 0.565941333770752, val accuracy: 0.8951048951048951\n",
      "14 epoch에서 저장. 이전 score: 0.5730679631233215, 현재 score: 0.565941333770752\n",
      "[15/500] train loss: 0.5593239665031433, val loss: 0.5587465167045593, val accuracy: 0.8951048951048951\n",
      "15 epoch에서 저장. 이전 score: 0.565941333770752, 현재 score: 0.5587465167045593\n",
      "[16/500] train loss: 0.5518611073493958, val loss: 0.5515184998512268, val accuracy: 0.8951048951048951\n",
      "16 epoch에서 저장. 이전 score: 0.5587465167045593, 현재 score: 0.5515184998512268\n",
      "[17/500] train loss: 0.5443323254585266, val loss: 0.5442618131637573, val accuracy: 0.9020979020979021\n",
      "17 epoch에서 저장. 이전 score: 0.5515184998512268, 현재 score: 0.5442618131637573\n",
      "[18/500] train loss: 0.5367134213447571, val loss: 0.5369781255722046, val accuracy: 0.9090909090909091\n",
      "18 epoch에서 저장. 이전 score: 0.5442618131637573, 현재 score: 0.5369781255722046\n",
      "[19/500] train loss: 0.528999924659729, val loss: 0.5296385288238525, val accuracy: 0.9090909090909091\n",
      "19 epoch에서 저장. 이전 score: 0.5369781255722046, 현재 score: 0.5296385288238525\n",
      "[20/500] train loss: 0.5211955308914185, val loss: 0.5222201347351074, val accuracy: 0.9090909090909091\n",
      "20 epoch에서 저장. 이전 score: 0.5296385288238525, 현재 score: 0.5222201347351074\n",
      "[21/500] train loss: 0.5133098363876343, val loss: 0.5147200226783752, val accuracy: 0.9090909090909091\n",
      "21 epoch에서 저장. 이전 score: 0.5222201347351074, 현재 score: 0.5147200226783752\n",
      "[22/500] train loss: 0.5053345561027527, val loss: 0.5071725845336914, val accuracy: 0.9090909090909091\n",
      "22 epoch에서 저장. 이전 score: 0.5147200226783752, 현재 score: 0.5071725845336914\n",
      "[23/500] train loss: 0.497283011674881, val loss: 0.49960556626319885, val accuracy: 0.9090909090909091\n",
      "23 epoch에서 저장. 이전 score: 0.5071725845336914, 현재 score: 0.49960556626319885\n",
      "[24/500] train loss: 0.4891607463359833, val loss: 0.49201512336730957, val accuracy: 0.9090909090909091\n",
      "24 epoch에서 저장. 이전 score: 0.49960556626319885, 현재 score: 0.49201512336730957\n",
      "[25/500] train loss: 0.4809872508049011, val loss: 0.4843722879886627, val accuracy: 0.9090909090909091\n",
      "25 epoch에서 저장. 이전 score: 0.49201512336730957, 현재 score: 0.4843722879886627\n",
      "[26/500] train loss: 0.4727369248867035, val loss: 0.4766836166381836, val accuracy: 0.916083916083916\n",
      "26 epoch에서 저장. 이전 score: 0.4843722879886627, 현재 score: 0.4766836166381836\n",
      "[27/500] train loss: 0.46441414952278137, val loss: 0.46894896030426025, val accuracy: 0.916083916083916\n",
      "27 epoch에서 저장. 이전 score: 0.4766836166381836, 현재 score: 0.46894896030426025\n",
      "[28/500] train loss: 0.4560571014881134, val loss: 0.4611867070198059, val accuracy: 0.916083916083916\n",
      "28 epoch에서 저장. 이전 score: 0.46894896030426025, 현재 score: 0.4611867070198059\n",
      "[29/500] train loss: 0.447671115398407, val loss: 0.45346879959106445, val accuracy: 0.916083916083916\n",
      "29 epoch에서 저장. 이전 score: 0.4611867070198059, 현재 score: 0.45346879959106445\n",
      "[30/500] train loss: 0.43927696347236633, val loss: 0.44571083784103394, val accuracy: 0.916083916083916\n",
      "30 epoch에서 저장. 이전 score: 0.45346879959106445, 현재 score: 0.44571083784103394\n",
      "[31/500] train loss: 0.4308561682701111, val loss: 0.437921404838562, val accuracy: 0.916083916083916\n",
      "31 epoch에서 저장. 이전 score: 0.44571083784103394, 현재 score: 0.437921404838562\n",
      "[32/500] train loss: 0.42243143916130066, val loss: 0.43012866377830505, val accuracy: 0.916083916083916\n",
      "32 epoch에서 저장. 이전 score: 0.437921404838562, 현재 score: 0.43012866377830505\n",
      "[33/500] train loss: 0.41401368379592896, val loss: 0.42234811186790466, val accuracy: 0.916083916083916\n",
      "33 epoch에서 저장. 이전 score: 0.43012866377830505, 현재 score: 0.42234811186790466\n",
      "[34/500] train loss: 0.40562233328819275, val loss: 0.4146084785461426, val accuracy: 0.916083916083916\n",
      "34 epoch에서 저장. 이전 score: 0.42234811186790466, 현재 score: 0.4146084785461426\n",
      "[35/500] train loss: 0.39725634455680847, val loss: 0.4069107174873352, val accuracy: 0.916083916083916\n",
      "35 epoch에서 저장. 이전 score: 0.4146084785461426, 현재 score: 0.4069107174873352\n",
      "[36/500] train loss: 0.3889210522174835, val loss: 0.39924633502960205, val accuracy: 0.916083916083916\n",
      "36 epoch에서 저장. 이전 score: 0.4069107174873352, 현재 score: 0.39924633502960205\n",
      "[37/500] train loss: 0.3806231915950775, val loss: 0.39161548018455505, val accuracy: 0.916083916083916\n",
      "37 epoch에서 저장. 이전 score: 0.39924633502960205, 현재 score: 0.39161548018455505\n",
      "[38/500] train loss: 0.37236931920051575, val loss: 0.38405340909957886, val accuracy: 0.9230769230769231\n",
      "38 epoch에서 저장. 이전 score: 0.39161548018455505, 현재 score: 0.38405340909957886\n",
      "[39/500] train loss: 0.3641795516014099, val loss: 0.3765508830547333, val accuracy: 0.9230769230769231\n",
      "39 epoch에서 저장. 이전 score: 0.38405340909957886, 현재 score: 0.3765508830547333\n",
      "[40/500] train loss: 0.3560550808906555, val loss: 0.3690943717956543, val accuracy: 0.9230769230769231\n",
      "40 epoch에서 저장. 이전 score: 0.3765508830547333, 현재 score: 0.3690943717956543\n",
      "[41/500] train loss: 0.34798869490623474, val loss: 0.3617338538169861, val accuracy: 0.9230769230769231\n",
      "41 epoch에서 저장. 이전 score: 0.3690943717956543, 현재 score: 0.3617338538169861\n",
      "[42/500] train loss: 0.3400007486343384, val loss: 0.35444846749305725, val accuracy: 0.9230769230769231\n",
      "42 epoch에서 저장. 이전 score: 0.3617338538169861, 현재 score: 0.35444846749305725\n",
      "[43/500] train loss: 0.33211055397987366, val loss: 0.34722933173179626, val accuracy: 0.9230769230769231\n",
      "43 epoch에서 저장. 이전 score: 0.35444846749305725, 현재 score: 0.34722933173179626\n",
      "[44/500] train loss: 0.324322909116745, val loss: 0.34010806679725647, val accuracy: 0.9230769230769231\n",
      "44 epoch에서 저장. 이전 score: 0.34722933173179626, 현재 score: 0.34010806679725647\n",
      "[45/500] train loss: 0.31664350628852844, val loss: 0.33307546377182007, val accuracy: 0.9230769230769231\n",
      "45 epoch에서 저장. 이전 score: 0.34010806679725647, 현재 score: 0.33307546377182007\n",
      "[46/500] train loss: 0.30907031893730164, val loss: 0.32613405585289, val accuracy: 0.9230769230769231\n",
      "46 epoch에서 저장. 이전 score: 0.33307546377182007, 현재 score: 0.32613405585289\n",
      "[47/500] train loss: 0.301613986492157, val loss: 0.3192938566207886, val accuracy: 0.916083916083916\n",
      "47 epoch에서 저장. 이전 score: 0.32613405585289, 현재 score: 0.3192938566207886\n",
      "[48/500] train loss: 0.2942759096622467, val loss: 0.3125525116920471, val accuracy: 0.916083916083916\n",
      "48 epoch에서 저장. 이전 score: 0.3192938566207886, 현재 score: 0.3125525116920471\n",
      "[49/500] train loss: 0.2870609760284424, val loss: 0.3059086501598358, val accuracy: 0.916083916083916\n",
      "49 epoch에서 저장. 이전 score: 0.3125525116920471, 현재 score: 0.3059086501598358\n",
      "[50/500] train loss: 0.27997633814811707, val loss: 0.2993640899658203, val accuracy: 0.916083916083916\n",
      "50 epoch에서 저장. 이전 score: 0.3059086501598358, 현재 score: 0.2993640899658203\n",
      "[51/500] train loss: 0.273023784160614, val loss: 0.2929607331752777, val accuracy: 0.9230769230769231\n",
      "51 epoch에서 저장. 이전 score: 0.2993640899658203, 현재 score: 0.2929607331752777\n",
      "[52/500] train loss: 0.2662140429019928, val loss: 0.28668150305747986, val accuracy: 0.9230769230769231\n",
      "52 epoch에서 저장. 이전 score: 0.2929607331752777, 현재 score: 0.28668150305747986\n",
      "[53/500] train loss: 0.2595632076263428, val loss: 0.2805236577987671, val accuracy: 0.9230769230769231\n",
      "53 epoch에서 저장. 이전 score: 0.28668150305747986, 현재 score: 0.2805236577987671\n",
      "[54/500] train loss: 0.25306767225265503, val loss: 0.274515837430954, val accuracy: 0.9230769230769231\n",
      "54 epoch에서 저장. 이전 score: 0.2805236577987671, 현재 score: 0.274515837430954\n",
      "[55/500] train loss: 0.24672506749629974, val loss: 0.2686486542224884, val accuracy: 0.9230769230769231\n",
      "55 epoch에서 저장. 이전 score: 0.274515837430954, 현재 score: 0.2686486542224884\n",
      "[56/500] train loss: 0.24053359031677246, val loss: 0.26290830969810486, val accuracy: 0.9230769230769231\n",
      "56 epoch에서 저장. 이전 score: 0.2686486542224884, 현재 score: 0.26290830969810486\n",
      "[57/500] train loss: 0.23449645936489105, val loss: 0.25728705525398254, val accuracy: 0.9230769230769231\n",
      "57 epoch에서 저장. 이전 score: 0.26290830969810486, 현재 score: 0.25728705525398254\n",
      "[58/500] train loss: 0.22861939668655396, val loss: 0.2518084645271301, val accuracy: 0.9300699300699301\n",
      "58 epoch에서 저장. 이전 score: 0.25728705525398254, 현재 score: 0.2518084645271301\n",
      "[59/500] train loss: 0.2229057252407074, val loss: 0.2464747428894043, val accuracy: 0.9300699300699301\n",
      "59 epoch에서 저장. 이전 score: 0.2518084645271301, 현재 score: 0.2464747428894043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/500] train loss: 0.21734999120235443, val loss: 0.24130815267562866, val accuracy: 0.9300699300699301\n",
      "60 epoch에서 저장. 이전 score: 0.2464747428894043, 현재 score: 0.24130815267562866\n",
      "[61/500] train loss: 0.2119549810886383, val loss: 0.2362888902425766, val accuracy: 0.9300699300699301\n",
      "61 epoch에서 저장. 이전 score: 0.24130815267562866, 현재 score: 0.2362888902425766\n",
      "[62/500] train loss: 0.2067217379808426, val loss: 0.23141101002693176, val accuracy: 0.9300699300699301\n",
      "62 epoch에서 저장. 이전 score: 0.2362888902425766, 현재 score: 0.23141101002693176\n",
      "[63/500] train loss: 0.20164398849010468, val loss: 0.22667613625526428, val accuracy: 0.9370629370629371\n",
      "63 epoch에서 저장. 이전 score: 0.23141101002693176, 현재 score: 0.22667613625526428\n",
      "[64/500] train loss: 0.19671709835529327, val loss: 0.2220856249332428, val accuracy: 0.9370629370629371\n",
      "64 epoch에서 저장. 이전 score: 0.22667613625526428, 현재 score: 0.2220856249332428\n",
      "[65/500] train loss: 0.19194123148918152, val loss: 0.21764081716537476, val accuracy: 0.9370629370629371\n",
      "65 epoch에서 저장. 이전 score: 0.2220856249332428, 현재 score: 0.21764081716537476\n",
      "[66/500] train loss: 0.18730370700359344, val loss: 0.21334756910800934, val accuracy: 0.9370629370629371\n",
      "66 epoch에서 저장. 이전 score: 0.21764081716537476, 현재 score: 0.21334756910800934\n",
      "[67/500] train loss: 0.1828087568283081, val loss: 0.20921465754508972, val accuracy: 0.9370629370629371\n",
      "67 epoch에서 저장. 이전 score: 0.21334756910800934, 현재 score: 0.20921465754508972\n",
      "[68/500] train loss: 0.17845863103866577, val loss: 0.20520853996276855, val accuracy: 0.9370629370629371\n",
      "68 epoch에서 저장. 이전 score: 0.20921465754508972, 현재 score: 0.20520853996276855\n",
      "[69/500] train loss: 0.1742493212223053, val loss: 0.201315239071846, val accuracy: 0.9370629370629371\n",
      "69 epoch에서 저장. 이전 score: 0.20520853996276855, 현재 score: 0.201315239071846\n",
      "[70/500] train loss: 0.1701766550540924, val loss: 0.19754581153392792, val accuracy: 0.9370629370629371\n",
      "70 epoch에서 저장. 이전 score: 0.201315239071846, 현재 score: 0.19754581153392792\n",
      "[71/500] train loss: 0.16623446345329285, val loss: 0.19392141699790955, val accuracy: 0.9370629370629371\n",
      "71 epoch에서 저장. 이전 score: 0.19754581153392792, 현재 score: 0.19392141699790955\n",
      "[72/500] train loss: 0.16242386400699615, val loss: 0.19042083621025085, val accuracy: 0.9370629370629371\n",
      "72 epoch에서 저장. 이전 score: 0.19392141699790955, 현재 score: 0.19042083621025085\n",
      "[73/500] train loss: 0.15873336791992188, val loss: 0.18700583279132843, val accuracy: 0.9370629370629371\n",
      "73 epoch에서 저장. 이전 score: 0.19042083621025085, 현재 score: 0.18700583279132843\n",
      "[74/500] train loss: 0.15516060590744019, val loss: 0.1837068647146225, val accuracy: 0.9370629370629371\n",
      "74 epoch에서 저장. 이전 score: 0.18700583279132843, 현재 score: 0.1837068647146225\n",
      "[75/500] train loss: 0.15170857310295105, val loss: 0.18051399290561676, val accuracy: 0.9370629370629371\n",
      "75 epoch에서 저장. 이전 score: 0.1837068647146225, 현재 score: 0.18051399290561676\n",
      "[76/500] train loss: 0.1483672559261322, val loss: 0.17742648720741272, val accuracy: 0.9370629370629371\n",
      "76 epoch에서 저장. 이전 score: 0.18051399290561676, 현재 score: 0.17742648720741272\n",
      "[77/500] train loss: 0.14514175057411194, val loss: 0.17444998025894165, val accuracy: 0.9370629370629371\n",
      "77 epoch에서 저장. 이전 score: 0.17742648720741272, 현재 score: 0.17444998025894165\n",
      "[78/500] train loss: 0.14202944934368134, val loss: 0.1715848594903946, val accuracy: 0.9440559440559441\n",
      "78 epoch에서 저장. 이전 score: 0.17444998025894165, 현재 score: 0.1715848594903946\n",
      "[79/500] train loss: 0.1390189677476883, val loss: 0.16882529854774475, val accuracy: 0.9440559440559441\n",
      "79 epoch에서 저장. 이전 score: 0.1715848594903946, 현재 score: 0.16882529854774475\n",
      "[80/500] train loss: 0.13611195981502533, val loss: 0.16616930067539215, val accuracy: 0.9440559440559441\n",
      "80 epoch에서 저장. 이전 score: 0.16882529854774475, 현재 score: 0.16616930067539215\n",
      "[81/500] train loss: 0.13330607116222382, val loss: 0.16361522674560547, val accuracy: 0.9440559440559441\n",
      "81 epoch에서 저장. 이전 score: 0.16616930067539215, 현재 score: 0.16361522674560547\n",
      "[82/500] train loss: 0.13058829307556152, val loss: 0.16113880276679993, val accuracy: 0.9440559440559441\n",
      "82 epoch에서 저장. 이전 score: 0.16361522674560547, 현재 score: 0.16113880276679993\n",
      "[83/500] train loss: 0.1279614120721817, val loss: 0.15874482691287994, val accuracy: 0.9440559440559441\n",
      "83 epoch에서 저장. 이전 score: 0.16113880276679993, 현재 score: 0.15874482691287994\n",
      "[84/500] train loss: 0.1254250407218933, val loss: 0.15643072128295898, val accuracy: 0.9440559440559441\n",
      "84 epoch에서 저장. 이전 score: 0.15874482691287994, 현재 score: 0.15643072128295898\n",
      "[85/500] train loss: 0.12297489494085312, val loss: 0.15419448912143707, val accuracy: 0.9440559440559441\n",
      "85 epoch에서 저장. 이전 score: 0.15643072128295898, 현재 score: 0.15419448912143707\n",
      "[86/500] train loss: 0.12060617655515671, val loss: 0.15204881131649017, val accuracy: 0.951048951048951\n",
      "86 epoch에서 저장. 이전 score: 0.15419448912143707, 현재 score: 0.15204881131649017\n",
      "[87/500] train loss: 0.11831606179475784, val loss: 0.14998561143875122, val accuracy: 0.951048951048951\n",
      "87 epoch에서 저장. 이전 score: 0.15204881131649017, 현재 score: 0.14998561143875122\n",
      "[88/500] train loss: 0.11610181629657745, val loss: 0.14799362421035767, val accuracy: 0.951048951048951\n",
      "88 epoch에서 저장. 이전 score: 0.14998561143875122, 현재 score: 0.14799362421035767\n",
      "[89/500] train loss: 0.11395754665136337, val loss: 0.14607173204421997, val accuracy: 0.951048951048951\n",
      "89 epoch에서 저장. 이전 score: 0.14799362421035767, 현재 score: 0.14607173204421997\n",
      "[90/500] train loss: 0.11187957972288132, val loss: 0.1442127525806427, val accuracy: 0.9440559440559441\n",
      "90 epoch에서 저장. 이전 score: 0.14607173204421997, 현재 score: 0.1442127525806427\n",
      "[91/500] train loss: 0.10986536741256714, val loss: 0.14242389798164368, val accuracy: 0.9440559440559441\n",
      "91 epoch에서 저장. 이전 score: 0.1442127525806427, 현재 score: 0.14242389798164368\n",
      "[92/500] train loss: 0.10791810601949692, val loss: 0.14070558547973633, val accuracy: 0.9440559440559441\n",
      "92 epoch에서 저장. 이전 score: 0.14242389798164368, 현재 score: 0.14070558547973633\n",
      "[93/500] train loss: 0.10603705048561096, val loss: 0.1390564739704132, val accuracy: 0.9440559440559441\n",
      "93 epoch에서 저장. 이전 score: 0.14070558547973633, 현재 score: 0.1390564739704132\n",
      "[94/500] train loss: 0.10421925783157349, val loss: 0.13747090101242065, val accuracy: 0.9440559440559441\n",
      "94 epoch에서 저장. 이전 score: 0.1390564739704132, 현재 score: 0.13747090101242065\n",
      "[95/500] train loss: 0.10246103256940842, val loss: 0.13593712449073792, val accuracy: 0.9440559440559441\n",
      "95 epoch에서 저장. 이전 score: 0.13747090101242065, 현재 score: 0.13593712449073792\n",
      "[96/500] train loss: 0.10076171904802322, val loss: 0.13445733487606049, val accuracy: 0.9440559440559441\n",
      "96 epoch에서 저장. 이전 score: 0.13593712449073792, 현재 score: 0.13445733487606049\n",
      "[97/500] train loss: 0.09911846369504929, val loss: 0.1330249011516571, val accuracy: 0.9440559440559441\n",
      "97 epoch에서 저장. 이전 score: 0.13445733487606049, 현재 score: 0.1330249011516571\n",
      "[98/500] train loss: 0.09752583503723145, val loss: 0.13164308667182922, val accuracy: 0.9440559440559441\n",
      "98 epoch에서 저장. 이전 score: 0.1330249011516571, 현재 score: 0.13164308667182922\n",
      "[99/500] train loss: 0.09598413854837418, val loss: 0.13031452894210815, val accuracy: 0.9440559440559441\n",
      "99 epoch에서 저장. 이전 score: 0.13164308667182922, 현재 score: 0.13031452894210815\n",
      "[100/500] train loss: 0.09449143707752228, val loss: 0.12903934717178345, val accuracy: 0.951048951048951\n",
      "100 epoch에서 저장. 이전 score: 0.13031452894210815, 현재 score: 0.12903934717178345\n",
      "[101/500] train loss: 0.09304629266262054, val loss: 0.12781508266925812, val accuracy: 0.951048951048951\n",
      "101 epoch에서 저장. 이전 score: 0.12903934717178345, 현재 score: 0.12781508266925812\n",
      "[102/500] train loss: 0.09164800494909286, val loss: 0.1266331821680069, val accuracy: 0.951048951048951\n",
      "102 epoch에서 저장. 이전 score: 0.12781508266925812, 현재 score: 0.1266331821680069\n",
      "[103/500] train loss: 0.0902908518910408, val loss: 0.12549889087677002, val accuracy: 0.951048951048951\n",
      "103 epoch에서 저장. 이전 score: 0.1266331821680069, 현재 score: 0.12549889087677002\n",
      "[104/500] train loss: 0.08897281438112259, val loss: 0.12444305419921875, val accuracy: 0.951048951048951\n",
      "104 epoch에서 저장. 이전 score: 0.12549889087677002, 현재 score: 0.12444305419921875\n",
      "[105/500] train loss: 0.08769382536411285, val loss: 0.12343970686197281, val accuracy: 0.951048951048951\n",
      "105 epoch에서 저장. 이전 score: 0.12444305419921875, 현재 score: 0.12343970686197281\n",
      "[106/500] train loss: 0.08645060658454895, val loss: 0.1224772036075592, val accuracy: 0.951048951048951\n",
      "106 epoch에서 저장. 이전 score: 0.12343970686197281, 현재 score: 0.1224772036075592\n",
      "[107/500] train loss: 0.08523768186569214, val loss: 0.12155959755182266, val accuracy: 0.951048951048951\n",
      "107 epoch에서 저장. 이전 score: 0.1224772036075592, 현재 score: 0.12155959755182266\n",
      "[108/500] train loss: 0.08405591547489166, val loss: 0.12068060040473938, val accuracy: 0.951048951048951\n",
      "108 epoch에서 저장. 이전 score: 0.12155959755182266, 현재 score: 0.12068060040473938\n",
      "[109/500] train loss: 0.08290907740592957, val loss: 0.11983766406774521, val accuracy: 0.951048951048951\n",
      "109 epoch에서 저장. 이전 score: 0.12068060040473938, 현재 score: 0.11983766406774521\n",
      "[110/500] train loss: 0.08179567009210587, val loss: 0.11902900040149689, val accuracy: 0.951048951048951\n",
      "110 epoch에서 저장. 이전 score: 0.11983766406774521, 현재 score: 0.11902900040149689\n",
      "[111/500] train loss: 0.08071652054786682, val loss: 0.11824862658977509, val accuracy: 0.951048951048951\n",
      "111 epoch에서 저장. 이전 score: 0.11902900040149689, 현재 score: 0.11824862658977509\n",
      "[112/500] train loss: 0.07966688275337219, val loss: 0.11749667674303055, val accuracy: 0.951048951048951\n",
      "112 epoch에서 저장. 이전 score: 0.11824862658977509, 현재 score: 0.11749667674303055\n",
      "[113/500] train loss: 0.07864752411842346, val loss: 0.11677488684654236, val accuracy: 0.951048951048951\n",
      "113 epoch에서 저장. 이전 score: 0.11749667674303055, 현재 score: 0.11677488684654236\n",
      "[114/500] train loss: 0.07765862345695496, val loss: 0.1160842627286911, val accuracy: 0.951048951048951\n",
      "114 epoch에서 저장. 이전 score: 0.11677488684654236, 현재 score: 0.1160842627286911\n",
      "[115/500] train loss: 0.0766976848244667, val loss: 0.11541964113712311, val accuracy: 0.951048951048951\n",
      "115 epoch에서 저장. 이전 score: 0.1160842627286911, 현재 score: 0.11541964113712311\n",
      "[116/500] train loss: 0.0757613405585289, val loss: 0.11477735638618469, val accuracy: 0.951048951048951\n",
      "116 epoch에서 저장. 이전 score: 0.11541964113712311, 현재 score: 0.11477735638618469\n",
      "[117/500] train loss: 0.07485248148441315, val loss: 0.11416028439998627, val accuracy: 0.951048951048951\n",
      "117 epoch에서 저장. 이전 score: 0.11477735638618469, 현재 score: 0.11416028439998627\n",
      "[118/500] train loss: 0.07396801561117172, val loss: 0.11356636881828308, val accuracy: 0.951048951048951\n",
      "118 epoch에서 저장. 이전 score: 0.11416028439998627, 현재 score: 0.11356636881828308\n",
      "[119/500] train loss: 0.07310719043016434, val loss: 0.11299318820238113, val accuracy: 0.951048951048951\n",
      "119 epoch에서 저장. 이전 score: 0.11356636881828308, 현재 score: 0.11299318820238113\n",
      "[120/500] train loss: 0.07226917892694473, val loss: 0.11243949830532074, val accuracy: 0.951048951048951\n",
      "120 epoch에서 저장. 이전 score: 0.11299318820238113, 현재 score: 0.11243949830532074\n",
      "[121/500] train loss: 0.0714525580406189, val loss: 0.11190672218799591, val accuracy: 0.951048951048951\n",
      "121 epoch에서 저장. 이전 score: 0.11243949830532074, 현재 score: 0.11190672218799591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122/500] train loss: 0.0706569254398346, val loss: 0.11139895021915436, val accuracy: 0.951048951048951\n",
      "122 epoch에서 저장. 이전 score: 0.11190672218799591, 현재 score: 0.11139895021915436\n",
      "[123/500] train loss: 0.0698816254734993, val loss: 0.11091078072786331, val accuracy: 0.951048951048951\n",
      "123 epoch에서 저장. 이전 score: 0.11139895021915436, 현재 score: 0.11091078072786331\n",
      "[124/500] train loss: 0.06912502646446228, val loss: 0.11043915152549744, val accuracy: 0.951048951048951\n",
      "124 epoch에서 저장. 이전 score: 0.11091078072786331, 현재 score: 0.11043915152549744\n",
      "[125/500] train loss: 0.06838864088058472, val loss: 0.10997837781906128, val accuracy: 0.951048951048951\n",
      "125 epoch에서 저장. 이전 score: 0.11043915152549744, 현재 score: 0.10997837781906128\n",
      "[126/500] train loss: 0.06767302751541138, val loss: 0.10953080654144287, val accuracy: 0.951048951048951\n",
      "126 epoch에서 저장. 이전 score: 0.10997837781906128, 현재 score: 0.10953080654144287\n",
      "[127/500] train loss: 0.06697509437799454, val loss: 0.10909894853830338, val accuracy: 0.951048951048951\n",
      "127 epoch에서 저장. 이전 score: 0.10953080654144287, 현재 score: 0.10909894853830338\n",
      "[128/500] train loss: 0.0662921816110611, val loss: 0.10868076980113983, val accuracy: 0.951048951048951\n",
      "128 epoch에서 저장. 이전 score: 0.10909894853830338, 현재 score: 0.10868076980113983\n",
      "[129/500] train loss: 0.06562699377536774, val loss: 0.10827659070491791, val accuracy: 0.951048951048951\n",
      "129 epoch에서 저장. 이전 score: 0.10868076980113983, 현재 score: 0.10827659070491791\n",
      "[130/500] train loss: 0.06498131901025772, val loss: 0.10788237303495407, val accuracy: 0.951048951048951\n",
      "130 epoch에서 저장. 이전 score: 0.10827659070491791, 현재 score: 0.10788237303495407\n",
      "[131/500] train loss: 0.0643518790602684, val loss: 0.10749901831150055, val accuracy: 0.958041958041958\n",
      "131 epoch에서 저장. 이전 score: 0.10788237303495407, 현재 score: 0.10749901831150055\n",
      "[132/500] train loss: 0.06373723596334457, val loss: 0.10713179409503937, val accuracy: 0.958041958041958\n",
      "132 epoch에서 저장. 이전 score: 0.10749901831150055, 현재 score: 0.10713179409503937\n",
      "[133/500] train loss: 0.06313847005367279, val loss: 0.10677886754274368, val accuracy: 0.958041958041958\n",
      "133 epoch에서 저장. 이전 score: 0.10713179409503937, 현재 score: 0.10677886754274368\n",
      "[134/500] train loss: 0.06255175918340683, val loss: 0.10643971711397171, val accuracy: 0.958041958041958\n",
      "134 epoch에서 저장. 이전 score: 0.10677886754274368, 현재 score: 0.10643971711397171\n",
      "[135/500] train loss: 0.061978988349437714, val loss: 0.10611340403556824, val accuracy: 0.958041958041958\n",
      "135 epoch에서 저장. 이전 score: 0.10643971711397171, 현재 score: 0.10611340403556824\n",
      "[136/500] train loss: 0.06141884997487068, val loss: 0.10579923540353775, val accuracy: 0.958041958041958\n",
      "136 epoch에서 저장. 이전 score: 0.10611340403556824, 현재 score: 0.10579923540353775\n",
      "[137/500] train loss: 0.06087034195661545, val loss: 0.1054949089884758, val accuracy: 0.958041958041958\n",
      "137 epoch에서 저장. 이전 score: 0.10579923540353775, 현재 score: 0.1054949089884758\n",
      "[138/500] train loss: 0.06033271551132202, val loss: 0.10520098358392715, val accuracy: 0.958041958041958\n",
      "138 epoch에서 저장. 이전 score: 0.1054949089884758, 현재 score: 0.10520098358392715\n",
      "[139/500] train loss: 0.05980558693408966, val loss: 0.10490242391824722, val accuracy: 0.958041958041958\n",
      "139 epoch에서 저장. 이전 score: 0.10520098358392715, 현재 score: 0.10490242391824722\n",
      "[140/500] train loss: 0.059288185089826584, val loss: 0.10461044311523438, val accuracy: 0.958041958041958\n",
      "140 epoch에서 저장. 이전 score: 0.10490242391824722, 현재 score: 0.10461044311523438\n",
      "[141/500] train loss: 0.05878118425607681, val loss: 0.10432793200016022, val accuracy: 0.958041958041958\n",
      "141 epoch에서 저장. 이전 score: 0.10461044311523438, 현재 score: 0.10432793200016022\n",
      "[142/500] train loss: 0.05828382819890976, val loss: 0.10405312478542328, val accuracy: 0.958041958041958\n",
      "142 epoch에서 저장. 이전 score: 0.10432793200016022, 현재 score: 0.10405312478542328\n",
      "[143/500] train loss: 0.05779564008116722, val loss: 0.10377547889947891, val accuracy: 0.958041958041958\n",
      "143 epoch에서 저장. 이전 score: 0.10405312478542328, 현재 score: 0.10377547889947891\n",
      "[144/500] train loss: 0.05731738358736038, val loss: 0.10350599139928818, val accuracy: 0.958041958041958\n",
      "144 epoch에서 저장. 이전 score: 0.10377547889947891, 현재 score: 0.10350599139928818\n",
      "[145/500] train loss: 0.05684848874807358, val loss: 0.10324540734291077, val accuracy: 0.958041958041958\n",
      "145 epoch에서 저장. 이전 score: 0.10350599139928818, 현재 score: 0.10324540734291077\n",
      "[146/500] train loss: 0.056388527154922485, val loss: 0.10299743711948395, val accuracy: 0.958041958041958\n",
      "146 epoch에서 저장. 이전 score: 0.10324540734291077, 현재 score: 0.10299743711948395\n",
      "[147/500] train loss: 0.05593637377023697, val loss: 0.10276027768850327, val accuracy: 0.965034965034965\n",
      "147 epoch에서 저장. 이전 score: 0.10299743711948395, 현재 score: 0.10276027768850327\n",
      "[148/500] train loss: 0.055491961538791656, val loss: 0.10253072530031204, val accuracy: 0.965034965034965\n",
      "148 epoch에서 저장. 이전 score: 0.10276027768850327, 현재 score: 0.10253072530031204\n",
      "[149/500] train loss: 0.05505413934588432, val loss: 0.10231097042560577, val accuracy: 0.965034965034965\n",
      "149 epoch에서 저장. 이전 score: 0.10253072530031204, 현재 score: 0.10231097042560577\n",
      "[150/500] train loss: 0.05462416633963585, val loss: 0.10210870951414108, val accuracy: 0.965034965034965\n",
      "150 epoch에서 저장. 이전 score: 0.10231097042560577, 현재 score: 0.10210870951414108\n",
      "[151/500] train loss: 0.05420120805501938, val loss: 0.10192782431840897, val accuracy: 0.965034965034965\n",
      "151 epoch에서 저장. 이전 score: 0.10210870951414108, 현재 score: 0.10192782431840897\n",
      "[152/500] train loss: 0.05378494784235954, val loss: 0.10175557434558868, val accuracy: 0.965034965034965\n",
      "152 epoch에서 저장. 이전 score: 0.10192782431840897, 현재 score: 0.10175557434558868\n",
      "[153/500] train loss: 0.0533757247030735, val loss: 0.1015910655260086, val accuracy: 0.965034965034965\n",
      "153 epoch에서 저장. 이전 score: 0.10175557434558868, 현재 score: 0.1015910655260086\n",
      "[154/500] train loss: 0.05297335609793663, val loss: 0.10143546015024185, val accuracy: 0.965034965034965\n",
      "154 epoch에서 저장. 이전 score: 0.1015910655260086, 현재 score: 0.10143546015024185\n",
      "[155/500] train loss: 0.05257825925946236, val loss: 0.1012825146317482, val accuracy: 0.972027972027972\n",
      "155 epoch에서 저장. 이전 score: 0.10143546015024185, 현재 score: 0.1012825146317482\n",
      "[156/500] train loss: 0.05219000205397606, val loss: 0.10114659368991852, val accuracy: 0.972027972027972\n",
      "156 epoch에서 저장. 이전 score: 0.1012825146317482, 현재 score: 0.10114659368991852\n",
      "[157/500] train loss: 0.051807623356580734, val loss: 0.10101760178804398, val accuracy: 0.972027972027972\n",
      "157 epoch에서 저장. 이전 score: 0.10114659368991852, 현재 score: 0.10101760178804398\n",
      "[158/500] train loss: 0.051430054008960724, val loss: 0.10089267045259476, val accuracy: 0.972027972027972\n",
      "158 epoch에서 저장. 이전 score: 0.10101760178804398, 현재 score: 0.10089267045259476\n",
      "[159/500] train loss: 0.05105789005756378, val loss: 0.10076107829809189, val accuracy: 0.972027972027972\n",
      "159 epoch에서 저장. 이전 score: 0.10089267045259476, 현재 score: 0.10076107829809189\n",
      "[160/500] train loss: 0.05069023370742798, val loss: 0.10063222795724869, val accuracy: 0.972027972027972\n",
      "160 epoch에서 저장. 이전 score: 0.10076107829809189, 현재 score: 0.10063222795724869\n",
      "[161/500] train loss: 0.050326939672231674, val loss: 0.10050742328166962, val accuracy: 0.972027972027972\n",
      "161 epoch에서 저장. 이전 score: 0.10063222795724869, 현재 score: 0.10050742328166962\n",
      "[162/500] train loss: 0.04996952414512634, val loss: 0.10038650780916214, val accuracy: 0.972027972027972\n",
      "162 epoch에서 저장. 이전 score: 0.10050742328166962, 현재 score: 0.10038650780916214\n",
      "[163/500] train loss: 0.049617670476436615, val loss: 0.10027004033327103, val accuracy: 0.972027972027972\n",
      "163 epoch에서 저장. 이전 score: 0.10038650780916214, 현재 score: 0.10027004033327103\n",
      "[164/500] train loss: 0.049272697418928146, val loss: 0.10015755891799927, val accuracy: 0.972027972027972\n",
      "164 epoch에서 저장. 이전 score: 0.10027004033327103, 현재 score: 0.10015755891799927\n",
      "[165/500] train loss: 0.04893512278795242, val loss: 0.10004930198192596, val accuracy: 0.972027972027972\n",
      "165 epoch에서 저장. 이전 score: 0.10015755891799927, 현재 score: 0.10004930198192596\n",
      "[166/500] train loss: 0.048601698130369186, val loss: 0.09994510561227798, val accuracy: 0.972027972027972\n",
      "166 epoch에서 저장. 이전 score: 0.10004930198192596, 현재 score: 0.09994510561227798\n",
      "[167/500] train loss: 0.04827228561043739, val loss: 0.09984471648931503, val accuracy: 0.972027972027972\n",
      "167 epoch에서 저장. 이전 score: 0.09994510561227798, 현재 score: 0.09984471648931503\n",
      "[168/500] train loss: 0.04794690012931824, val loss: 0.09974655508995056, val accuracy: 0.972027972027972\n",
      "168 epoch에서 저장. 이전 score: 0.09984471648931503, 현재 score: 0.09974655508995056\n",
      "[169/500] train loss: 0.047624919563531876, val loss: 0.09965261071920395, val accuracy: 0.972027972027972\n",
      "169 epoch에서 저장. 이전 score: 0.09974655508995056, 현재 score: 0.09965261071920395\n",
      "[170/500] train loss: 0.047308146953582764, val loss: 0.09956258535385132, val accuracy: 0.972027972027972\n",
      "170 epoch에서 저장. 이전 score: 0.09965261071920395, 현재 score: 0.09956258535385132\n",
      "[171/500] train loss: 0.04699588567018509, val loss: 0.09947621077299118, val accuracy: 0.965034965034965\n",
      "171 epoch에서 저장. 이전 score: 0.09956258535385132, 현재 score: 0.09947621077299118\n",
      "[172/500] train loss: 0.046686332672834396, val loss: 0.09939396381378174, val accuracy: 0.965034965034965\n",
      "172 epoch에서 저장. 이전 score: 0.09947621077299118, 현재 score: 0.09939396381378174\n",
      "[173/500] train loss: 0.04637996852397919, val loss: 0.09931675344705582, val accuracy: 0.965034965034965\n",
      "173 epoch에서 저장. 이전 score: 0.09939396381378174, 현재 score: 0.09931675344705582\n",
      "[174/500] train loss: 0.04607893154025078, val loss: 0.09924336522817612, val accuracy: 0.965034965034965\n",
      "174 epoch에서 저장. 이전 score: 0.09931675344705582, 현재 score: 0.09924336522817612\n",
      "[175/500] train loss: 0.045782674103975296, val loss: 0.09917376935482025, val accuracy: 0.965034965034965\n",
      "175 epoch에서 저장. 이전 score: 0.09924336522817612, 현재 score: 0.09917376935482025\n",
      "[176/500] train loss: 0.045489128679037094, val loss: 0.09910785406827927, val accuracy: 0.965034965034965\n",
      "176 epoch에서 저장. 이전 score: 0.09917376935482025, 현재 score: 0.09910785406827927\n",
      "[177/500] train loss: 0.04519862309098244, val loss: 0.09904494881629944, val accuracy: 0.965034965034965\n",
      "177 epoch에서 저장. 이전 score: 0.09910785406827927, 현재 score: 0.09904494881629944\n",
      "[178/500] train loss: 0.04491175338625908, val loss: 0.09898367524147034, val accuracy: 0.965034965034965\n",
      "178 epoch에서 저장. 이전 score: 0.09904494881629944, 현재 score: 0.09898367524147034\n",
      "[179/500] train loss: 0.04462829604744911, val loss: 0.09892607480287552, val accuracy: 0.958041958041958\n",
      "179 epoch에서 저장. 이전 score: 0.09898367524147034, 현재 score: 0.09892607480287552\n",
      "[180/500] train loss: 0.044347696006298065, val loss: 0.09887146949768066, val accuracy: 0.958041958041958\n",
      "180 epoch에서 저장. 이전 score: 0.09892607480287552, 현재 score: 0.09887146949768066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[181/500] train loss: 0.04406960308551788, val loss: 0.09882136434316635, val accuracy: 0.958041958041958\n",
      "181 epoch에서 저장. 이전 score: 0.09887146949768066, 현재 score: 0.09882136434316635\n",
      "[182/500] train loss: 0.043794550001621246, val loss: 0.09877733886241913, val accuracy: 0.958041958041958\n",
      "182 epoch에서 저장. 이전 score: 0.09882136434316635, 현재 score: 0.09877733886241913\n",
      "[183/500] train loss: 0.04352545365691185, val loss: 0.09873593598604202, val accuracy: 0.958041958041958\n",
      "183 epoch에서 저장. 이전 score: 0.09877733886241913, 현재 score: 0.09873593598604202\n",
      "[184/500] train loss: 0.04325904697179794, val loss: 0.0986969918012619, val accuracy: 0.958041958041958\n",
      "184 epoch에서 저장. 이전 score: 0.09873593598604202, 현재 score: 0.0986969918012619\n",
      "[185/500] train loss: 0.042996201664209366, val loss: 0.09866152703762054, val accuracy: 0.958041958041958\n",
      "185 epoch에서 저장. 이전 score: 0.0986969918012619, 현재 score: 0.09866152703762054\n",
      "[186/500] train loss: 0.042735032737255096, val loss: 0.09862961620092392, val accuracy: 0.958041958041958\n",
      "186 epoch에서 저장. 이전 score: 0.09866152703762054, 현재 score: 0.09862961620092392\n",
      "[187/500] train loss: 0.042477983981370926, val loss: 0.09860058128833771, val accuracy: 0.958041958041958\n",
      "187 epoch에서 저장. 이전 score: 0.09862961620092392, 현재 score: 0.09860058128833771\n",
      "[188/500] train loss: 0.04222502559423447, val loss: 0.09857162833213806, val accuracy: 0.958041958041958\n",
      "188 epoch에서 저장. 이전 score: 0.09860058128833771, 현재 score: 0.09857162833213806\n",
      "[189/500] train loss: 0.04197398200631142, val loss: 0.09854426234960556, val accuracy: 0.958041958041958\n",
      "189 epoch에서 저장. 이전 score: 0.09857162833213806, 현재 score: 0.09854426234960556\n",
      "[190/500] train loss: 0.04172501713037491, val loss: 0.09851759672164917, val accuracy: 0.958041958041958\n",
      "190 epoch에서 저장. 이전 score: 0.09854426234960556, 현재 score: 0.09851759672164917\n",
      "[191/500] train loss: 0.04147902876138687, val loss: 0.09849340468645096, val accuracy: 0.958041958041958\n",
      "191 epoch에서 저장. 이전 score: 0.09851759672164917, 현재 score: 0.09849340468645096\n",
      "[192/500] train loss: 0.041235677897930145, val loss: 0.0984695702791214, val accuracy: 0.958041958041958\n",
      "192 epoch에서 저장. 이전 score: 0.09849340468645096, 현재 score: 0.0984695702791214\n",
      "[193/500] train loss: 0.04099435731768608, val loss: 0.09843694418668747, val accuracy: 0.958041958041958\n",
      "193 epoch에서 저장. 이전 score: 0.0984695702791214, 현재 score: 0.09843694418668747\n",
      "[194/500] train loss: 0.040754321962594986, val loss: 0.0983995646238327, val accuracy: 0.958041958041958\n",
      "194 epoch에서 저장. 이전 score: 0.09843694418668747, 현재 score: 0.0983995646238327\n",
      "[195/500] train loss: 0.040514878928661346, val loss: 0.09835406392812729, val accuracy: 0.958041958041958\n",
      "195 epoch에서 저장. 이전 score: 0.0983995646238327, 현재 score: 0.09835406392812729\n",
      "[196/500] train loss: 0.04027872905135155, val loss: 0.09830980747938156, val accuracy: 0.958041958041958\n",
      "196 epoch에서 저장. 이전 score: 0.09835406392812729, 현재 score: 0.09830980747938156\n",
      "[197/500] train loss: 0.040045157074928284, val loss: 0.0982663631439209, val accuracy: 0.958041958041958\n",
      "197 epoch에서 저장. 이전 score: 0.09830980747938156, 현재 score: 0.0982663631439209\n",
      "[198/500] train loss: 0.03981270268559456, val loss: 0.09822410345077515, val accuracy: 0.958041958041958\n",
      "198 epoch에서 저장. 이전 score: 0.0982663631439209, 현재 score: 0.09822410345077515\n",
      "[199/500] train loss: 0.03958049416542053, val loss: 0.09818241000175476, val accuracy: 0.958041958041958\n",
      "199 epoch에서 저장. 이전 score: 0.09822410345077515, 현재 score: 0.09818241000175476\n",
      "[200/500] train loss: 0.03934820368885994, val loss: 0.09814301133155823, val accuracy: 0.958041958041958\n",
      "200 epoch에서 저장. 이전 score: 0.09818241000175476, 현재 score: 0.09814301133155823\n",
      "[201/500] train loss: 0.03911881521344185, val loss: 0.09810364246368408, val accuracy: 0.958041958041958\n",
      "201 epoch에서 저장. 이전 score: 0.09814301133155823, 현재 score: 0.09810364246368408\n",
      "[202/500] train loss: 0.03889209404587746, val loss: 0.09806443750858307, val accuracy: 0.958041958041958\n",
      "202 epoch에서 저장. 이전 score: 0.09810364246368408, 현재 score: 0.09806443750858307\n",
      "[203/500] train loss: 0.03866718336939812, val loss: 0.09802357852458954, val accuracy: 0.958041958041958\n",
      "203 epoch에서 저장. 이전 score: 0.09806443750858307, 현재 score: 0.09802357852458954\n",
      "[204/500] train loss: 0.038444045931100845, val loss: 0.09797748178243637, val accuracy: 0.958041958041958\n",
      "204 epoch에서 저장. 이전 score: 0.09802357852458954, 현재 score: 0.09797748178243637\n",
      "[205/500] train loss: 0.03822243958711624, val loss: 0.09792809933423996, val accuracy: 0.958041958041958\n",
      "205 epoch에서 저장. 이전 score: 0.09797748178243637, 현재 score: 0.09792809933423996\n",
      "[206/500] train loss: 0.03800208494067192, val loss: 0.09787817299365997, val accuracy: 0.958041958041958\n",
      "206 epoch에서 저장. 이전 score: 0.09792809933423996, 현재 score: 0.09787817299365997\n",
      "[207/500] train loss: 0.037783823907375336, val loss: 0.0978320837020874, val accuracy: 0.958041958041958\n",
      "207 epoch에서 저장. 이전 score: 0.09787817299365997, 현재 score: 0.0978320837020874\n",
      "[208/500] train loss: 0.03756679967045784, val loss: 0.09778936952352524, val accuracy: 0.958041958041958\n",
      "208 epoch에서 저장. 이전 score: 0.0978320837020874, 현재 score: 0.09778936952352524\n",
      "[209/500] train loss: 0.03735224902629852, val loss: 0.09774753451347351, val accuracy: 0.958041958041958\n",
      "209 epoch에서 저장. 이전 score: 0.09778936952352524, 현재 score: 0.09774753451347351\n",
      "[210/500] train loss: 0.037139710038900375, val loss: 0.09770539402961731, val accuracy: 0.958041958041958\n",
      "210 epoch에서 저장. 이전 score: 0.09774753451347351, 현재 score: 0.09770539402961731\n",
      "[211/500] train loss: 0.03692878037691116, val loss: 0.09766080975532532, val accuracy: 0.958041958041958\n",
      "211 epoch에서 저장. 이전 score: 0.09770539402961731, 현재 score: 0.09766080975532532\n",
      "[212/500] train loss: 0.03671887516975403, val loss: 0.0976114422082901, val accuracy: 0.958041958041958\n",
      "212 epoch에서 저장. 이전 score: 0.09766080975532532, 현재 score: 0.0976114422082901\n",
      "[213/500] train loss: 0.03651022911071777, val loss: 0.0975620225071907, val accuracy: 0.958041958041958\n",
      "213 epoch에서 저장. 이전 score: 0.0976114422082901, 현재 score: 0.0975620225071907\n",
      "[214/500] train loss: 0.03630289435386658, val loss: 0.09751110523939133, val accuracy: 0.958041958041958\n",
      "214 epoch에서 저장. 이전 score: 0.0975620225071907, 현재 score: 0.09751110523939133\n",
      "[215/500] train loss: 0.03609819337725639, val loss: 0.0974598079919815, val accuracy: 0.958041958041958\n",
      "215 epoch에서 저장. 이전 score: 0.09751110523939133, 현재 score: 0.0974598079919815\n",
      "[216/500] train loss: 0.035894766449928284, val loss: 0.09740998595952988, val accuracy: 0.958041958041958\n",
      "216 epoch에서 저장. 이전 score: 0.0974598079919815, 현재 score: 0.09740998595952988\n",
      "[217/500] train loss: 0.03569580987095833, val loss: 0.09736233949661255, val accuracy: 0.958041958041958\n",
      "217 epoch에서 저장. 이전 score: 0.09740998595952988, 현재 score: 0.09736233949661255\n",
      "[218/500] train loss: 0.03549661114811897, val loss: 0.09731639176607132, val accuracy: 0.958041958041958\n",
      "218 epoch에서 저장. 이전 score: 0.09736233949661255, 현재 score: 0.09731639176607132\n",
      "[219/500] train loss: 0.03529863804578781, val loss: 0.097270168364048, val accuracy: 0.958041958041958\n",
      "219 epoch에서 저장. 이전 score: 0.09731639176607132, 현재 score: 0.097270168364048\n",
      "[220/500] train loss: 0.03510125353932381, val loss: 0.09722398221492767, val accuracy: 0.958041958041958\n",
      "220 epoch에서 저장. 이전 score: 0.097270168364048, 현재 score: 0.09722398221492767\n",
      "[221/500] train loss: 0.034906428307294846, val loss: 0.09717907011508942, val accuracy: 0.958041958041958\n",
      "221 epoch에서 저장. 이전 score: 0.09722398221492767, 현재 score: 0.09717907011508942\n",
      "[222/500] train loss: 0.03471272811293602, val loss: 0.0971386507153511, val accuracy: 0.958041958041958\n",
      "222 epoch에서 저장. 이전 score: 0.09717907011508942, 현재 score: 0.0971386507153511\n",
      "[223/500] train loss: 0.0345248207449913, val loss: 0.09709648787975311, val accuracy: 0.958041958041958\n",
      "223 epoch에서 저장. 이전 score: 0.0971386507153511, 현재 score: 0.09709648787975311\n",
      "[224/500] train loss: 0.03433782607316971, val loss: 0.0970577597618103, val accuracy: 0.958041958041958\n",
      "224 epoch에서 저장. 이전 score: 0.09709648787975311, 현재 score: 0.0970577597618103\n",
      "[225/500] train loss: 0.03415052220225334, val loss: 0.09701400250196457, val accuracy: 0.958041958041958\n",
      "225 epoch에서 저장. 이전 score: 0.0970577597618103, 현재 score: 0.09701400250196457\n",
      "[226/500] train loss: 0.03396472707390785, val loss: 0.09696853905916214, val accuracy: 0.958041958041958\n",
      "226 epoch에서 저장. 이전 score: 0.09701400250196457, 현재 score: 0.09696853905916214\n",
      "[227/500] train loss: 0.03377927094697952, val loss: 0.0969218835234642, val accuracy: 0.958041958041958\n",
      "227 epoch에서 저장. 이전 score: 0.09696853905916214, 현재 score: 0.0969218835234642\n",
      "[228/500] train loss: 0.03359365463256836, val loss: 0.09687434136867523, val accuracy: 0.965034965034965\n",
      "228 epoch에서 저장. 이전 score: 0.0969218835234642, 현재 score: 0.09687434136867523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229/500] train loss: 0.0334082767367363, val loss: 0.09682408720254898, val accuracy: 0.965034965034965\n",
      "229 epoch에서 저장. 이전 score: 0.09687434136867523, 현재 score: 0.09682408720254898\n",
      "[230/500] train loss: 0.03322436660528183, val loss: 0.0967714935541153, val accuracy: 0.965034965034965\n",
      "230 epoch에서 저장. 이전 score: 0.09682408720254898, 현재 score: 0.0967714935541153\n",
      "[231/500] train loss: 0.033040717244148254, val loss: 0.0967160314321518, val accuracy: 0.965034965034965\n",
      "231 epoch에서 저장. 이전 score: 0.0967714935541153, 현재 score: 0.0967160314321518\n",
      "[232/500] train loss: 0.032859381288290024, val loss: 0.09666299819946289, val accuracy: 0.965034965034965\n",
      "232 epoch에서 저장. 이전 score: 0.0967160314321518, 현재 score: 0.09666299819946289\n",
      "[233/500] train loss: 0.03268008306622505, val loss: 0.09661456942558289, val accuracy: 0.965034965034965\n",
      "233 epoch에서 저장. 이전 score: 0.09666299819946289, 현재 score: 0.09661456942558289\n",
      "[234/500] train loss: 0.0325038842856884, val loss: 0.09657156467437744, val accuracy: 0.965034965034965\n",
      "234 epoch에서 저장. 이전 score: 0.09661456942558289, 현재 score: 0.09657156467437744\n",
      "[235/500] train loss: 0.03232770413160324, val loss: 0.09653525054454803, val accuracy: 0.965034965034965\n",
      "235 epoch에서 저장. 이전 score: 0.09657156467437744, 현재 score: 0.09653525054454803\n",
      "[236/500] train loss: 0.03215136379003525, val loss: 0.09650532156229019, val accuracy: 0.965034965034965\n",
      "236 epoch에서 저장. 이전 score: 0.09653525054454803, 현재 score: 0.09650532156229019\n",
      "[237/500] train loss: 0.031975194811820984, val loss: 0.09648239612579346, val accuracy: 0.965034965034965\n",
      "237 epoch에서 저장. 이전 score: 0.09650532156229019, 현재 score: 0.09648239612579346\n",
      "[238/500] train loss: 0.03179898113012314, val loss: 0.09646247327327728, val accuracy: 0.965034965034965\n",
      "238 epoch에서 저장. 이전 score: 0.09648239612579346, 현재 score: 0.09646247327327728\n",
      "[239/500] train loss: 0.03162365034222603, val loss: 0.09644250571727753, val accuracy: 0.965034965034965\n",
      "239 epoch에서 저장. 이전 score: 0.09646247327327728, 현재 score: 0.09644250571727753\n",
      "[240/500] train loss: 0.031449493020772934, val loss: 0.09642113000154495, val accuracy: 0.965034965034965\n",
      "240 epoch에서 저장. 이전 score: 0.09644250571727753, 현재 score: 0.09642113000154495\n",
      "[241/500] train loss: 0.03127766773104668, val loss: 0.096398264169693, val accuracy: 0.972027972027972\n",
      "241 epoch에서 저장. 이전 score: 0.09642113000154495, 현재 score: 0.096398264169693\n",
      "[242/500] train loss: 0.031107014045119286, val loss: 0.09637068212032318, val accuracy: 0.972027972027972\n",
      "242 epoch에서 저장. 이전 score: 0.096398264169693, 현재 score: 0.09637068212032318\n",
      "[243/500] train loss: 0.030939454212784767, val loss: 0.09634435921907425, val accuracy: 0.972027972027972\n",
      "243 epoch에서 저장. 이전 score: 0.09637068212032318, 현재 score: 0.09634435921907425\n",
      "[244/500] train loss: 0.030772848054766655, val loss: 0.09631717950105667, val accuracy: 0.972027972027972\n",
      "244 epoch에서 저장. 이전 score: 0.09634435921907425, 현재 score: 0.09631717950105667\n",
      "[245/500] train loss: 0.030606083571910858, val loss: 0.09628887474536896, val accuracy: 0.972027972027972\n",
      "245 epoch에서 저장. 이전 score: 0.09631717950105667, 현재 score: 0.09628887474536896\n",
      "[246/500] train loss: 0.030441412702202797, val loss: 0.09626288712024689, val accuracy: 0.972027972027972\n",
      "246 epoch에서 저장. 이전 score: 0.09628887474536896, 현재 score: 0.09626288712024689\n",
      "[247/500] train loss: 0.030277647078037262, val loss: 0.09623511880636215, val accuracy: 0.972027972027972\n",
      "247 epoch에서 저장. 이전 score: 0.09626288712024689, 현재 score: 0.09623511880636215\n",
      "[248/500] train loss: 0.030116012319922447, val loss: 0.09620895981788635, val accuracy: 0.972027972027972\n",
      "248 epoch에서 저장. 이전 score: 0.09623511880636215, 현재 score: 0.09620895981788635\n",
      "[249/500] train loss: 0.02995356172323227, val loss: 0.09618927538394928, val accuracy: 0.972027972027972\n",
      "249 epoch에서 저장. 이전 score: 0.09620895981788635, 현재 score: 0.09618927538394928\n",
      "[250/500] train loss: 0.029791170731186867, val loss: 0.09617213159799576, val accuracy: 0.972027972027972\n",
      "250 epoch에서 저장. 이전 score: 0.09618927538394928, 현재 score: 0.09617213159799576\n",
      "[251/500] train loss: 0.02962963841855526, val loss: 0.0961531326174736, val accuracy: 0.972027972027972\n",
      "251 epoch에서 저장. 이전 score: 0.09617213159799576, 현재 score: 0.0961531326174736\n",
      "[252/500] train loss: 0.029469367116689682, val loss: 0.09613510221242905, val accuracy: 0.972027972027972\n",
      "252 epoch에서 저장. 이전 score: 0.0961531326174736, 현재 score: 0.09613510221242905\n",
      "[253/500] train loss: 0.029309187084436417, val loss: 0.09611496329307556, val accuracy: 0.972027972027972\n",
      "253 epoch에서 저장. 이전 score: 0.09613510221242905, 현재 score: 0.09611496329307556\n",
      "[254/500] train loss: 0.029149148613214493, val loss: 0.0960908979177475, val accuracy: 0.972027972027972\n",
      "254 epoch에서 저장. 이전 score: 0.09611496329307556, 현재 score: 0.0960908979177475\n",
      "[255/500] train loss: 0.0289893951267004, val loss: 0.09606608003377914, val accuracy: 0.972027972027972\n",
      "255 epoch에서 저장. 이전 score: 0.0960908979177475, 현재 score: 0.09606608003377914\n",
      "[256/500] train loss: 0.02883153036236763, val loss: 0.09604530036449432, val accuracy: 0.972027972027972\n",
      "256 epoch에서 저장. 이전 score: 0.09606608003377914, 현재 score: 0.09604530036449432\n",
      "[257/500] train loss: 0.028673391789197922, val loss: 0.09602507948875427, val accuracy: 0.972027972027972\n",
      "257 epoch에서 저장. 이전 score: 0.09604530036449432, 현재 score: 0.09602507948875427\n",
      "[258/500] train loss: 0.02851642481982708, val loss: 0.09600987285375595, val accuracy: 0.972027972027972\n",
      "258 epoch에서 저장. 이전 score: 0.09602507948875427, 현재 score: 0.09600987285375595\n",
      "[259/500] train loss: 0.028359631076455116, val loss: 0.09599638730287552, val accuracy: 0.972027972027972\n",
      "259 epoch에서 저장. 이전 score: 0.09600987285375595, 현재 score: 0.09599638730287552\n",
      "[260/500] train loss: 0.028204498812556267, val loss: 0.09598369151353836, val accuracy: 0.972027972027972\n",
      "260 epoch에서 저장. 이전 score: 0.09599638730287552, 현재 score: 0.09598369151353836\n",
      "[261/500] train loss: 0.028049133718013763, val loss: 0.09597184509038925, val accuracy: 0.972027972027972\n",
      "261 epoch에서 저장. 이전 score: 0.09598369151353836, 현재 score: 0.09597184509038925\n",
      "[262/500] train loss: 0.027892963960766792, val loss: 0.09596388787031174, val accuracy: 0.972027972027972\n",
      "262 epoch에서 저장. 이전 score: 0.09597184509038925, 현재 score: 0.09596388787031174\n",
      "[263/500] train loss: 0.027737680822610855, val loss: 0.09595302492380142, val accuracy: 0.972027972027972\n",
      "263 epoch에서 저장. 이전 score: 0.09596388787031174, 현재 score: 0.09595302492380142\n",
      "[264/500] train loss: 0.027587367221713066, val loss: 0.09594017267227173, val accuracy: 0.972027972027972\n",
      "264 epoch에서 저장. 이전 score: 0.09595302492380142, 현재 score: 0.09594017267227173\n",
      "[265/500] train loss: 0.02743840031325817, val loss: 0.0959220603108406, val accuracy: 0.972027972027972\n",
      "265 epoch에서 저장. 이전 score: 0.09594017267227173, 현재 score: 0.0959220603108406\n",
      "[266/500] train loss: 0.02729227766394615, val loss: 0.09589775651693344, val accuracy: 0.972027972027972\n",
      "266 epoch에서 저장. 이전 score: 0.0959220603108406, 현재 score: 0.09589775651693344\n",
      "[267/500] train loss: 0.027145830914378166, val loss: 0.09588620066642761, val accuracy: 0.972027972027972\n",
      "267 epoch에서 저장. 이전 score: 0.09589775651693344, 현재 score: 0.09588620066642761\n",
      "[268/500] train loss: 0.027001161128282547, val loss: 0.0958772599697113, val accuracy: 0.972027972027972\n",
      "268 epoch에서 저장. 이전 score: 0.09588620066642761, 현재 score: 0.0958772599697113\n",
      "[269/500] train loss: 0.026856331154704094, val loss: 0.09586510062217712, val accuracy: 0.972027972027972\n",
      "269 epoch에서 저장. 이전 score: 0.0958772599697113, 현재 score: 0.09586510062217712\n",
      "[270/500] train loss: 0.02671097218990326, val loss: 0.09585122764110565, val accuracy: 0.972027972027972\n",
      "270 epoch에서 저장. 이전 score: 0.09586510062217712, 현재 score: 0.09585122764110565\n",
      "[271/500] train loss: 0.026570815593004227, val loss: 0.09583736956119537, val accuracy: 0.972027972027972\n",
      "271 epoch에서 저장. 이전 score: 0.09585122764110565, 현재 score: 0.09583736956119537\n",
      "[272/500] train loss: 0.026431409642100334, val loss: 0.095814548432827, val accuracy: 0.972027972027972\n",
      "272 epoch에서 저장. 이전 score: 0.09583736956119537, 현재 score: 0.095814548432827\n",
      "[273/500] train loss: 0.026291044428944588, val loss: 0.09579121321439743, val accuracy: 0.972027972027972\n",
      "273 epoch에서 저장. 이전 score: 0.095814548432827, 현재 score: 0.09579121321439743\n",
      "[274/500] train loss: 0.026151832193136215, val loss: 0.09576915949583054, val accuracy: 0.972027972027972\n",
      "274 epoch에서 저장. 이전 score: 0.09579121321439743, 현재 score: 0.09576915949583054\n",
      "[275/500] train loss: 0.02601240947842598, val loss: 0.09574809670448303, val accuracy: 0.972027972027972\n",
      "275 epoch에서 저장. 이전 score: 0.09576915949583054, 현재 score: 0.09574809670448303\n",
      "[276/500] train loss: 0.025874784216284752, val loss: 0.09572556614875793, val accuracy: 0.972027972027972\n",
      "276 epoch에서 저장. 이전 score: 0.09574809670448303, 현재 score: 0.09572556614875793\n",
      "[277/500] train loss: 0.025736358016729355, val loss: 0.09570127725601196, val accuracy: 0.972027972027972\n",
      "277 epoch에서 저장. 이전 score: 0.09572556614875793, 현재 score: 0.09570127725601196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278/500] train loss: 0.02559776045382023, val loss: 0.09568211436271667, val accuracy: 0.972027972027972\n",
      "278 epoch에서 저장. 이전 score: 0.09570127725601196, 현재 score: 0.09568211436271667\n",
      "[279/500] train loss: 0.025461256504058838, val loss: 0.09566379338502884, val accuracy: 0.972027972027972\n",
      "279 epoch에서 저장. 이전 score: 0.09568211436271667, 현재 score: 0.09566379338502884\n",
      "[280/500] train loss: 0.025325367227196693, val loss: 0.09564615786075592, val accuracy: 0.972027972027972\n",
      "280 epoch에서 저장. 이전 score: 0.09566379338502884, 현재 score: 0.09564615786075592\n",
      "[281/500] train loss: 0.025189358741044998, val loss: 0.09563092887401581, val accuracy: 0.972027972027972\n",
      "281 epoch에서 저장. 이전 score: 0.09564615786075592, 현재 score: 0.09563092887401581\n",
      "[282/500] train loss: 0.02505399100482464, val loss: 0.09560950100421906, val accuracy: 0.972027972027972\n",
      "282 epoch에서 저장. 이전 score: 0.09563092887401581, 현재 score: 0.09560950100421906\n",
      "[283/500] train loss: 0.024920862168073654, val loss: 0.09558483213186264, val accuracy: 0.972027972027972\n",
      "283 epoch에서 저장. 이전 score: 0.09560950100421906, 현재 score: 0.09558483213186264\n",
      "[284/500] train loss: 0.024788210168480873, val loss: 0.0955580547451973, val accuracy: 0.965034965034965\n",
      "284 epoch에서 저장. 이전 score: 0.09558483213186264, 현재 score: 0.0955580547451973\n",
      "[285/500] train loss: 0.024656204506754875, val loss: 0.09553570300340652, val accuracy: 0.965034965034965\n",
      "285 epoch에서 저장. 이전 score: 0.0955580547451973, 현재 score: 0.09553570300340652\n",
      "[286/500] train loss: 0.024526186287403107, val loss: 0.09552064538002014, val accuracy: 0.965034965034965\n",
      "286 epoch에서 저장. 이전 score: 0.09553570300340652, 현재 score: 0.09552064538002014\n",
      "[287/500] train loss: 0.024397142231464386, val loss: 0.09551199525594711, val accuracy: 0.965034965034965\n",
      "287 epoch에서 저장. 이전 score: 0.09552064538002014, 현재 score: 0.09551199525594711\n",
      "[288/500] train loss: 0.024267716333270073, val loss: 0.0955142006278038, val accuracy: 0.965034965034965\n",
      "[289/500] train loss: 0.024138769134879112, val loss: 0.0955166444182396, val accuracy: 0.965034965034965\n",
      "[290/500] train loss: 0.024010809138417244, val loss: 0.09551933407783508, val accuracy: 0.965034965034965\n",
      "[291/500] train loss: 0.023882931098341942, val loss: 0.09551402926445007, val accuracy: 0.965034965034965\n",
      "[292/500] train loss: 0.023755481466650963, val loss: 0.09550835192203522, val accuracy: 0.965034965034965\n",
      "292 epoch에서 저장. 이전 score: 0.09551199525594711, 현재 score: 0.09550835192203522\n",
      "[293/500] train loss: 0.023629015311598778, val loss: 0.09550651162862778, val accuracy: 0.965034965034965\n",
      "293 epoch에서 저장. 이전 score: 0.09550835192203522, 현재 score: 0.09550651162862778\n",
      "[294/500] train loss: 0.02350393496453762, val loss: 0.0955086201429367, val accuracy: 0.965034965034965\n",
      "[295/500] train loss: 0.023378072306513786, val loss: 0.09551268070936203, val accuracy: 0.965034965034965\n",
      "[296/500] train loss: 0.023252882063388824, val loss: 0.09551835060119629, val accuracy: 0.965034965034965\n",
      "[297/500] train loss: 0.02312857285141945, val loss: 0.09552373737096786, val accuracy: 0.965034965034965\n",
      "[298/500] train loss: 0.023004313930869102, val loss: 0.09552740305662155, val accuracy: 0.965034965034965\n",
      "[299/500] train loss: 0.022881686687469482, val loss: 0.09553108364343643, val accuracy: 0.965034965034965\n",
      "[300/500] train loss: 0.02275909297168255, val loss: 0.09553227573633194, val accuracy: 0.965034965034965\n",
      "[301/500] train loss: 0.022637132555246353, val loss: 0.09553299844264984, val accuracy: 0.965034965034965\n",
      "[302/500] train loss: 0.02251613698899746, val loss: 0.09554161876440048, val accuracy: 0.965034965034965\n",
      "[303/500] train loss: 0.022394733503460884, val loss: 0.09554845839738846, val accuracy: 0.965034965034965\n",
      "302 epoch에서 종료. 0.09550651162862778에서 성능이 개선되지 않음.\n",
      "2.424805164337158초 걸림\n"
     ]
    }
   ],
   "source": [
    "### 학습\n",
    "N_EPOCH = 500\n",
    "train_loss_list, val_loss_list, val_acc_list = [], [], []\n",
    "\n",
    "model = BreastCancerModel().to(device)\n",
    "loss_fn = nn.BCELoss()  # 이진분류-positive확률출력모델의 loss-Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# early stopping, 모델 저장을 위한 변수\n",
    "best_score = torch.inf\n",
    "save_model_path_bc = 'models/breast_cancer_best_model.pt'\n",
    "\n",
    "patience = 10 # 개선될때까지 몇번 기다릴지.\n",
    "trigger_cnt = 0 # 몇번째 기다렸는지 \n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(N_EPOCH):\n",
    "    #### 학습\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X, y in wb_train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 추정                         \n",
    "        pred = model(X) # positive의 확률 - loss 계산\n",
    "        loss = loss_fn(pred, y)\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    # 학습 종료 -> train loss 평균\n",
    "    train_loss /= len(wb_train_loader)\n",
    "    #### 검증\n",
    "    model.eval()\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in wb_test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            \n",
    "            # 추정\n",
    "            pred_val = model(X_val)  # positive의 확률  0.XXXX -> loss계산\n",
    "                        \n",
    "            #type(타입)=>타입변환. bool=>정수. True->1, False->0\n",
    "            pred_label = (pred_val >= 0.5).type(torch.int32) # accuracy 계산\n",
    "               \n",
    "            loss_val = loss_fn(pred_val, y_val)\n",
    "            val_loss += loss_val.item()\n",
    "            val_acc +=  torch.sum(pred_label == y_val).item()\n",
    "        \n",
    "        val_loss /= len(wb_test_loader)\n",
    "        val_acc /= len(wb_test_loader.dataset)\n",
    "    \n",
    "    # 현재 epoch 학습/검증 종료\n",
    "    # 로그출력\n",
    "    print(f\"[{epoch+1}/{N_EPOCH}] train loss: {train_loss}, val loss: {val_loss}, val accuracy: {val_acc}\")\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    \n",
    "    #### early stopping, model 저장 처리.\n",
    "    if val_loss < best_score: # 성능 개선\n",
    "        print(f\"{epoch+1} epoch에서 저장. 이전 score: {best_score}, 현재 score: {val_loss}\")\n",
    "        best_score = val_loss\n",
    "        torch.save(model, save_model_path_bc)\n",
    "        trigger_cnt = 0\n",
    "    else: # 현재 epoch에서 성능 개선이 안됨.\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt: # 조기 종료\n",
    "            print(f\"{epoch+1} epoch에서 종료. {best_score}에서 성능이 개선되지 않음.\")\n",
    "            break\n",
    "\n",
    "e = time.time()\n",
    "print(f\"{e-s}초 걸림\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BreastCancerModel                        [100, 1]                  --\n",
       "├─Linear: 1-1                            [100, 32]                 992\n",
       "├─Linear: 1-2                            [100, 16]                 528\n",
       "├─Linear: 1-3                            [100, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 1,537\n",
       "Trainable params: 1,537\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model, (100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"LOSS\")\n",
    "plt.plot(train_loss_list, label=\"Train\")\n",
    "plt.plot(val_loss_list, label=\"Validation\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(val_acc_list)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BreastCancerModel(\n",
       "  (lr1): Linear(in_features=30, out_features=32, bias=True)\n",
       "  (lr2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (output): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "load_model = torch.load(save_model_path_bc)\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for X_val, y_val in wb_test_loader:\n",
    "        X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "        # 추정\n",
    "        pred_val = load_model(X_val)  # positive의 확률  0.XXXX -> loss계산\n",
    "\n",
    "        #type(타입)=>타입변환. bool=>정수. True->1, False->0\n",
    "        pred_label = (pred_val >= 0.5).type(torch.int32) # accuracy 계산\n",
    "\n",
    "        loss_val = loss_fn(pred_val, y_val)\n",
    "        val_loss += loss_val.item()\n",
    "        val_acc +=  torch.sum(pred_label == y_val).item()\n",
    "\n",
    "    val_loss /= len(wb_test_loader)\n",
    "    val_acc /= len(wb_test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09550651162862778 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BreastCancerModel()\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100] - Train loss: 0.70621 Train Accucracy: 0.37559 || Validation Loss: 0.70954 Validation Accuracy: 0.37762\n",
      "====================================================================================================\n",
      "저장: 1 - 이전 : inf, 현재: 0.7095409631729126\n",
      "Epoch[2/100] - Train loss: 0.69890 Train Accucracy: 0.37559 || Validation Loss: 0.70296 Validation Accuracy: 0.37762\n",
      "====================================================================================================\n",
      "저장: 2 - 이전 : 0.7095409631729126, 현재: 0.7029569149017334\n",
      "Epoch[3/100] - Train loss: 0.69172 Train Accucracy: 0.37793 || Validation Loss: 0.69654 Validation Accuracy: 0.37063\n",
      "====================================================================================================\n",
      "저장: 3 - 이전 : 0.7029569149017334, 현재: 0.6965402364730835\n",
      "Epoch[4/100] - Train loss: 0.68465 Train Accucracy: 0.38028 || Validation Loss: 0.69021 Validation Accuracy: 0.37063\n",
      "====================================================================================================\n",
      "저장: 4 - 이전 : 0.6965402364730835, 현재: 0.6902101039886475\n",
      "Epoch[5/100] - Train loss: 0.67769 Train Accucracy: 0.38028 || Validation Loss: 0.68395 Validation Accuracy: 0.37762\n",
      "====================================================================================================\n",
      "저장: 5 - 이전 : 0.6902101039886475, 현재: 0.6839491128921509\n",
      "Epoch[6/100] - Train loss: 0.67083 Train Accucracy: 0.38498 || Validation Loss: 0.67779 Validation Accuracy: 0.37762\n",
      "====================================================================================================\n",
      "저장: 6 - 이전 : 0.6839491128921509, 현재: 0.6777855157852173\n",
      "Epoch[7/100] - Train loss: 0.66401 Train Accucracy: 0.38498 || Validation Loss: 0.67174 Validation Accuracy: 0.39161\n",
      "====================================================================================================\n",
      "저장: 7 - 이전 : 0.6777855157852173, 현재: 0.6717358827590942\n",
      "Epoch[8/100] - Train loss: 0.65727 Train Accucracy: 0.39671 || Validation Loss: 0.66572 Validation Accuracy: 0.41259\n",
      "====================================================================================================\n",
      "저장: 8 - 이전 : 0.6717358827590942, 현재: 0.6657228469848633\n",
      "Epoch[9/100] - Train loss: 0.65059 Train Accucracy: 0.41784 || Validation Loss: 0.65971 Validation Accuracy: 0.44755\n",
      "====================================================================================================\n",
      "저장: 9 - 이전 : 0.6657228469848633, 현재: 0.6597076654434204\n",
      "Epoch[10/100] - Train loss: 0.64394 Train Accucracy: 0.44131 || Validation Loss: 0.65370 Validation Accuracy: 0.46154\n",
      "====================================================================================================\n",
      "저장: 10 - 이전 : 0.6597076654434204, 현재: 0.6537004113197327\n",
      "Epoch[11/100] - Train loss: 0.63731 Train Accucracy: 0.48357 || Validation Loss: 0.64771 Validation Accuracy: 0.48252\n",
      "====================================================================================================\n",
      "저장: 11 - 이전 : 0.6537004113197327, 현재: 0.6477149128913879\n",
      "Epoch[12/100] - Train loss: 0.63067 Train Accucracy: 0.52347 || Validation Loss: 0.64179 Validation Accuracy: 0.51049\n",
      "====================================================================================================\n",
      "저장: 12 - 이전 : 0.6477149128913879, 현재: 0.6417946219444275\n",
      "Epoch[13/100] - Train loss: 0.62399 Train Accucracy: 0.58451 || Validation Loss: 0.63586 Validation Accuracy: 0.53147\n",
      "====================================================================================================\n",
      "저장: 13 - 이전 : 0.6417946219444275, 현재: 0.6358612775802612\n",
      "Epoch[14/100] - Train loss: 0.61730 Train Accucracy: 0.61972 || Validation Loss: 0.62985 Validation Accuracy: 0.55245\n",
      "====================================================================================================\n",
      "저장: 14 - 이전 : 0.6358612775802612, 현재: 0.6298456192016602\n",
      "Epoch[15/100] - Train loss: 0.61056 Train Accucracy: 0.65962 || Validation Loss: 0.62378 Validation Accuracy: 0.59441\n",
      "====================================================================================================\n",
      "저장: 15 - 이전 : 0.6298456192016602, 현재: 0.6237820386886597\n",
      "Epoch[16/100] - Train loss: 0.60375 Train Accucracy: 0.70423 || Validation Loss: 0.61765 Validation Accuracy: 0.62937\n",
      "====================================================================================================\n",
      "저장: 16 - 이전 : 0.6237820386886597, 현재: 0.6176451444625854\n",
      "Epoch[17/100] - Train loss: 0.59685 Train Accucracy: 0.74648 || Validation Loss: 0.61145 Validation Accuracy: 0.66434\n",
      "====================================================================================================\n",
      "저장: 17 - 이전 : 0.6176451444625854, 현재: 0.6114515662193298\n",
      "Epoch[18/100] - Train loss: 0.58984 Train Accucracy: 0.78169 || Validation Loss: 0.60517 Validation Accuracy: 0.70629\n",
      "====================================================================================================\n",
      "저장: 18 - 이전 : 0.6114515662193298, 현재: 0.6051693558692932\n",
      "Epoch[19/100] - Train loss: 0.58279 Train Accucracy: 0.80751 || Validation Loss: 0.59877 Validation Accuracy: 0.74825\n",
      "====================================================================================================\n",
      "저장: 19 - 이전 : 0.6051693558692932, 현재: 0.5987688899040222\n",
      "Epoch[20/100] - Train loss: 0.57564 Train Accucracy: 0.83099 || Validation Loss: 0.59224 Validation Accuracy: 0.76224\n",
      "====================================================================================================\n",
      "저장: 20 - 이전 : 0.5987688899040222, 현재: 0.5922425985336304\n",
      "Epoch[21/100] - Train loss: 0.56840 Train Accucracy: 0.83568 || Validation Loss: 0.58565 Validation Accuracy: 0.81119\n",
      "====================================================================================================\n",
      "저장: 21 - 이전 : 0.5922425985336304, 현재: 0.5856503844261169\n",
      "Epoch[22/100] - Train loss: 0.56109 Train Accucracy: 0.85915 || Validation Loss: 0.57900 Validation Accuracy: 0.81119\n",
      "====================================================================================================\n",
      "저장: 22 - 이전 : 0.5856503844261169, 현재: 0.5789971947669983\n",
      "Epoch[23/100] - Train loss: 0.55369 Train Accucracy: 0.86854 || Validation Loss: 0.57220 Validation Accuracy: 0.82517\n",
      "====================================================================================================\n",
      "저장: 23 - 이전 : 0.5789971947669983, 현재: 0.5721982717514038\n",
      "Epoch[24/100] - Train loss: 0.54617 Train Accucracy: 0.88967 || Validation Loss: 0.56522 Validation Accuracy: 0.83217\n",
      "====================================================================================================\n",
      "저장: 24 - 이전 : 0.5721982717514038, 현재: 0.5652164816856384\n",
      "Epoch[25/100] - Train loss: 0.53850 Train Accucracy: 0.89906 || Validation Loss: 0.55809 Validation Accuracy: 0.85315\n",
      "====================================================================================================\n",
      "저장: 25 - 이전 : 0.5652164816856384, 현재: 0.558093249797821\n",
      "Epoch[26/100] - Train loss: 0.53070 Train Accucracy: 0.90610 || Validation Loss: 0.55081 Validation Accuracy: 0.86014\n",
      "====================================================================================================\n",
      "저장: 26 - 이전 : 0.558093249797821, 현재: 0.5508085489273071\n",
      "Epoch[27/100] - Train loss: 0.52277 Train Accucracy: 0.91315 || Validation Loss: 0.54338 Validation Accuracy: 0.85315\n",
      "====================================================================================================\n",
      "저장: 27 - 이전 : 0.5508085489273071, 현재: 0.5433765649795532\n",
      "Epoch[28/100] - Train loss: 0.51468 Train Accucracy: 0.92254 || Validation Loss: 0.53580 Validation Accuracy: 0.86014\n",
      "====================================================================================================\n",
      "저장: 28 - 이전 : 0.5433765649795532, 현재: 0.535802960395813\n",
      "Epoch[29/100] - Train loss: 0.50647 Train Accucracy: 0.92723 || Validation Loss: 0.52810 Validation Accuracy: 0.88112\n",
      "====================================================================================================\n",
      "저장: 29 - 이전 : 0.535802960395813, 현재: 0.5280952453613281\n",
      "Epoch[30/100] - Train loss: 0.49809 Train Accucracy: 0.93662 || Validation Loss: 0.52025 Validation Accuracy: 0.88811\n",
      "====================================================================================================\n",
      "저장: 30 - 이전 : 0.5280952453613281, 현재: 0.5202476382255554\n",
      "Epoch[31/100] - Train loss: 0.48956 Train Accucracy: 0.94131 || Validation Loss: 0.51226 Validation Accuracy: 0.88811\n",
      "====================================================================================================\n",
      "저장: 31 - 이전 : 0.5202476382255554, 현재: 0.5122577548027039\n",
      "Epoch[32/100] - Train loss: 0.48092 Train Accucracy: 0.94601 || Validation Loss: 0.50412 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 32 - 이전 : 0.5122577548027039, 현재: 0.5041224360466003\n",
      "Epoch[33/100] - Train loss: 0.47214 Train Accucracy: 0.94836 || Validation Loss: 0.49586 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 33 - 이전 : 0.5041224360466003, 현재: 0.4958576560020447\n",
      "Epoch[34/100] - Train loss: 0.46323 Train Accucracy: 0.95070 || Validation Loss: 0.48748 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 34 - 이전 : 0.4958576560020447, 현재: 0.4874762296676636\n",
      "Epoch[35/100] - Train loss: 0.45422 Train Accucracy: 0.95540 || Validation Loss: 0.47898 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 35 - 이전 : 0.4874762296676636, 현재: 0.47898241877555847\n",
      "Epoch[36/100] - Train loss: 0.44511 Train Accucracy: 0.95775 || Validation Loss: 0.47038 Validation Accuracy: 0.90210\n",
      "====================================================================================================\n",
      "저장: 36 - 이전 : 0.47898241877555847, 현재: 0.4703805148601532\n",
      "Epoch[37/100] - Train loss: 0.43591 Train Accucracy: 0.95775 || Validation Loss: 0.46167 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 37 - 이전 : 0.4703805148601532, 현재: 0.46167200803756714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[38/100] - Train loss: 0.42662 Train Accucracy: 0.95775 || Validation Loss: 0.45287 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 38 - 이전 : 0.46167200803756714, 현재: 0.4528667628765106\n",
      "Epoch[39/100] - Train loss: 0.41725 Train Accucracy: 0.96479 || Validation Loss: 0.44400 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 39 - 이전 : 0.4528667628765106, 현재: 0.4439961910247803\n",
      "Epoch[40/100] - Train loss: 0.40780 Train Accucracy: 0.96479 || Validation Loss: 0.43506 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 40 - 이전 : 0.4439961910247803, 현재: 0.4350636601448059\n",
      "Epoch[41/100] - Train loss: 0.39831 Train Accucracy: 0.96479 || Validation Loss: 0.42607 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 41 - 이전 : 0.4350636601448059, 현재: 0.42607229948043823\n",
      "Epoch[42/100] - Train loss: 0.38878 Train Accucracy: 0.96714 || Validation Loss: 0.41703 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 42 - 이전 : 0.42607229948043823, 현재: 0.41703352332115173\n",
      "Epoch[43/100] - Train loss: 0.37922 Train Accucracy: 0.96948 || Validation Loss: 0.40796 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 43 - 이전 : 0.41703352332115173, 현재: 0.4079585075378418\n",
      "Epoch[44/100] - Train loss: 0.36965 Train Accucracy: 0.96948 || Validation Loss: 0.39885 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 44 - 이전 : 0.4079585075378418, 현재: 0.3988530933856964\n",
      "Epoch[45/100] - Train loss: 0.36010 Train Accucracy: 0.96948 || Validation Loss: 0.38972 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 45 - 이전 : 0.3988530933856964, 현재: 0.3897208869457245\n",
      "Epoch[46/100] - Train loss: 0.35057 Train Accucracy: 0.96948 || Validation Loss: 0.38061 Validation Accuracy: 0.90909\n",
      "====================================================================================================\n",
      "저장: 46 - 이전 : 0.3897208869457245, 현재: 0.3806088864803314\n",
      "Epoch[47/100] - Train loss: 0.34109 Train Accucracy: 0.96948 || Validation Loss: 0.37154 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 47 - 이전 : 0.3806088864803314, 현재: 0.37153586745262146\n",
      "Epoch[48/100] - Train loss: 0.33168 Train Accucracy: 0.96948 || Validation Loss: 0.36253 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 48 - 이전 : 0.37153586745262146, 현재: 0.36253488063812256\n",
      "Epoch[49/100] - Train loss: 0.32237 Train Accucracy: 0.96948 || Validation Loss: 0.35363 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 49 - 이전 : 0.36253488063812256, 현재: 0.3536345064640045\n",
      "Epoch[50/100] - Train loss: 0.31317 Train Accucracy: 0.96948 || Validation Loss: 0.34485 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 50 - 이전 : 0.3536345064640045, 현재: 0.3448536992073059\n",
      "Epoch[51/100] - Train loss: 0.30410 Train Accucracy: 0.97183 || Validation Loss: 0.33619 Validation Accuracy: 0.91608\n",
      "====================================================================================================\n",
      "저장: 51 - 이전 : 0.3448536992073059, 현재: 0.3361901044845581\n",
      "Epoch[52/100] - Train loss: 0.29517 Train Accucracy: 0.97183 || Validation Loss: 0.32767 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 52 - 이전 : 0.3361901044845581, 현재: 0.3276745676994324\n",
      "Epoch[53/100] - Train loss: 0.28639 Train Accucracy: 0.97183 || Validation Loss: 0.31932 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 53 - 이전 : 0.3276745676994324, 현재: 0.31931862235069275\n",
      "Epoch[54/100] - Train loss: 0.27779 Train Accucracy: 0.96948 || Validation Loss: 0.31113 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 54 - 이전 : 0.31931862235069275, 현재: 0.3111265003681183\n",
      "Epoch[55/100] - Train loss: 0.26937 Train Accucracy: 0.96948 || Validation Loss: 0.30313 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 55 - 이전 : 0.3111265003681183, 현재: 0.3031303882598877\n",
      "Epoch[56/100] - Train loss: 0.26114 Train Accucracy: 0.96948 || Validation Loss: 0.29534 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 56 - 이전 : 0.3031303882598877, 현재: 0.29533761739730835\n",
      "Epoch[57/100] - Train loss: 0.25312 Train Accucracy: 0.96948 || Validation Loss: 0.28774 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 57 - 이전 : 0.29533761739730835, 현재: 0.2877441346645355\n",
      "Epoch[58/100] - Train loss: 0.24532 Train Accucracy: 0.96948 || Validation Loss: 0.28036 Validation Accuracy: 0.92308\n",
      "====================================================================================================\n",
      "저장: 58 - 이전 : 0.2877441346645355, 현재: 0.28036171197891235\n",
      "Epoch[59/100] - Train loss: 0.23774 Train Accucracy: 0.96948 || Validation Loss: 0.27320 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 59 - 이전 : 0.28036171197891235, 현재: 0.2731964886188507\n",
      "Epoch[60/100] - Train loss: 0.23040 Train Accucracy: 0.96948 || Validation Loss: 0.26626 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 60 - 이전 : 0.2731964886188507, 현재: 0.26626142859458923\n",
      "Epoch[61/100] - Train loss: 0.22329 Train Accucracy: 0.96948 || Validation Loss: 0.25955 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 61 - 이전 : 0.26626142859458923, 현재: 0.25955355167388916\n",
      "Epoch[62/100] - Train loss: 0.21642 Train Accucracy: 0.96948 || Validation Loss: 0.25308 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 62 - 이전 : 0.25955355167388916, 현재: 0.2530781924724579\n",
      "Epoch[63/100] - Train loss: 0.20980 Train Accucracy: 0.96948 || Validation Loss: 0.24684 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 63 - 이전 : 0.2530781924724579, 현재: 0.2468411922454834\n",
      "Epoch[64/100] - Train loss: 0.20343 Train Accucracy: 0.96948 || Validation Loss: 0.24084 Validation Accuracy: 0.93007\n",
      "====================================================================================================\n",
      "저장: 64 - 이전 : 0.2468411922454834, 현재: 0.24084250628948212\n",
      "Epoch[65/100] - Train loss: 0.19730 Train Accucracy: 0.97183 || Validation Loss: 0.23508 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 65 - 이전 : 0.24084250628948212, 현재: 0.2350825071334839\n",
      "Epoch[66/100] - Train loss: 0.19142 Train Accucracy: 0.97418 || Validation Loss: 0.22956 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 66 - 이전 : 0.2350825071334839, 현재: 0.22956423461437225\n",
      "Epoch[67/100] - Train loss: 0.18577 Train Accucracy: 0.97418 || Validation Loss: 0.22428 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 67 - 이전 : 0.22956423461437225, 현재: 0.22427891194820404\n",
      "Epoch[68/100] - Train loss: 0.18035 Train Accucracy: 0.97418 || Validation Loss: 0.21922 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 68 - 이전 : 0.22427891194820404, 현재: 0.2192213386297226\n",
      "Epoch[69/100] - Train loss: 0.17517 Train Accucracy: 0.97418 || Validation Loss: 0.21439 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 69 - 이전 : 0.2192213386297226, 현재: 0.21438613533973694\n",
      "Epoch[70/100] - Train loss: 0.17021 Train Accucracy: 0.97418 || Validation Loss: 0.20976 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 70 - 이전 : 0.21438613533973694, 현재: 0.20975863933563232\n",
      "Epoch[71/100] - Train loss: 0.16546 Train Accucracy: 0.97418 || Validation Loss: 0.20534 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 71 - 이전 : 0.20975863933563232, 현재: 0.205343559384346\n",
      "Epoch[72/100] - Train loss: 0.16093 Train Accucracy: 0.97418 || Validation Loss: 0.20113 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 72 - 이전 : 0.205343559384346, 현재: 0.20112797617912292\n",
      "Epoch[73/100] - Train loss: 0.15659 Train Accucracy: 0.97418 || Validation Loss: 0.19711 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 73 - 이전 : 0.20112797617912292, 현재: 0.19710877537727356\n",
      "Epoch[74/100] - Train loss: 0.15245 Train Accucracy: 0.97418 || Validation Loss: 0.19328 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 74 - 이전 : 0.19710877537727356, 현재: 0.1932818740606308\n",
      "Epoch[75/100] - Train loss: 0.14850 Train Accucracy: 0.97418 || Validation Loss: 0.18963 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 75 - 이전 : 0.1932818740606308, 현재: 0.1896335780620575\n",
      "Epoch[76/100] - Train loss: 0.14472 Train Accucracy: 0.97418 || Validation Loss: 0.18616 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 76 - 이전 : 0.1896335780620575, 현재: 0.18616017699241638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[77/100] - Train loss: 0.14111 Train Accucracy: 0.97418 || Validation Loss: 0.18285 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 77 - 이전 : 0.18616017699241638, 현재: 0.18284811079502106\n",
      "Epoch[78/100] - Train loss: 0.13767 Train Accucracy: 0.97418 || Validation Loss: 0.17969 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 78 - 이전 : 0.18284811079502106, 현재: 0.1796851009130478\n",
      "Epoch[79/100] - Train loss: 0.13437 Train Accucracy: 0.97887 || Validation Loss: 0.17667 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 79 - 이전 : 0.1796851009130478, 현재: 0.17666594684123993\n",
      "Epoch[80/100] - Train loss: 0.13123 Train Accucracy: 0.97887 || Validation Loss: 0.17379 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 80 - 이전 : 0.17666594684123993, 현재: 0.17379026114940643\n",
      "Epoch[81/100] - Train loss: 0.12823 Train Accucracy: 0.97887 || Validation Loss: 0.17104 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 81 - 이전 : 0.17379026114940643, 현재: 0.17103752493858337\n",
      "Epoch[82/100] - Train loss: 0.12536 Train Accucracy: 0.97887 || Validation Loss: 0.16840 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 82 - 이전 : 0.17103752493858337, 현재: 0.16839545965194702\n",
      "Epoch[83/100] - Train loss: 0.12261 Train Accucracy: 0.97887 || Validation Loss: 0.16587 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 83 - 이전 : 0.16839545965194702, 현재: 0.16586905717849731\n",
      "Epoch[84/100] - Train loss: 0.11998 Train Accucracy: 0.97887 || Validation Loss: 0.16346 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 84 - 이전 : 0.16586905717849731, 현재: 0.1634615808725357\n",
      "Epoch[85/100] - Train loss: 0.11746 Train Accucracy: 0.98122 || Validation Loss: 0.16116 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 85 - 이전 : 0.1634615808725357, 현재: 0.16116352379322052\n",
      "Epoch[86/100] - Train loss: 0.11505 Train Accucracy: 0.98357 || Validation Loss: 0.15896 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 86 - 이전 : 0.16116352379322052, 현재: 0.15896102786064148\n",
      "Epoch[87/100] - Train loss: 0.11274 Train Accucracy: 0.98357 || Validation Loss: 0.15684 Validation Accuracy: 0.93706\n",
      "====================================================================================================\n",
      "저장: 87 - 이전 : 0.15896102786064148, 현재: 0.1568416953086853\n",
      "Epoch[88/100] - Train loss: 0.11052 Train Accucracy: 0.98357 || Validation Loss: 0.15481 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 88 - 이전 : 0.1568416953086853, 현재: 0.15481281280517578\n",
      "Epoch[89/100] - Train loss: 0.10839 Train Accucracy: 0.98357 || Validation Loss: 0.15287 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 89 - 이전 : 0.15481281280517578, 현재: 0.15286557376384735\n",
      "Epoch[90/100] - Train loss: 0.10634 Train Accucracy: 0.98357 || Validation Loss: 0.15101 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 90 - 이전 : 0.15286557376384735, 현재: 0.1510087549686432\n",
      "Epoch[91/100] - Train loss: 0.10437 Train Accucracy: 0.98357 || Validation Loss: 0.14923 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 91 - 이전 : 0.1510087549686432, 현재: 0.14922881126403809\n",
      "Epoch[92/100] - Train loss: 0.10249 Train Accucracy: 0.98357 || Validation Loss: 0.14752 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 92 - 이전 : 0.14922881126403809, 현재: 0.1475222408771515\n",
      "Epoch[93/100] - Train loss: 0.10068 Train Accucracy: 0.98357 || Validation Loss: 0.14589 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 93 - 이전 : 0.1475222408771515, 현재: 0.14588916301727295\n",
      "Epoch[94/100] - Train loss: 0.09894 Train Accucracy: 0.98592 || Validation Loss: 0.14431 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 94 - 이전 : 0.14588916301727295, 현재: 0.14430958032608032\n",
      "Epoch[95/100] - Train loss: 0.09727 Train Accucracy: 0.98826 || Validation Loss: 0.14279 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 95 - 이전 : 0.14430958032608032, 현재: 0.14279384911060333\n",
      "Epoch[96/100] - Train loss: 0.09566 Train Accucracy: 0.98826 || Validation Loss: 0.14134 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 96 - 이전 : 0.14279384911060333, 현재: 0.14134082198143005\n",
      "Epoch[97/100] - Train loss: 0.09411 Train Accucracy: 0.98826 || Validation Loss: 0.13995 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 97 - 이전 : 0.14134082198143005, 현재: 0.13995078206062317\n",
      "Epoch[98/100] - Train loss: 0.09262 Train Accucracy: 0.98826 || Validation Loss: 0.13861 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 98 - 이전 : 0.13995078206062317, 현재: 0.13860806822776794\n",
      "Epoch[99/100] - Train loss: 0.09119 Train Accucracy: 0.98826 || Validation Loss: 0.13732 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 99 - 이전 : 0.13860806822776794, 현재: 0.13732042908668518\n",
      "Epoch[100/100] - Train loss: 0.08981 Train Accucracy: 0.98826 || Validation Loss: 0.13609 Validation Accuracy: 0.94406\n",
      "====================================================================================================\n",
      "저장: 100 - 이전 : 0.13732042908668518, 현재: 0.1360933780670166\n",
      "1.1338746547698975 초\n"
     ]
    }
   ],
   "source": [
    "result = train.fit(wb_train_loader, \n",
    "                   wb_test_loader, \n",
    "                   model, \n",
    "                   loss_fn,\n",
    "                   optimizer,\n",
    "                   100, \n",
    "                   save_model_path=\"models/bc_model.pt\",\n",
    "                   patience=5, \n",
    "                   device=device\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5] - Train loss: 0.41529 Train Accucracy: 0.84445 || Validation Loss: 0.45145 Validation Accuracy: 0.83180\n",
      "====================================================================================================\n",
      "저장: 1 - 이전 : inf, 현재: 0.4514491139333459\n",
      "Epoch[2/5] - Train loss: 0.35009 Train Accucracy: 0.87058 || Validation Loss: 0.40213 Validation Accuracy: 0.85830\n",
      "====================================================================================================\n",
      "저장: 2 - 이전 : 0.4514491139333459, 현재: 0.4021347248101536\n",
      "Epoch[3/5] - Train loss: 0.32157 Train Accucracy: 0.88220 || Validation Loss: 0.38074 Validation Accuracy: 0.86540\n",
      "====================================================================================================\n",
      "저장: 3 - 이전 : 0.4021347248101536, 현재: 0.38073535131502756\n",
      "Epoch[4/5] - Train loss: 0.30813 Train Accucracy: 0.88730 || Validation Loss: 0.37125 Validation Accuracy: 0.86850\n",
      "====================================================================================================\n",
      "저장: 4 - 이전 : 0.38073535131502756, 현재: 0.3712546576427508\n",
      "Epoch[5/5] - Train loss: 0.28368 Train Accucracy: 0.89715 || Validation Loss: 0.36250 Validation Accuracy: 0.87580\n",
      "====================================================================================================\n",
      "저장: 5 - 이전 : 0.3712546576427508, 현재: 0.36250071567070635\n",
      "247.4015190601349 초\n"
     ]
    }
   ],
   "source": [
    "# 다중분류\n",
    "model2 = FashionMNISTModel()\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "result2 = train.fit(fmnist_train_loader, \n",
    "                    fmnist_test_loader, \n",
    "                    model2, \n",
    "                    loss_fn2,\n",
    "                    optimizer2, \n",
    "                    5,\n",
    "                    save_model_path=\"models/fminist_model.pt\", \n",
    "                    early_stopping=False, \n",
    "                    device=device,\n",
    "                    mode=\"multi\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
