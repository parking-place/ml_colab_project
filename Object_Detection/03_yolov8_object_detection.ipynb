{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c408c92e",
   "metadata": {},
   "source": [
    "# [YOLOv8](https://docs.ultralytics.com/)\n",
    "\n",
    "## ì„¤ì¹˜\n",
    "\n",
    "1. íŒŒì´í† ì¹˜ ì„¤ì¹˜\n",
    "2. YOLOv8 ì„¤ì¹˜\n",
    "    - `pip install ultralytics`\n",
    "3. ì£¼í”¼í„°ë…¸íŠ¸ë¶ì—ì„œ ì‹¤í–‰í•  ê²½ìš° í”„ë¡œê·¸ë˜ìŠ¤ë°”ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ ë‹¤ìŒì„ ì„¤ì¹˜í•œë‹¤. (í•„ìˆ˜ëŠ” ì•„ë‹˜)\n",
    "    - `conda install -y -c conda-forge ipywidgets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b0210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (8.0.117)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (4.7.0.72)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (2.28.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (2.0.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (0.15.2+cu118)\n",
      "Requirement already satisfied: psutil in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (9.3.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.24.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (4.39.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.13)\n",
      "Requirement already satisfied: typing-extensions in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (4.4.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
      "Requirement already satisfied: networkx in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.0)\n",
      "Requirement already satisfied: filelock in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n",
      "Requirement already satisfied: cmake in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (15.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from sympy->torch>=1.7.0->ultralytics) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01f3d95a",
   "metadata": {},
   "source": [
    "## ì‚¬ìš©\n",
    "- CLI (command line interface)ì—ì„œ í„°ë¯¸ë„ ëª…ë ¹ì–´ë¡œ ì¶”ë¡ /í‰ê°€/í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.\n",
    "- Python lib ë¥¼ ì´ìš©í•´ ì½”ë“œìƒì— ì›í•˜ëŠ” ì¶”ë¡ /í‰ê°€/í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bbfe070",
   "metadata": {},
   "source": [
    "# CLI ê¸°ë³¸ ëª…ë ¹ì–´ êµ¬ì¡°\n",
    "\n",
    "- êµ¬ë¬¸\n",
    "    - <span style='font-size:1.3em'>**yolo**  **task**=detect|classify|segment  **mode**=train|val|predict  **model**=yolov8n.yaml|yolov8n.pt|..  **args**</span>\n",
    "    - <b style='font-size:1.2em'>task:</b> \\[detect, classify, segment\\] ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •í•œë‹¤. \\[optional\\]ë¡œ ìƒëµí•˜ë©´ modelì„ ë³´ê³  ì¶”ì¸¡í•´ì„œ taskë¥¼ ì •í•œë‹¤.\n",
    "        - **detect:** Object detection\n",
    "        - **classify:** Image classification\n",
    "        - **segment:** Instance segmentation\n",
    "    - <b style='font-size:1.2em'>mode:</b> \\[train, val, predict, export\\] ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •í•œë‹¤. \\[í•„ìˆ˜\\]ë¡œ ì…ë ¥í•´ì•¼ í•œë‹¤.\n",
    "        - **train:** custom datasetì„ train ì‹œí‚¨ë‹¤.\n",
    "        - **val:** ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n",
    "        - **predict:** ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ í•œë‹¤.\n",
    "        - **export:** ëª¨ë¸ì„ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œë‹¤.\n",
    "    - <b style='font-size:1.2em'>model:</b> **pretrained ëª¨ë¸**ì´ë‚˜ **ëª¨ë¸ ì„¤ì • yaml íŒŒì¼**ì˜ ê²½ë¡œë¥¼ ì„¤ì •í•œë‹¤. \\[í•„ìˆ˜\\]ë¡œ ì…ë ¥í•´ì•¼ í•œë‹¤.\n",
    "        - pretrained ëª¨ë¸ íŒŒì¼ê²½ë¡œ\n",
    "            - taskì— ë§ëŠ” pretrained ëª¨ë¸íŒŒì¼ì˜ ì €ì¥ ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤.\n",
    "            - transfer learngingì„ í•˜ê±°ë‚˜ fine tuning ì‹œ ë°©ë²•\n",
    "        - ëª¨ë¸ êµ¬ì¡° ì„¤ì • yaml íŒŒì¼ ê²½ë¡œ\n",
    "            - taskì— ë§ëŠ” pretrained ëª¨ë¸ ì„¤ì •íŒŒì¼(yamlíŒŒì¼)ì˜ ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤.\n",
    "            - train modeì—ì„œ ì§€ì •í•˜ë©° ëª¨ë¸ì„ ìƒˆë¡œ ìƒì„±í•´ì„œ ì²˜ìŒë¶€í„° í•™ìŠµ ì‹œí‚¬ ê²½ìš° ì§€ì •í•œë‹¤.\n",
    "        - Ultralyticsì—ì„œ ì œê³µí•˜ëŠ” Pretrained ëª¨ë¸\n",
    "            - ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ 5ê°œì˜ ëª¨ë¸ì„ ì œê³µí•˜ë©° í° ëª¨ë¸ì€ ì‘ì€ ëª¨ë¸ì— ë¹„í•´ ì¶”ë¡  ì„±ëŠ¥ì´ ì¢‹ì€ëŒ€ì‹  ì†ë„ëŠ” ëŠë¦¬ë‹¤.\n",
    "            - ëª¨ë¸ì€ ì²˜ìŒ ì¶”ë¡ ë˜ëŠ” í•™ìŠµí• ë•Œ local ì»´í“¨í„°ì— ì—†ìœ¼ë©´ download ë°›ëŠ”ë‹¤.\n",
    "            - https://github.com/ultralytics/ultralytics#models\n",
    "            - ### ì œê³µ ëª¨ë¸\n",
    "            \n",
    "            | **task\\ëª¨ë¸í¬ê¸°**           | **nano** | **small_** | **medium** | **large** | **xlarge** |\n",
    "            |:--------------------|----------|-------------|------------|-----------|----------|\n",
    "            | **detection**      | yolov8n  | yolov8s     | yolov8m    | yolov8l   | yolov8x    |\n",
    "            | **segmentation**   | yolov8n-seg  | yolov8s-seg     | yolov8m-seg    | yolov8l-seg   | yolov8x-seg    |\n",
    "            | **classification** | yolov8n-cls  | yolov8s-cls     | yolov8m-cls    | yolov8l-cls   | yolov8x-cls    |         \n",
    "            | **pose estimation** | yolov8n-pose  | yolov8s-pose     | yolov8m-pose    | yolov8l-pose   | yolov8x-pose    |\n",
    "            \n",
    "            - í™•ì¥ìê°€ `pt`ì´ë©´ pretrained ëœ ëª¨ë¸ì„, `yaml`ì´ë©´ ëª¨ë¸ êµ¬ì¡° ì„¤ì •íŒŒì¼ì„ downloadí•˜ì—¬ ì‹¤í–‰í•œë‹¤.\n",
    "                - pretrained modelì€ fine tuningì´ë‚˜ ì¶”ë¡ í•  ë•Œ, yamlì„¤ì •íŒŒì¼ì€ ì²˜ìŒë¶€í„° í•™ìŠµí•  ê²½ìš° ì„¤ì •í•˜ì—¬ ë°›ëŠ”ë‹¤.\n",
    "    - <b style='font-size:1.2em'>args:</b> taskì™€ modeê³¼ ê´€ë ¨í•œ ì¶”ê°€ ì„¤ì •ê°’ë“¤ì„ ì§€ì •í•œë‹¤.\n",
    "        - https://docs.ultralytics.com/cfg/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb7e10cd",
   "metadata": {},
   "source": [
    "# [Object Detection](https://docs.ultralytics.com/tasks/detection/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ca360a0",
   "metadata": {},
   "source": [
    "##  Predict (ì¶”ë¡ )\n",
    "\n",
    "### ëª¨ë¸ë¡œë”©\n",
    "- Ultralyticsì—ì„œ ì œê³µí•˜ëŠ” Pretrained Modelì´ë‚˜ ì§ì ‘ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ì´ìš©í•´ ì¶”ë¡ í•œë‹¤.\n",
    "- UltralyticsëŠ” Object Detectionì„ ìœ„í•œ [Pretrained ëª¨ë¸](#ì œê³µ-ëª¨ë¸)ì„ ì œê³µí•œë‹¤.\n",
    "    - Object Detection ëª¨ë¸ì€ COCO datasetìœ¼ë¡œ í•™ìŠµë˜ì—ˆë‹¤.\n",
    "    - ëª¨ë¸ ëª…ì„ ì§€ì •í•˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë¥¼ ë°›ëŠ”ë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8235d4bc",
   "metadata": {},
   "source": [
    "### CLI\n",
    "`yolo task=detect mode=predict model=model_path source=ì¶”ë¡ í• _image_path`\n",
    "- ì¶”ê°€ ì„¤ì • (configuration)\n",
    "    - https://docs.ultralytics.com/cfg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631c8155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (8.0.117)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (9.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (0.15.2+cu118)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (4.7.0.72)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (2.28.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: psutil in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (2.0.1+cu118)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (1.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from ultralytics) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.24.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.9.0)\n",
      "Requirement already satisfied: networkx in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.0)\n",
      "Requirement already satisfied: sympy in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: cmake in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.0)\n",
      "Requirement already satisfied: lit in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (15.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/parking/anaconda3/envs/ml/lib/python3.10/site-packages (from sympy->torch>=1.7.0->ultralytics) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b52437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /home/parking/ml/ml_colab_project/Object_Detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d91578f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T07:29:55.783401Z",
     "start_time": "2023-03-09T07:29:46.404535Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.117 ğŸš€ Python-3.10.11 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12042MiB)\n",
      "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients\n",
      "\n",
      "/home/parking/anaconda3/envs/ml/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "image 1/1 /home/parking/ml/ml_colab_project/Object_Detection/test_image/3.jpg: 448x640 1 car, 1 cup, 1 chair, 1 tv, 1 mouse, 1 keyboard, 3 cell phones, 128.3ms\n",
      "Speed: 1.3ms preprocess, 128.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n",
      "1 label saved to runs/detect/predict3/labels\n"
     ]
    }
   ],
   "source": [
    "!yolo  task=detect  mode=predict   model=models/yolov8s.pt   source=test_image/3.jpg  save=True  save_txt=True  line_width=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b0f28a7",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f37008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T07:38:46.694238Z",
     "start_time": "2023-03-09T07:38:42.088027Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "522e29bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T07:40:51.237815Z",
     "start_time": "2023-03-09T07:40:25.206490Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt to models/yolov8x.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131M/131M [00:04<00:00, 28.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"models/yolov8x.pt\")  # YOLO í´ë˜ìŠ¤ ê°ì²´ ìƒì„±í•˜ë©´ì„œ ì‚¬ìš©í•  pretrained modelì˜ ê²½ë¡œë¥¼ ì§€ì •."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2881ee4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T07:48:11.948615Z",
     "start_time": "2023-03-09T07:48:09.261815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/parking/ml/ml_colab_project/Object_Detection/test_image/bus.jpg: 640x480 5 persons, 1 bicycle, 1 bus, 141.8ms\n",
      "Speed: 12.7ms preprocess, 141.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n",
      "3 labels saved to runs/detect/predict7/labels\n"
     ]
    }
   ],
   "source": [
    "image_path = 'test_image/bus.jpg'\n",
    "result_list = model(image_path, save=True, save_txt=True, line_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e271a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:08:59.743991Z",
     "start_time": "2023-03-09T08:08:59.729442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_list), len(result_list)\n",
    "# ë¦¬ìŠ¤íŠ¸ì— ì¶”ë¡ í•œ ê²°ê³¼ë¥¼ ì¶”ë¡ í•œ ì´ë¯¸ì§€ë³„ë¡œ ì €ì¥í•´ì„œ ë°˜í™˜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb48e01f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:10:01.164886Z",
     "start_time": "2023-03-09T08:10:01.155305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.yolo.engine.results.Results"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_list[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f33daef5",
   "metadata": {},
   "source": [
    "### í•œë²ˆì— ì—¬ëŸ¬ì¥ ì¶”ë¡ \n",
    "- ì¶”ë¡ í•  íŒŒì¼ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ì„œ ì¶”ë¡ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a4ca18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:12:43.479144Z",
     "start_time": "2023-03-09T08:12:43.459754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_image/1433148424953500.jpg',\n",
       " 'test_image/2.jpg',\n",
       " 'test_image/3.jpg',\n",
       " 'test_image/4.jpg',\n",
       " 'test_image/6.jpg',\n",
       " 'test_image/1.jpg',\n",
       " 'test_image/5.jpg',\n",
       " 'test_image/catsss.jpg',\n",
       " 'test_image/bus.jpg']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "file_path = glob('test_image/*.jpg')\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9373630b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:14:18.181503Z",
     "start_time": "2023-03-09T08:14:09.443734Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 11 persons, 2 elephants, 1: 640x640 15 persons, 6 cars, 4 buss, 4 traffic lights, 1 umbrella, 2: 640x640 1 car, 2 chairs, 1 tv, 1 mouse, 1 keyboard, 2 cell phones, 3: 640x640 3 elephants, 1 zebra, 4: 640x640 3 persons, 1 wine glass, 2 cups, 1 fork, 1 knife, 3 pizzas, 3 dining tables, 5: 640x640 9 persons, 5 bicycles, 5 cars, 1 motorcycle, 1 bus, 4 traffic lights, 1 dog, 1 backpack, 1 handbag, 6: 640x640 9 persons, 1 tie, 1 bottle, 15 wine glasss, 3 cups, 3 forks, 1 knife, 1 spoon, 1 bowl, 1 potted plant, 1 dining table, 1 vase, 7: 640x640 10 cars, 2 cats, 1 bowl, 8: 640x640 4 persons, 1 bicycle, 1 bus, 329.8ms\n",
      "Speed: 1.6ms preprocess, 36.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict10\u001b[0m\n",
      "9 labels saved to runs/detect/predict10/labels\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('models/yolov8x.pt')\n",
    "result_list = model(file_path, save=True, save_txt=True, line_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca45b257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:19:58.998534Z",
     "start_time": "2023-03-09T08:19:58.979069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, ultralytics.yolo.engine.results.Results)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_list), type(result_list[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "471f3231",
   "metadata": {},
   "source": [
    "### web ìƒì˜ ì´ë¯¸ì§€ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c7a79d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:22:05.173435Z",
     "start_time": "2023-03-09T08:22:02.111740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://ultralytics.com/images/bus.jpg to bus.jpg...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 476k/476k [00:00<00:00, 3.83MB/s]\n",
      "image 1/1 /home/parking/ml/ml_colab_project/Object_Detection/bus.jpg: 640x480 5 persons, 1 bicycle, 1 bus, 38.1ms\n",
      "Speed: 10.5ms preprocess, 38.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict8\u001b[0m\n",
      "7 labels saved to runs/detect/predict8/labels\n"
     ]
    }
   ],
   "source": [
    "result_list = model(\"https://ultralytics.com/images/bus.jpg\", save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "488fd9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://d2u3dcdbebyaiu.cloudfront.net/uploads/atch_img/169/f531ef142dbb8dd3211fb40d3fca8f5a_res_a..gif to f531ef142dbb8dd3211fb40d3fca8f5a_res_a..gif...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159k/159k [00:00<00:00, 1.51MB/s]\n",
      "\n",
      "    WARNING âš ï¸ stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (1/1) /home/parking/ml/ml_colab_project/Object_Detection/f531ef142dbb8dd3211fb40d3fca8f5a_res_a..gif: 384x640 1 person, 2 benchs, 1 cat, 126.1ms\n",
      "Speed: 20.7ms preprocess, 126.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict10\u001b[0m\n",
      "10 labels saved to runs/detect/predict10/labels\n"
     ]
    }
   ],
   "source": [
    "result_list = model('https://d2u3dcdbebyaiu.cloudfront.net/uploads/atch_img/169/f531ef142dbb8dd3211fb40d3fca8f5a_res_a..gif', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe7dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffcea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https:\\jjal.today\\data\\file\\gallery\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif to 1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.19M/1.19M [00:00<00:00, 1.27MB/s]\n",
      "\n",
      "    WARNING  stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (1/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 7 persons, 2 ties, 1 cup, 1 clock, 917.2ms\n",
      "video 1/1 (2/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 5 persons, 2 ties, 856.1ms\n",
      "video 1/1 (3/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 5 persons, 2 ties, 830.3ms\n",
      "video 1/1 (4/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 6 persons, 2 ties, 815.2ms\n",
      "video 1/1 (5/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 6 persons, 2 ties, 854.3ms\n",
      "video 1/1 (6/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 6 persons, 3 ties, 1 cup, 963.8ms\n",
      "video 1/1 (7/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 6 persons, 3 ties, 836.0ms\n",
      "video 1/1 (8/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 6 persons, 3 ties, 895.3ms\n",
      "video 1/1 (9/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 6 persons, 3 ties, 845.2ms\n",
      "video 1/1 (10/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 8 persons, 844.3ms\n",
      "video 1/1 (11/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 10 persons, 862.7ms\n",
      "video 1/1 (12/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 8 persons, 899.4ms\n",
      "video 1/1 (13/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 9 persons, 857.6ms\n",
      "video 1/1 (14/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 8 persons, 868.2ms\n",
      "video 1/1 (15/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 8 persons, 834.4ms\n",
      "video 1/1 (16/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 11 persons, 839.0ms\n",
      "video 1/1 (17/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 8 persons, 865.6ms\n",
      "video 1/1 (18/18) e:\\Python\\ml_colab_project\\Object_Detection\\1850094512_s9OJf0Qw_0d61635e94751fd731caba342e24a60ab5a5e0f5.gif: 384x640 7 persons, 839.6ms\n",
      "Speed: 2.4ms preprocess, 862.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "img_url = 'https://s.gae9.com/trend/99e262b59dda948c.orig'\n",
    "result_list = model(img_url, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4fe0fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-09T08:24:59.228076Z",
     "start_time": "2023-03-09T08:24:57.908118Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://storage3.ilyo.co.kr/contents/article/images/2015/0601/1433148424953500.jpg to 1433148424953500.jpg...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 228k/228k [00:00<00:00, 14.7MB/s]\n",
      "image 1/1 /home/parking/ml/ml_colab_project/Object_Detection/1433148424953500.jpg: 448x640 12 persons, 2 elephants, 31.1ms\n",
      "Speed: 2.5ms preprocess, 31.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict8\u001b[0m\n",
      "8 labels saved to runs/detect/predict8/labels\n"
     ]
    }
   ],
   "source": [
    "result_list = model('https://storage3.ilyo.co.kr/contents/article/images/2015/0601/1433148424953500.jpg', save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcd00d48",
   "metadata": {},
   "source": [
    "## ì¶”ë¡ ê²°ê³¼"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49ea25ad",
   "metadata": {},
   "source": [
    "### ultralytics.yolo.engine.results.Results\n",
    "- ëª¨ë¸ì˜ ì¶”ë¡  ê²°ê³¼ëŠ” listì— ì´ë¯¸ì§€ë³„ ì˜ˆì¸¡ê²°ê³¼ë¥¼ Resultsì— ë‹´ì•„ ë°˜í™˜í•œë‹¤.\n",
    "- **Results** : í•œê°œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ê²°ê³¼ë¥¼ ë‹´ëŠ” ê°ì²´\n",
    "- ì¶”ë¡  ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ìŒ ì†ì„±ì„ ì´ìš©í•´ ê²°ê³¼ë¥¼ ì¡°íšŒí•œë‹¤.\n",
    "    - Detection: `result.boxes` - Boxes type\n",
    "    - Segmentation: `result.masks` - Masks type\n",
    "    - Classification: `result.probs` - torch.Tensor type\n",
    "    - Pose: `result.keypoints` - Keypoints type\n",
    "- ì¶”ê°€ ì •ë³´\n",
    "    - Results.orig_img: ì¶”ë¡ í•œ ì›ë³¸ ì´ë¯¸ì§€\n",
    "    - Results.orig_shape: ì¶”ë¡ í•œ ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸° (height, width)\n",
    "    - Results.path: ì¶”ë¡ í•œ ì›ë³¸ì´ë¯¸ì§€ì˜ ê²½ë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1163e3a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T02:37:27.849021Z",
     "start_time": "2023-03-10T02:37:25.757658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Python\\ml_colab_project\\Object_Detection\\test_image\\bus.jpg: 640x480 4 persons, 1 bus, 622.1ms\n",
      "Speed: 3.0ms preprocess, 622.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('models/yolov8m.pt')\n",
    "result_list = model('./test_image/bus.jpg', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "decff4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T00:47:22.201958Z",
     "start_time": "2023-03-10T00:47:22.181626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1, ultralytics.yolo.engine.results.Results)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_list), len(result_list), type(result_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ae20f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:18:30.976636Z",
     "start_time": "2023-03-10T01:18:30.946843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ: e:\\Python\\ml_colab_project\\Object_Detection\\test_image\\bus.jpg\n",
      "ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: (1080, 810)\n",
      "ì›ë³¸ ì´ë¯¸ì§€: (1080, 810, 3)\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ë¡ í•œ ì›ë³¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì •ë³´\n",
    "result = result_list[0]\n",
    "print('ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ:', result.path)\n",
    "print('ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°:', result.orig_shape)\n",
    "print('ì›ë³¸ ì´ë¯¸ì§€:', result.orig_img.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2825ee77",
   "metadata": {},
   "source": [
    "### Object Detection ê²°ê³¼ê°’ ì¡°íšŒ\n",
    "\n",
    "- ultralytics.yolo.engine.results.**Boxes**ì— ì¶”ë¡  ê²°ê³¼ë¥¼ ë‹´ì•„ ë°˜í™˜\n",
    "    - Results.boxesë¡œ ì¡°íšŒ\n",
    "- ì£¼ìš” ì†ì„±\n",
    "    - shape: ê²°ê³¼ shape. (ì°¾ì€ ë¬¼ì²´ê°œìˆ˜, 6)\n",
    "    - boxes\n",
    "        - 6: ì¢Œìƒë‹¨ x, ì¢Œìƒë‹¨ y, ìš°í•˜ë‹¨ x, ìš°í•˜ë‹¨ y, confidence score, label\n",
    "    - xyxy\n",
    "        - bounding boxì˜ `ì¢Œìƒë‹¨ x, ì¢Œìƒë‹¨ y, ìš°í•˜ë‹¨ x, ìš°í•˜ë‹¨ y` ì¢Œí‘œ ë°˜í™˜\n",
    "    - xyxyn\n",
    "        - xyxyë¥¼ ì´ë¯¸ì§€ ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ë°˜í™˜\n",
    "    - xywh\n",
    "        - bounding boxì˜ `center x, center y, ë„ˆë¹„, ë†’ì´` ë¥¼ ë°˜í™˜\n",
    "    - xywhn\n",
    "        - xywhë¥¼ ì´ë¯¸ì§€ ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ë°˜í™˜\n",
    "    - cls: ì°¾ì€ ë¬¼ì²´ì˜ label\n",
    "    - conf: clsì— ëŒ€í•œ confidence score (ê·¸ ë¬¼ì²´ì¼ í™•ë¥ )\n",
    "    - boxes\n",
    "        - `x, y, x, y, conf, cls` tensorë¥¼ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6487f920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:27:01.037555Z",
     "start_time": "2023-03-10T01:27:01.020187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.yolo.engine.results.Boxes'>\n"
     ]
    }
   ],
   "source": [
    "boxes = result.boxes  # detectioní•œ ê²°ê³¼ë¥¼ ì¡°íšŒ (Boxes ê°ì²´)\n",
    "print(type(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f364174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:27:30.413278Z",
     "start_time": "2023-03-10T01:27:30.389162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.shape\n",
    "# [n, 6] : n-ì°¾ì€ bbox(object) ê°œìˆ˜,  6-x y x y label conf  (x y x y : ì¢Œìƒë‹¨ ìš°í•˜ë‹¨ ì¢Œí‘œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c40a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:31:12.123370Z",
     "start_time": "2023-03-10T01:31:12.101794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 0., 0., 0., 0.])\n",
      "tensor([0.9595, 0.9276, 0.9226, 0.9017, 0.7927])\n"
     ]
    }
   ],
   "source": [
    "# ì°¾ì€ bboxë“¤ì— ëŒ€í•œ classification ì •ë³´\n",
    "print(boxes.cls)  # ì°¾ì€ nê°œ bboxì— ëŒ€í•œ class ë“¤ì„ ë°˜í™˜.\n",
    "print(boxes.conf) # ì°¾ì€ nê°œ bboxì—ëŒ€í•œ confidence score(í™•ë¥ ) ë“¤ì„ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "647c242e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:37:16.938027Z",
     "start_time": "2023-03-10T01:37:16.918682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.9311e+00, 2.2913e+02, 8.0436e+02, 7.4081e+02],\n",
      "        [5.0429e+01, 3.9968e+02, 2.4746e+02, 9.0459e+02],\n",
      "        [6.6839e+02, 3.9516e+02, 8.0971e+02, 8.8096e+02],\n",
      "        [2.2227e+02, 4.1113e+02, 3.4418e+02, 8.6109e+02],\n",
      "        [2.3668e-01, 5.5000e+02, 7.8286e+01, 8.7247e+02]])\n",
      "tensor([[3.6187e-03, 2.1216e-01, 9.9304e-01, 6.8594e-01],\n",
      "        [6.2258e-02, 3.7008e-01, 3.0551e-01, 8.3759e-01],\n",
      "        [8.2517e-01, 3.6589e-01, 9.9964e-01, 8.1570e-01],\n",
      "        [2.7441e-01, 3.8067e-01, 4.2491e-01, 7.9730e-01],\n",
      "        [2.9220e-04, 5.0926e-01, 9.6650e-02, 8.0784e-01]])\n"
     ]
    }
   ],
   "source": [
    "# bboxì˜ ìœ„ì¹˜ì •ë³´\n",
    "print(boxes.xyxy)  # ì¢Œìƒë‹¨ ìš°í•˜ë‹¨ x/yì¢Œí‘œ\n",
    "print(boxes.xyxyn) #ì´ë¯¸ì§€í¬ê¸° ëŒ€ë¹„ ë¹„ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd37164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:38:38.165810Z",
     "start_time": "2023-03-10T01:38:38.146348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[403.6475, 484.9729, 801.4327, 511.6810],\n",
      "        [148.9455, 652.1385, 197.0324, 504.9107],\n",
      "        [739.0483, 638.0619, 141.3201, 485.7975],\n",
      "        [283.2275, 636.1057, 121.9057, 449.9594],\n",
      "        [ 39.2615, 711.2346,  78.0497, 322.4717]])\n",
      "tensor([[0.4983, 0.4490, 0.9894, 0.4738],\n",
      "        [0.1839, 0.6038, 0.2432, 0.4675],\n",
      "        [0.9124, 0.5908, 0.1745, 0.4498],\n",
      "        [0.3497, 0.5890, 0.1505, 0.4166],\n",
      "        [0.0485, 0.6586, 0.0964, 0.2986]])\n"
     ]
    }
   ],
   "source": [
    "print(boxes.xywh)  # center x, yì¢Œí‘œ, bbox width, height\n",
    "print(boxes.xywhn)  # center x, yì¢Œí‘œ, bbox width, height ì´ë¯¸ì§€í¬ê¸° ëŒ€ë¹„ ë¹„ìœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c782961a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:45:00.968554Z",
     "start_time": "2023-03-10T01:45:00.949314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 92, 92)\n",
      "person\n",
      "goldfish, Carassius auratus\n"
     ]
    }
   ],
   "source": [
    "from module import util\n",
    "\n",
    "print(util.get_color(0))\n",
    "print(util.get_coco80_classname(0))\n",
    "print(util.get_imagenet_classname(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753dfe79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T01:49:19.595145Z",
     "start_time": "2023-03-10T01:49:19.585693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bus'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = boxes.cls[0]\n",
    "\n",
    "util.get_coco80_classname(int(idx.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55a13893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T02:39:36.686733Z",
     "start_time": "2023-03-10T02:39:34.470549Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Python\\ml_colab_project\\Object_Detection\\test_image\\bus.jpg: 640x480 4 persons, 1 bus, 1 stop sign, 222.0ms\n",
      "Speed: 2.0ms preprocess, 222.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ì›ë³¸ ì´ë¯¸ì§€ì— ì¶”ë¡  ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('models/yolov8n.pt')\n",
    "path = 'bus.jpg'\n",
    "path = 'test_image/bus.jpg'\n",
    "result_list = model(path, save=True)\n",
    "result = result_list[0]\n",
    "\n",
    "org_img = result.orig_img  #BGR\n",
    "img = org_img.copy()\n",
    "\n",
    "boxes = result.boxes\n",
    "xyxy_list = boxes.xyxy  #ì¢Œìƒë‹¨/ìš°í•˜ë‹¨ ì¢Œí‘œ\n",
    "cls_list = boxes.cls    #label\n",
    "conf_list = boxes.conf  #label  í™•ë¥ .\n",
    "for xyxy, cls, conf in zip(xyxy_list, cls_list, conf_list):\n",
    "#     print(xyxy, conf, cls)\n",
    "    xyxy_arr = xyxy.to('cpu').numpy().astype('int32')\n",
    "    pt1 = xyxy_arr[:2]\n",
    "    pt2 = xyxy_arr[2:]\n",
    "    \n",
    "    label_name = util.get_coco80_classname(int(cls.item()))\n",
    "    txt = f\"{label_name}-{conf.item()*100:.2f}\"\n",
    "    \n",
    "    color = util.get_color(int(cls.item()) % 10)\n",
    "    # bbox\n",
    "    cv2.rectangle(img, pt1=pt1, pt2=pt2, color=color, thickness=2)\n",
    "    # label\n",
    "    cv2.putText(img, text=txt, org=pt1-5, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, \n",
    "                color=color, thickness=1, lineType=cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4d39f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T02:47:26.274458Z",
     "start_time": "2023-03-10T02:47:08.973212Z"
    }
   },
   "outputs": [],
   "source": [
    "cv2.imshow('frame', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59ff3703",
   "metadata": {},
   "source": [
    "## ì‹¤ì‹œê°„ Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88d0397a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T05:27:33.119152Z",
     "start_time": "2023-03-10T05:27:05.214673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https:\\github.com\\ultralytics\\assets\\releases\\download\\v0.0.0\\yolov8n.pt to models\\yolov8n.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:02<00:00, 2.59MB/s]\n",
      "\n",
      "0: 384x640 (no detections), 309.0ms\n",
      "Speed: 5.1ms preprocess, 309.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 386.0ms\n",
      "Speed: 12.3ms preprocess, 386.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 384.8ms\n",
      "Speed: 6.5ms preprocess, 384.8ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 bottles, 2 tvs, 1 laptop, 367.7ms\n",
      "Speed: 6.3ms preprocess, 367.7ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 2 laptops, 231.7ms\n",
      "Speed: 4.7ms preprocess, 231.7ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 2 laptops, 327.3ms\n",
      "Speed: 5.9ms preprocess, 327.3ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 3 laptops, 309.5ms\n",
      "Speed: 5.0ms preprocess, 309.5ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 1 laptop, 278.3ms\n",
      "Speed: 5.7ms preprocess, 278.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 209.3ms\n",
      "Speed: 4.8ms preprocess, 209.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 2 laptops, 197.2ms\n",
      "Speed: 5.6ms preprocess, 197.2ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 196.2ms\n",
      "Speed: 4.0ms preprocess, 196.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 2 laptops, 202.1ms\n",
      "Speed: 3.5ms preprocess, 202.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 3 laptops, 200.4ms\n",
      "Speed: 4.1ms preprocess, 200.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 205.3ms\n",
      "Speed: 5.0ms preprocess, 205.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 3 laptops, 209.5ms\n",
      "Speed: 5.2ms preprocess, 209.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 207.4ms\n",
      "Speed: 4.1ms preprocess, 207.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 3 laptops, 208.6ms\n",
      "Speed: 4.7ms preprocess, 208.6ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 1 laptop, 205.6ms\n",
      "Speed: 3.2ms preprocess, 205.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 2 laptops, 293.3ms\n",
      "Speed: 7.5ms preprocess, 293.3ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 3 laptops, 247.2ms\n",
      "Speed: 5.0ms preprocess, 247.2ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 bottles, 1 chair, 2 tvs, 2 laptops, 264.5ms\n",
      "Speed: 5.4ms preprocess, 264.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 2 laptops, 219.3ms\n",
      "Speed: 3.0ms preprocess, 219.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 2 laptops, 240.5ms\n",
      "Speed: 5.2ms preprocess, 240.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 chair, 2 tvs, 3 laptops, 214.5ms\n",
      "Speed: 6.1ms preprocess, 214.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 tvs, 2 laptops, 219.6ms\n",
      "Speed: 5.4ms preprocess, 219.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 chair, 2 tvs, 3 laptops, 208.6ms\n",
      "Speed: 4.0ms preprocess, 208.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 3 laptops, 201.6ms\n",
      "Speed: 4.0ms preprocess, 201.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 2 laptops, 209.1ms\n",
      "Speed: 4.1ms preprocess, 209.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 194.7ms\n",
      "Speed: 5.1ms preprocess, 194.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 tvs, 2 laptops, 200.3ms\n",
      "Speed: 4.0ms preprocess, 200.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 tvs, 2 laptops, 274.3ms\n",
      "Speed: 5.2ms preprocess, 274.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 3 laptops, 269.7ms\n",
      "Speed: 4.6ms preprocess, 269.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 2 laptops, 278.0ms\n",
      "Speed: 4.6ms preprocess, 278.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 2 laptops, 235.2ms\n",
      "Speed: 4.9ms preprocess, 235.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 3 laptops, 286.3ms\n",
      "Speed: 4.1ms preprocess, 286.3ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 3 laptops, 302.5ms\n",
      "Speed: 6.2ms preprocess, 302.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 2 laptops, 196.8ms\n",
      "Speed: 2.6ms preprocess, 196.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 tvs, 1 laptop, 248.6ms\n",
      "Speed: 4.2ms preprocess, 248.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 2 laptops, 196.1ms\n",
      "Speed: 3.0ms preprocess, 196.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 2 laptops, 188.7ms\n",
      "Speed: 2.0ms preprocess, 188.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 tvs, 2 laptops, 213.6ms\n",
      "Speed: 2.5ms preprocess, 213.6ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 tvs, 3 laptops, 220.4ms\n",
      "Speed: 5.2ms preprocess, 220.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 2 laptops, 195.7ms\n",
      "Speed: 4.0ms preprocess, 195.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 bottles, 1 cup, 1 chair, 1 tv, 2 laptops, 180.5ms\n",
      "Speed: 4.0ms preprocess, 180.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 2 laptops, 212.3ms\n",
      "Speed: 3.0ms preprocess, 212.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 bottles, 2 tvs, 2 laptops, 235.8ms\n",
      "Speed: 5.0ms preprocess, 235.8ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 tvs, 1 laptop, 358.2ms\n",
      "Speed: 6.1ms preprocess, 358.2ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 2 laptops, 304.2ms\n",
      "Speed: 5.4ms preprocess, 304.2ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 2 laptops, 238.7ms\n",
      "Speed: 5.1ms preprocess, 238.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 2 tvs, 202.0ms\n",
      "Speed: 4.2ms preprocess, 202.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from module import util\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ì›¹ìº  ì—°ë™\n",
    "cap = cv2.VideoCapture(0)\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "model = YOLO('models/yolov8n.pt')\n",
    "while True:\n",
    "    # í•œ Frame ì½ê¸°\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print('í”„ë ˆì„ì„ ì½ì§€ ëª»í•¨')\n",
    "        break\n",
    "        \n",
    "    frame = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    result = model(frame)[0]\n",
    "    xyxy_list = result.boxes.xyxy.to('cpu').numpy().astype('int32')\n",
    "    cls_list = result.boxes.cls.to('cpu').numpy().astype('int32')\n",
    "    conf_list = result.boxes.conf.to('cpu').numpy()\n",
    "    \n",
    "    for xyxy, cls, conf in zip(xyxy_list, cls_list, conf_list):\n",
    "        pt1, pt2 = xyxy[:2], xyxy[2:]\n",
    "        txt = f\"{util.get_coco80_classname(cls)}-{conf*100:.2f}%\"\n",
    "        color = util.get_color(cls % 10)\n",
    "        cv2.rectangle(frame, pt1, pt2, color=color)\n",
    "        cv2.putText(frame, txt, org=pt1, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1,\n",
    "                    color=color, thickness=2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    # í™”ë©´ì— ì¶œë ¥\n",
    "    cv2.imshow('frame', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if cv2.waitKey(1) == 27: # esc \n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460c373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d6bb26803bcea62f49e4c67963aa289be587a14c784102c9499967ce94407a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
